{"componentChunkName":"component---src-templates-tags-tsx","path":"/tags/dev-ops/","webpackCompilationHash":"28b74a5dec589e4586df","result":{"data":{"allTagYaml":{"edges":[{"node":{"id":"Angular","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Architecture","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"AWS","description":"Amazon Web Services is a subsidiary of Amazon that provides on-demand cloud computing platforms to individuals, companies and governments, on a paid subscription basis. The technology allows subscribers to have at their disposal a virtual cluster of computers, available all the time, through the Internet.","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Best","description":"Collection of in-deep guides about software development","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"CSS","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"DevOps","description":"Set of practices that automates the processes between software development and IT teams, in order that they can build, test, and release software faster and more reliably.","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Git","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Google Cloud","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Javascript","description":"The most used language in the world","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Kindle","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Node.js","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"React","description":"The most powerful UI library ever","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"Self","description":"Thoughs about life.","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}},{"node":{"id":"SQL","description":null,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAcZUED//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAWEAADAAAAAAAAAAAAAAAAAAAAASD/2gAIAQEAAT8hHP8A/9oADAMBAAIAAwAAABD47//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABgQAAMBAQAAAAAAAAAAAAAAAAABERAh/9oACAEBAAE/EKPlG+FP/9k=","aspectRatio":1.4992503748125936,"src":"/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg","srcSet":"/static/9fb9647467dab072b9ebe36e9e52ae95/f8f18/blog-cover.jpg 930w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/0e6ff/blog-cover.jpg 1860w,\n/static/9fb9647467dab072b9ebe36e9e52ae95/883ab/blog-cover.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}}}}]},"allMarkdownRemark":{"totalCount":7,"edges":[{"node":{"excerpt":"Dans Kubernetes, il existe plusieurs façons de publier une application. Il est donc nécessaire de choisir la bonne stratégie pour rendre votre infrastructure fiable lors de la mise à jour d’une application. Le choix de la procédure de déploiement appropriée dépend des besoins. Nous avons énuméré ci-dessous certaines des stratégies possibles à adopter: Recreate RollingUpdate Blue/Green Canary A/B testing Vous pouvez expérimenter chacune de ces stratégies avec Minikube, les manifestes et les étapes à suivre sont expliqués dans ce github Examinons chaque stratégie et voyons quel type d’application vous conviendrez le mieux.  Recreate - idéal pour l’environnement de développement Un déploiement défini avec une stratégie de type recreate mettra fin à toutes les instances en cours d’exécution, puis les recréera avec la version la plus récente.  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/recreate Avantage État d’application entièrement renouvelé Inconvénients Temps d’arrêt qui dépend à la fois de la durée d’arrêt et du démarrage de l’application  RollingUpdate - déploiement lent Ce déploiement met à jour les pods de façon progressive. Un ReplicaSet secondaire est créé avec la nouvelle version de l’application, puis le nombre de répliques de l’ancienne version est réduit et la nouvelle version est augmentée jusqu’à ce que le nombre correct de répliques soit atteint.  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/ramped Lors de la configuration avec la mise à l’ échelle automatique du pod horizontal, il peut être pratique d’utiliser une valeur en pourcentage au lieu d’un nombre pour maxSurge et maxUnavailable . maxSurge permet d’indiquez combien de Pod il peut créer en plus du nombre de répliqua actuellement configurer maxUnavailable permet d’indiquer combien de Pod peuvent être “non disponible” pendant la mise à jour, toujours en fonction du nombre de répliqua configuré Si vous déclenchez un déploiement alors qu’un déploiement existant est en cours, le déploiement mettra le déploiement en pause et passera à une nouvelle version en remplaçant le déploiement. Avantages la version est lentement publiée sur toutes les instances pratique pour les applications avec état pouvant gérer le rééquilibrage des données Inconvénients le déploiement/la restauration peut prendre du temps la prise en charge de plusieurs API est difficile aucun contrôle sur le trafic  Blue/Green - mieux éviter les problèmes de versioning de l’API Un déploiement blue/green diffère d’un déploiement parce que la version “green” de l’application est déployée en parallèle de la version “blue”. Après avoir vérifié que la nouvelle version réponde aux exigences, nous mettons à jour l’objet Kubernetes Service qui joue le rôle d’équilibreur de charge pour envoyer du trafic vers la nouvelle version en remplaçant l’étiquette de version dans le champ selector  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/blue-green Avantages déploiement instantané éviter le problème de versioning, changer l’état du cluster en une fois Inconvénients nécessite le double des ressources le bon test de toute la plate-forme doit être effectué avant la mise en production la gestion des applications avec état peut être difficile  Canary - laissez le consommateur faire le test Le déploiement Canary consiste à router un sous-ensemble d’utilisateurs vers une nouvelle fonctionnalité. Dans Kubernetes, un déploiement canary peut être effectué en utilisant deux déploiements avec des étiquettes de pods communes. Une réplique de la nouvelle version est publiée à côté de l’ancienne version. Ensuite, après un certain temps et si aucune erreur n’est détectée, augmentez le nombre de répliques de la nouvelle version et supprimez l’ancien déploiement. L’utilisation de cette technique ReplicaSet nécessite de faire tourner autant de pod que nécessaire pour obtenir le bon pourcentage de trafic. Cela dit, si vous voulez envoyer 1% du trafic vers la version B, vous devez avoir un pod fonctionnant avec la version B et 99 pods fonctionnant avec la version A. Cela peut être assez peu pratique à gérer donc si vous recherchez une meilleure répartition du trafic, regarder les équilibreurs de charge tels que HAProxy ou les mailles de service comme Linkerd, qui offrent un meilleur contrôle du trafic.  Dans l’exemple suivant, nous utilisons deux ReplicaSets côte à côte, la version A avec trois répliques (75% du trafic), la version B avec un répliqua (25% du trafic). Manifeste de déploiement tronqué version A: Manifeste de déploiement tronqué version B, notez que ous ne démarrons qu’un seul répliqua de l’application: Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/canary Avantages version publiée pour un sous-ensemble d’utilisateurs pratique pour la surveillance du taux d’erreur et des performances rollback rapide Inconvénients déploiement lent la répartition du trafic ajustée peut être coûteuse (99% A / 1% B = 99 pod A, 1 pod B) La procédure utilisée ci-dessus est native de Kubernetes, nous ajustons le nombre de répliques gérées par un ReplicaSet pour distribuer le trafic entre les versions. Si vous n’êtes pas sûr de l’impact que la sortie d’une nouvelle fonctionnalité pourrait avoir sur la stabilité de la plate-forme, une stratégie de sortie canari est suggérée.  A/B testing - idéal pour testes les fonctionnalités sur un sous-ensemble d’utilisateurs Le test A/B est en fait une technique permettant de prendre des décisions d’affaires basées sur des statistiques, plutôt qu’une stratégie de déploiement. Cependant, il est apparenté et peut être mis en oeuvre à l’aide d’un déploiement canari, c’est pourquoi nous en parlerons brièvement ici. En plus de répartir le trafic entre les versions en fonction du poids, vous pouvez cibler précisément un groupe donné d’utilisateurs en fonction de quelques paramètres (cookie, user agent, etc.). Cette technique est largement utilisée pour tester la conversion d’une fonctionnalité donnée et ne déployer que la version qui convient le plus. Istio, comme les autres maillages de service, fournis un moyen plus fin de subdiviser les instances de service avec un routage dynamique des requêtes basé sur des poids et/ou des en-têtes HTTP.  Vous trouverez ci-dessous un exemple d’installation de règles à l’aide d’Istio. Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/ab-testing D’autres outils comme Linkerd, Traefik, NGINX, HAProxy, vous permettent également de le faire. Avantages nécessite un équilibreur de charge intelligent plusieurs versions en parallèle un contrôle total sur la répartition du trafic Inconvénients les erreurs difficiles à dépanner pour une session donnée, le traçage distribué deviennent obligatoires pas simple, vous devez configurer des outils supplémentaires  En résumé Il y a différentes façons de déployer une application, lors de la mise en production dans un environnement de développement/staging, un déploiement big bang ou progressif est généralement un bon choix. Lorsqu’il s’agit de production, un déploiement progressif ou en blue/green est généralement un bon choix, mais un test approprié de la nouvelle plate-forme est nécessaire. Si vous n’êtes pas sûr de la stabilité de la plate-forme et de l’impact que pourrait avoir la sortie d’une nouvelle version de logiciel, alors une version canari devrait être la bonne solution. Ce faisant, vous laissez le consommateur tester l’application et son intégration à la plate-forme. Enfin, si votre entreprise a besoin de tester une nouvelle fonctionnalité parmi un groupe spécifique d’utilisateurs, par exemple, tous les utilisateurs accédant à l’application à l’aide d’un téléphone mobile sont envoyés à la version A, tous les utilisateurs accédant via un ordinateur passent à la version B. Vous pouvez alors utiliser la technique de test A/B qui en utilisant un réseau de services Kubernetes ou une configuration serveur personnalisée vous permet de déterminer où un utilisateur doit être dirigé en fonction de certains paramètres.","timeToRead":8,"frontmatter":{"title":"Stratégie de déploiement de Kubernetes","subtitle":"Dans Kubernetes, il y a plusieurs façons de publier une application, il est nécessaire de choisir la bonne stratégie pour rendre votre infrastructure fiable pendant la mise à jour d'une application.","tags":["Kubernetes","DevOps"],"date":"2019-05-25T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgT/xAAVAQEBAAAAAAAAAAAAAAAAAAABAP/aAAwDAQACEAMQAAABzC2jBYj/xAAaEAACAgMAAAAAAAAAAAAAAAABAgARITFC/9oACAEBAAEFAu8hQtzZANss/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BR//EABkQAAMAAwAAAAAAAAAAAAAAAAABERAxQf/aAAgBAQAGPwJXREcEs//EABgQAQEBAQEAAAAAAAAAAAAAAAERADFh/9oACAEBAAE/IRsrGWEVe4jVD66plnHI7Z2ZqM6Xf//aAAwDAQACAAMAAAAQwP8A/8QAFxEBAAMAAAAAAAAAAAAAAAAAAAERMf/aAAgBAwEBPxDZU//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQIBAT8QxIg//8QAHhABAAICAgMBAAAAAAAAAAAAAQAhEUExYXGBkaH/2gAIAQEAAT8QZGFr56O45sQgzWiXtrCEP6wYQCWcsZ1Hgda+xT8AxY17n//Z","aspectRatio":1.33422281521014,"src":"/static/cc32bd959fba19866af4a1302ca23593/8a760/interchange.jpg","srcSet":"/static/cc32bd959fba19866af4a1302ca23593/68709/interchange.jpg 310w,\n/static/cc32bd959fba19866af4a1302ca23593/53593/interchange.jpg 620w,\n/static/cc32bd959fba19866af4a1302ca23593/8a760/interchange.jpg 1240w,\n/static/cc32bd959fba19866af4a1302ca23593/0e6ff/interchange.jpg 1860w,\n/static/cc32bd959fba19866af4a1302ca23593/883ab/interchange.jpg 2000w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/k8s-deployement-strategies/"}}},{"node":{"excerpt":"Minikube est un outil idéal pour configuer localement Kubernetes afin de tester et d’expérimenter vos déploiements. Dans ce guide, je vais essayer de vous aider à le mettre en marche sur votre machine locale, à donner quelques conseils sur où et comment effectuer certaines tâches et à le rendre aussi capable (je suppose quand vous utilisez k8s que vous veulent apprendre et utiliser Helm, etcd, istio, etc.). Installation de minikube Minikube fonctionne avec une machine virtuelle. Pour cela, on peut utiliser diverses options en fonction de vos préférences et de votre système d’exploitation. Ma préférence dans ce cas est Orable VirtualBox. Vous pouvez utiliser brew pour tout installer: Dans ce cas, vous pourriez obtenir une erreur d’installation peu concluante liée à l’installation de virtualbox, en particulier sur Mojave et problablement par la suite. Quoi qu’il en soit, il s’agit problablement d’une nouvelle fonctionnalité de sécurité dans MacOs X qui vous gêne. Allez dans Préférences Système> Sécurité et confidentialité et sur l’écran Général, vous verrez un (ou quelques) messages concernant certains logiciels nécessitant une approbation pour être installés. Vous devez examiner attentivement la liste s’il en existe plusieurs et autoriser l’installation du logiciel dont vous avez besoin - dans ce cas, le logiciel Oracle. Cela fait, vous pourrez relancer la commande ci-dessus et vous devrez alors être prêt pour les étapes suivantes. Exécuter et accéder au cluster Afin d’utiliser de manière optimale les ressources de votre ordinateur local, je suggérerais de l’arrêter quand vous n’en avez plus besoin… Avec VirtualBox au centre, il utilisera la batterie de votre ordinateur portable assez rapidement. Recommencer plus tard vous ramènera là où vous l’avez laissé: Le tableau de bord Kubernetes est également disponible (lorsque minikube est en cours d’exécution): Je vais supposer que vous avez kubectl installé localement et que vous l’utilisez déjà pour certains clusters distants, vous disposez donc de plusieurs contextes. Dans ce cas, vous devez répertorier les contextes et passer à minikube. Vous vous trouvez maintenant dans le contexte de votre cluster k8 local qui s’exécute sur minikube et vous pouvez effectuer toutes les opérations k8 qu’il contient. Ingress Pour exécuter vos déploiements comportant un ingress, vous aurez besoin d’un add-on d’entrée: Assurez-vous que vous configurez l’ingress en fonction de vos hôtes locaux. Cela signifie fondamentalement que tout ce que vous définissez comme hôte dans vos règles d’ingress doit être configuré dans votre fichier /etc/hosts Où [minikube ip] devrait être remplacé par l’ip actuel de minikube. Il fonctionne également avec plusieurs hôtes locaux séparés par des espaces après l’adresse ip de minikube. Voici un raccourci pour le faire en bash Docker registry La réalité de l’utilisation réelle du registre de conteneurs dans l’environnement local est rude. Je vais donc vous fournir une option simple, rapide et simpliste qui facilite le déploiement de votre travail local sur vos k8s locaux, mais vous prive de l’expérience très importante du registre de conteneurs. Registre de conteneurs local Obtenez le contexte de votre docker local pour pointer vers le context minikube:  Dans le contexte minikube, pour démarrer le registre docker local Donc, vous avez maintenant un registre local dans lequel pousser des choses (tant que votre docker est dans le contexte minikube) Vous pouvez maintenant faire: A ce stade, vous pouvez utiliser localhost:5000/<your_tag>: comme image dans votre déploiement et c’est tout. Utilisation du référentiel de conteneur distant Pour utiliser localement le référentiel de conteneur distant, vous devez fournir un moyen d’authentification, qui se base sur les secrets de Kubernetes. Pour la gestion des secrets locaux pour ECR, GCR and Docker registry, je recommande d’utiliser l’addon minikube appelé registry-creds. Je ne le considère pas suffisamment sûr pour être utilisé ailleurs que dans l’environnement local. Helm Helm est un gestionnaire de paquets pour k8s et est souvent utilisé pour la gestion de la configuration d’un déploiement à l’autre. Compte tenu de la grande popularité de l’outil et de son adoption croissante, je voudrais terminer ce guide par une note sur l’ajout de helm à votre environnement Kubernetes local. C’est assez facile à ce stade, il suffit de mettre en place minikube et: Helm utilise un backend appelé Tiller. C’est ce qui est installé/déployé lors de l’exécution de helm init. Une lecture précieuse: https://helm.sh/docs/using_helm/ Maintenant, vous disposez d’un environnement Kubernetes local complet capable d’accepter tous vos déploiements de test avant de décider de les placer dans le cloud.","timeToRead":4,"frontmatter":{"title":"Configuration locale de Kubernetes avec minikube sur MacOS X","subtitle":"","tags":["Kubernetes","DevOps"],"date":"2019-05-23T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIFA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHN5DxbJYf/xAAaEAACAgMAAAAAAAAAAAAAAAAAAQIDEBET/9oACAEBAAEFAlaRsOmEbZ//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAYEAACAwAAAAAAAAAAAAAAAAAAMgEgMf/aAAgBAQAGPwJhpNp//8QAGhAAAwEAAwAAAAAAAAAAAAAAAAERITFhgf/aAAgBAQABPyHSUvSOU7FgtjtClyf/2gAMAwEAAgADAAAAEGQv/8QAFhEBAQEAAAAAAAAAAAAAAAAAABFB/9oACAEDAQE/EMV//8QAFREBAQAAAAAAAAAAAAAAAAAAEEH/2gAIAQIBAT8Qp//EABoQAQEAAwEBAAAAAAAAAAAAAAEAESExUZH/2gAIAQEAAT8QTFAXqtS5LFXZken2T1hAFInS/9k=","aspectRatio":1.7809439002671417,"src":"/static/aac12d8124ce6730c0247f5495bff894/8a760/ship.jpg","srcSet":"/static/aac12d8124ce6730c0247f5495bff894/68709/ship.jpg 310w,\n/static/aac12d8124ce6730c0247f5495bff894/53593/ship.jpg 620w,\n/static/aac12d8124ce6730c0247f5495bff894/8a760/ship.jpg 1240w,\n/static/aac12d8124ce6730c0247f5495bff894/0e6ff/ship.jpg 1860w,\n/static/aac12d8124ce6730c0247f5495bff894/883ab/ship.jpg 2000w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/k8s-minikube/"}}},{"node":{"excerpt":"La configuration, le CI et les flux de déploiement représente un peu l’ancien script bash. Malgré mon profond intérêt pour les subtilités de Bash (/sarcasme), j’ai continué à chercher des solution aux mêmes situations sur  Google et StackOverflow. Pour éviter d’avoir à le refaire moi-même et pour votre plaisir de lecture, les voici. Pour être dangereux en termes d’installation, de CI et de flux de déploiement, nous rencontrerons ce qui suit: azerty azerty Vérifier si un fichier existe Vérifier si un lien symbolique existe Vérifier si une variable d’environnement est définie Basculer une variable d’environnement Demander à l’utilisateur Un dernier conseil, s’il s’agit de plusieurs lignes, essayez d’utiliser quelque chose comme JavaScript ou Python pour écrire votre script. Injecter .env dans votre session/environnement Nous avons des fichiers .env, Docker Compose traite cela avec une utilisation habituelle, mais disons que nous voulons que quelque chose tourne en dehors de Docker (et sans utiliser quelque chose comme dotenv). Voici l’extrait de code pour un shell UNIX","timeToRead":2,"frontmatter":{"title":"Bash - Vérifier les variables d'environnement sont définies ou s'il existe des fichiers/références","subtitle":"","tags":["Bash","DevOps"],"date":"2019-05-21T22:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFak9W5wHg//8QAGxAAAQQDAAAAAAAAAAAAAAAAAQACAxESISL/2gAIAQEAAQUCKy6E1BuzHEyqC//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAQACAwEAAAAAAAAAAAAAAAABMRESIYH/2gAIAQEABj8C8Qi3WdVP/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAERITGR/9oACAEBAAE/IaNiOjRsui2rXBjEiRSTh//aAAwDAQACAAMAAAAQpN//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPxBX/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQIBAT8Qp//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRMUFhwf/aAAgBAQABPxDieh8j0AS9GPIJjMXQwMpryCyIpy61KNPs2wWhNAn/2Q==","aspectRatio":1.7777777777777777,"src":"/static/b9929b1a4915b5e4d241ad5af23c0901/8a760/servers.jpg","srcSet":"/static/b9929b1a4915b5e4d241ad5af23c0901/68709/servers.jpg 310w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/53593/servers.jpg 620w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/8a760/servers.jpg 1240w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg 1600w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/bash-justeAssezPourEtreDangereux/"}}},{"node":{"excerpt":"Cet article explique comment utiliser Travis CI pour surveiller les modifications apportées à la branche principale de notre référentiel GitHub . Nous allons configurer Travis CI de manière à ce que lors de la mise en place d’un nouveau commit pour notre application Angular, celui-ci: Exécuter le linter Lancer nos tests unitaires Lancer nos tests E2E Construire notre application pour la production Déployé vers GitHub Pages Définitions Test continu (CT) Les tests continus consistent à exécuter des tests automatisés dans le cadre du pipeline de livraison de logiciels pour obtenir un retour immédiat sur les risques commerciaux associés à une version candidate du lociel. Intégration Continue (CI) L’intégration continue est un ensemble de pratiques utilisées en génie logiciel consistant à vérifier à chaque modification de code source que le résultat des modifications ne produit pas de régression dans l’application développée. En pratique Ok, ces définitions sont bien. Mais, en pratique, qu’est-ce qu’ils signifient vraiment? Pour les besoins de cet article, je vais utiliser des définitions similaires, mais très pragmatiques et spécifiques: CT Nos tests sont exécutés automatiquement avant chaque déploiement. Et notre code ne sera pas déployé si des tests échouent. CI Transférer des modifications dans une branche spécifique de notre repo GitHub lance un processus de déploiement automatisé. Créer une application Angular Pour commencer, créez une nouvelle application Angular en utilisant Angular CLI. Aller sur: http://localhost:4200 GitHub Créer un nouveau repo GitHub: travis-demo Nous devons maintenant ajouter le repo GitHub en tant que distant de notre repo Git local. Vous devrez peut-être d’abord vous engager si vous avez effectué des mises à jour. Assurez-vous de remplacer ludovicwyffels par votre propre identifiant GitHub. Travis CI Inscrivez-vous à Travis CI en utilisant votre compte GitHub. Accédez à votre profil pour voir la liste des repos. Si vous avez déjà un compte, vous devrez utiliser le bouton Sync account pour que votre nouveau repo apparaisse dans la liste. De plus, vous devrez peut-être utiliser le filtre pour voir réellement votre repo si vous en avez beaucoup. Vous pouvez cliquer sur le bouton Paramètres à droite du repo. Ajouter une configuration Travis CI Afin de dire à Travis CI de faire quelque chose avec notre repo, nous devons ajouter un fichier de configuration Travis CI. Créez un fichier à la racine de votre espace de travail appelé .travis.yml avec le contenu suivant: Pour le --base-ref vous devrez utiliser votre propre identifiant GitHub. Ce fichier indique à Travis CI de procéder comme suit: Utilisez Node.js Surveillez uniquement la branche principale pour connaître les modifications. Avant d’exécuter l’un des scripts, installez Angular CLI Assurez-vous que l’application respecte nos règles en matière de linting utilisant ng lint. Construisez notre application en utilisant ng build. Après avoir ajouté le fichier de configuration Travis CI, commiter et pousser. Après avoir effectué cette opération, passez au tableau de bord Travis CI pour afficher le journal Travis CI au fur et à mesure de la création de votre application. Vous devrez peut-être cliquer sur l’onglet Current pour le voir se développer. À l’extrême droite du journal, vous pouvez cliquer sur Follow log pour faciliter l’affichage du journal mis à jour. Une fois que Travis CI a terminé notre script et créé l’application, vous verrez l’icône virer au vert pour vous informer que votre script s’est terminé avec succès. Test continu Maintenant que nous avons créé nos applications, allons dans le monde des tests continus en laissant Travis CI exécuter nos tests unitaires à chaque nouveau commit. Malheureusement, nous ne pouvons pas simplement npm run test sur Travis CI. En effet, nous ne pouvons pas réellement lancer une interface graphique de navigateur. Nous allons donc utiliser Headless Chrome pour exécuter nos tests unitaires. Suivez les instructions pour créer un nouveau script npm comme celui-ci: Nous pouvons maintenant exécuter nos tests unitaires en utilisant Headless Chrome comme ceci: Maintenant, ajouter npm run test-headless à notre fichier .travis.yml dans la section script comme ceci: Commiter et push sur GitHub: Après avoir pusher, passez au tableau de bord Travis CI pour regarder votre projet exécuter les tests unitaires, puis faire le build. Intégration continue Maintenant que nous testons et buildons, nous souhaitons déployer notre application. Nous devons créer un jeton d’accès personnel GitHub. Suivez ces instructions. Dans le tableau de bord Travis CI, accédez à votre projet. En haut à droite, cliquez sur Paramètres. Faites défiler jusqu’à la section Variables d’environnement. Ajoutez une nouvelle variable d’environnement nommée: GITHUB_TOKEN Pour la valeur, collez la valeur réelle de votre jeton d’accès personnel GitHub que vous avez généré. Cliquez sur Ajouter. Déploiement sur les pages GitHub Enfin, configurons Travis CI pour le déploiement sur GitHub Pages. Ajoutez une section de déploiement à votre .travis.yml. A ce stade, votre fichier devrait resembler à ceci: Encore une fois, assurez-vous que vous utilisez votre propre identifiant GitHub au lien de ludovicwyffels. Ceci indique à Travis CI d’utiliser notre jeton GitHub à partir de la variable d’environnement: GITHUB_TOKEN En outre, il spécifie que les fichiers à déployer proviennent de notre dossier dist: dist/travis-demo Engagez-vous et appuyez sur GitHub: Nous devrions également pouvoir accéder à l’URL de nos pages GitHub et voir notre application déployée.","timeToRead":5,"frontmatter":{"title":"Angular DevOps: CT/CI avec Travis CI et Github Pages","subtitle":"Utilisation de Travis CI pour implémenter les tests continus (CT) et l'intégration continue (CI) afin de déployer notre application Angular sur Github Pages","tags":["Angular","DevOps","Typescript","Node.js"],"date":"2019-04-29T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhElEQVQoz2P4/+/f/79/geQ/MPkfGfyDYIQgWBUUALkM/1HBPzBA6EUS/48BGH6/ePF1yfK7lTV3Fi76/uwZWB3I1I+fX57eu/TKqU0/fn0BciFWvXr1qrysLCQoaN3atSDNb1vaHylrPpk8bY2M/LWWtq9/fj9/+uz3r2+Xzi1c3puzeXH59Wubfv36/fr16z9//mRlZNRUV/f29MTFxGxYv57hfd+EV8npX67fOBafeHvCpMXr17c0tVy/cq63PK2hIru5OG3h3I7TZ06VlJRs2LAhODDwx48fQDvXrFmTnprK8HXn7rcZORe9/Y6mpL3ev//Ry5dXLl16/vzlhNbaSYVZrQkxi2dNe//h44EDBx4+fBgYELB7166fP34AGR3t7aAA+7F7783M7Kd790FCBhLmJ1as2RQcvy214O6JMyA///kDJPfs2bNkyZLr168nJSZ++vQJZ2j/+/Pn+4/vP//9QQ5tIPnz50+4GrBmYEhCEFbw7x9mhEH0M/ynAFCkGQDmsz6sxuzErwAAAABJRU5ErkJggg==","aspectRatio":1.7777777777777777,"src":"/static/1a448c2debbe7a75dd52ebe4c3d9c37e/e7595/ngTravisGit.png","srcSet":"/static/1a448c2debbe7a75dd52ebe4c3d9c37e/23f9c/ngTravisGit.png 310w,\n/static/1a448c2debbe7a75dd52ebe4c3d9c37e/391c3/ngTravisGit.png 620w,\n/static/1a448c2debbe7a75dd52ebe4c3d9c37e/e7595/ngTravisGit.png 1240w,\n/static/1a448c2debbe7a75dd52ebe4c3d9c37e/9ecf6/ngTravisGit.png 1600w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/angular-travis-ctci/"}}},{"node":{"excerpt":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent). Des cycles de publication de plusieurs mois ou années sont en train de devenir rares, en particulier parmi ceux qui construisent des produits logiciels purs. Aujourd’hui, en utilisant une approche axée sur les services et sur les microservices, les développeurs peuvent concevoir une base de code modulaire. Cela leur permet d’écrire et de déployer des modifications simultanément sur différentes parties de la base de code. Les avantages commerciaux de cycles de déploiement plus courts sont clairs: Le temps de mise sur le marché est réduit Les clients obtiennent la valeur du produit en moins de temps Les commentaires des clients sont également renvoyés plus rapidement dans l’équipe produit, ce qui permet à l’équipe de parcourir les fonctionnalités et de résoudre les problèmes plus rapidement. Le moral des développeurs augmente Cependant, ce changement crée également de nouveaux défis pour les opérations ou l’équipe de DevOps. Avec des déploiements plus fréquents, il est plus probable que le code déployé puisse affecter négativement la fiabilité du site ou l’expérience client. C’est pourquoi il est important de développer des stratégies de déploiement de code minimisant les risques pour le produit et les clients. Dans cet article, nous allons parler de différentes stratégies de déploiement, de meilleures pratiques et d’outils qui permettront à votre équipe de travailler plus rapidement et de manière plus fiable. Défis des applications modernes Les applications modernes sont souvent distribuées et basées sur le cloud. Ils peuvent évoluer pour répondre à la demande et sont plus résistants aux pannes grâce à des architectures hautement disponibles. Ils peuvent utiliser des services entièrement gérés comme AWS Lambda ou Elastic Container Service (ECS), où la plate-forme assume une partie de la responsabilité opérationnelle. Ces applications ont presque toujours des déploiements fréquents. Par exemple, une application mobile ou une application Web grand public peut subir plusieurs modifications au cours d’un mois. Certains sont même déployés en production plusieurs fois par jour. Ils utilisent souvent des architectures de microservices dans lesquelles plusieurs composants travaillent ensemble pour offrir une fonctionnalité complète. Il peut y avoir différents cycles de publication pour différents composants, mais ils doivent tous fonctionner ensemble de manière transparente. L’augmentation du nombre de pièces mobiles signifie plus de chances que quelque chose se passe mal. Avec plusieurs équipes de développement apportant des modifications dans la base de code, il peut être difficile de déterminer la cause première d’un problème inévitablement. Un autre défi: l’abstraction de la couche infrastructure, qui est maintenant considérée comme du code. Le déploiement d’une nouvelle application peut également nécessiter le déploiement d’un nouveau code d’infrastructure. Stratégies de déploiement populaires Pour relever ces défis, les équipes chargées des applications et de l’infrastructure doivent concevoir et adopter une stratégie de déploiement adaptée à leur cas d’utilisation. Nous en examinerons plusieurs et discuterons des avantages et des inconvénients de différentes stratégies de déploiement afin que vous puissiez choisir celle qui convient à votre organisation. Déploiement “Big Bang” Comme son nom l’indique, les déploiements “big bang” mettent à jour des parties entières ou volumineuses d’une application en un seul coup. Cette stratégie remonte à l’époque où le logiciel était publié sur un support physique et installé par le client. Les déploiements à grande échelle ont obligé l’entreprise à procéder à des développements et à des tests approfondis avant la publication, souvent associés au “modèle en cascade” de grandes versions séquentielles. Les applications modernes présentent l’avantage de se mettre à jour régulièrement et automatiquement, côté client ou côté serveur. Cela rend l’approche big bang plus lent et moins agile pour les équipes modernes. Les caractéristiques du déploiement du Big Bang comprennent: Toutes les pièces majeures conditionnées en un seul déploiement; Remplacer en grande partie ou totalement une version de logiciel existante par une nouvelle; Déploiement entraînant généralement de longs cycles de développement et de test; En supposant un risque minimal d’échec, les retours en arrière peuvent être impossibles ou peu pratiques; Les délais de réalisation sont généralement longs et peuvent nécessiter les efforts de plusieurs équipes; Action requise des clients pour mettre à jour l’installation côté client. Les déploiements à fort rendement ne conviennent pas aux applications modernes, car les risques sont inacceptables pour les applications grand public ou critiques pour les entreprises, où les pannes entraînent des pertes financières énormes. Les reculs sont souvent coûteux, prennent du temps, voire impossibles. L’approche big bang peut convenir à des systèmes hors production (par exemple, recréer un environnement de développement) ou à des solutions prêtes à l’emploi, tel que les applications de bureau. Déploiement progressif (Rolling) Les déploiements progressifs, par paliers ou par étapes sont préférables aux déploiements big bang, car ils minimisent de nombreux risques, y compris les temps d’arrêt pour les utilisateurs, sans restauration simple. Dans un déploiement en continu, la nouvelle version d’une application remplace progressivement l’ancienne. Le déploiement réel se produit sur une période de temps. Pendant ce temps, les nouvelles versions et les anciennes versions coexisteront sans affecter la fonctionnalité ou l’expérience utilisateur. Ce processus facilite la restauration de tout nouveau composant incompatible avec les anciens composants. Le diagramme suivant illustre le modèle de déploiement: l’ancienne version apparaît en bleu et la nouvelle version en vert sur chaque serveur du cluster.  Une mise à niveau d’une suite d’applications est un exemple de déploiement progressif. Si les applications d’origine ont été déployées dans des conteneurs, la mise à niveau peut traiter un conteneur à la fois. Chaque conteneur est modifié pour télécharger la dernière image à partir du site du fournisseur de l’application. S’il existe un problème de compatibilité pour l’une des applications, l’ancienne image peut recréer le conteneur. Dans ce cas, les nouvelles versions et les anciennes versions des applications de la suite coexistent jusqu’à ce que chaque application soit mise à niveau. Déploiement Blue-Green, Red-Black ou A/B C’est un autre processus à sécurité intégrée. Dans cette méthode, deux environnements de production identiques fonctionnent en parallèle. L’un est l’environnement de production en cours d’exécution recevant tout le trafic utilisateur (représenté en bleu). L’autre en est un clone, mais inactif (vert). Les deux utilisent la même base de données et la même configuration d’application:  La nouvelle version de l’application est déployée dans l’environnement vert et testée pour ses fonctionnalités et ses performances. Une fois les résultats des tests réussis, le trafic des applications passent du bleu au vert. Le vert devient alors la nouvelle production.  S’il y a un problème après que le vert devient actif, le trafic peut être redirigé vers le bleu. Dans un déploiement Blue-Green, les deux systèmes utilisent la même couche de persistance ou la même base de données. Il est essentiel de synchroniser les données de l’application, mais une base de données en miroir peut aider à atteindre cet objectif. Vous pouvez utiliser la base de données primaire en bleu pour les opérations d’écriture et la base secondaire en vert pour les opérations de lecture. Lors du passage du bleu au vert, la base de données est basculée du primaire au secondaire. Si le vert a également besoin d’écrire des données pendant le test, les bases de données peuvent être en réplication bidirectionnelle. Une fois que le vert devient actif, vous pouvez fermer ou recycler les anciennes instances bleues. Vous pouvez déployer une version plus récente sur ces instances et en faire le nouveau vert pour la prochaine version. Les déploiements Blue-Green reposent sur le routage du trafic. Cela peut être fait en mettant à jour les DNS (CNAMES) pour les hôtes. Cependant, des valeurs TTL longues peuvent retarder ces modifications. Vous pouvez également modifier les paramètres du load balancer pour que les modifications prennent effet immédiatement.  Déploiement canari Le déploiement canari ressemble à du Blue-Green, à la différence qu’il est moins enclin à prendre des risques. Au lieu de passer du bleu au vert en une seule étape, vous utilisez une approche progressive. Avec le déploiement canari, vous déployez un nouveau code d’application dans une petite partie de l’infrastructure de production. Une fois que l’application est validée pour la publication, seuls quelques utilisateurs y sont routés. Cela minimise tout impact. En l’absence d’erreur signalée, la nouvelle version peut progressivement être déployée dans le reste de l’infrastructure. L’image ci-dessous illustre le déploiement canari:  Le principal défi du déploiement canari est de trouver un moyen d’acheminer certains utilisateurs vers la nouvelle application. En outre, certaines applications peuvent toujours avoir besoin du même groupe d’utilisateurs pour les tests, tandis que d’autres peuvent nécessiter un groupe différent à chaque fois. Envisagez un moyen d’acheminer de nouveaux utilisateurs en explorant plusieurs techniques: Exposer les utilisateurs internes au déploiement canari avant d’autoriser l’accès des utilisateurs externes; Baser le routage sur la plage IP source; Livrer l’application dans des régions géographiques spécifiques; Utilisation d’une logique d’application pour déverrouiller de nouvelles fonctionnalités pour des utilisateurs et des groupes spécifiques. Cette logique est supprimée lorsque l’application est en ligne pour le reste des utilisateurs. Meilleures pratiques de déploiement Les équipes d’applications modernes peuvent suivre un certain nombre de meilleures pratiques pour minimiser les risques de déploiement: Utilisez une liste de déploiement. Par exemple, un élément de la liste peut consister à “sauvegarder toutes les bases de données uniquement après que les services d’application ont été arrêtés” afin d’éviter toute corruption des données. Adopter l’intégration continue (CI). CI s’assure que le code archivé dans la branche de fonctionnalité d’un référentiel de code ne fusionne avec sa branche principale qu’après une série de vérifications de dépendance, de tests d’unité et d’intégration et une génération réussit. S’il y a des erreurs le long du chemin, la construction échoue et l’équipe de l’application en est informée. L’utilisation de CI signifie donc que chaque modification apportée à l’application est testée avant son déploiement. Les exemples d’outils de CI incluent: CircleCI, Jenkins. Adoptez la livraison continue (CD). Avec le CD, l’artefact de code construit par le CI est packagé et toujours prêt à être déployé dans un ou plusieurs environnements. Utilisez des environnements d’exploitation standard pour assurer la cohérence de l’environnement. Vous pouvez utiliser des outils tels que Vagrant et Packer pour les postes de travail et les serveurs de développement. Utilisez les outils d’automatisation de build pour automatiser les générations d’environnement. Avec ces outils, il est souvent simple de cliquer sur un bouton pour détruire une pile d’infrastructure complète et la reconstruire à partir de zéro. CloudFormation est un exemple de tels outils. Utilisez des outils de gestion de configuration tels que Puppet, Chef ou Ansible sur les serveurs cibles pour appliquer automatiquement les paramètres du système d’exploitation, appliquer des correctifs ou installer des logiciels. Utilisez des canaux de communication tels que Slack pour les notifications automatisées des générations infructueuses et des échecs d’application. Créez un processus pour alerter l’équipe responsable des déploiements qui échouent. Dans l’idéal, vous les rencontrez dans l’environnement CI, mais si les modifications sont appliquées, vous aurez besoin d’un moyen d’avertir l’équipe responsable. Activez les restaurations automatisées pour les déploiements qui échouent aux vérifications de l’état, que ce soit en raison de problèmes de disponibilité ou de taux d’erreur. Surveillance post-déploiement Même après que vous ayez adopté toutes ces pratiques, il est possible que des choses se passent mal. Pour cette raison, la surveillance des problèmes survenant immédiatement après un déploiement est aussi importante que la planification et l’exécution d’un déploiement parfait. Un outil de surveillance des performances des applications peut aider votre équipe à surveiller les métriques de performances critiques, y compris les temps de réponse du serveur après les déploiements. Les modifications apportées à l’architecture des applications ou du système peuvent affecter considérablement les performances des applications. Une solution de surveillance des erreurs est également essentielle. Il informera rapidement votre équipe des erreurs nouvelles ou réactivées lors d’un déploiement est susceptibles de révéler des bogues importants nécessitant une intervention immédiate. Sans outil de surveillance des erreurs, les bogues n’auraient peut-être jamais été découverts. Alors que quelques utilisateurs rencontrant les bugs prendront le temps de les signaler, la plupart des autres ne le font pas. L’expérience client négative peut entraîner des problèmes de satisfaction au fil du temps ou, pire, empêcher les transactions commerciales d’avoir lieu. Un outil de surveillance des erreurs crée également une visibilité partagée de tous les problèmes de post-déploiement parmi les équipes Opérateur / DevOps et les développeurs. Cette compréhension partagée permet aux équipes d’être plus collaboratives et réactives.","timeToRead":9,"frontmatter":{"title":"Introduction aux stratégies de déploiement","subtitle":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent).","tags":["CICD","Deploy","DevOps"],"date":"2019-02-02T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFak9W5wHg//8QAGxAAAQQDAAAAAAAAAAAAAAAAAQACAxESISL/2gAIAQEAAQUCKy6E1BuzHEyqC//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAQACAwEAAAAAAAAAAAAAAAABMRESIYH/2gAIAQEABj8C8Qi3WdVP/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAERITGR/9oACAEBAAE/IaNiOjRsui2rXBjEiRSTh//aAAwDAQACAAMAAAAQpN//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPxBX/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQIBAT8Qp//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRMUFhwf/aAAgBAQABPxDieh8j0AS9GPIJjMXQwMpryCyIpy61KNPs2wWhNAn/2Q==","aspectRatio":1.7777777777777777,"src":"/static/b9929b1a4915b5e4d241ad5af23c0901/8a760/servers.jpg","srcSet":"/static/b9929b1a4915b5e4d241ad5af23c0901/68709/servers.jpg 310w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/53593/servers.jpg 620w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/8a760/servers.jpg 1240w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg 1600w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/deploy/"}}},{"node":{"excerpt":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser. Introduction Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif. Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. Docker Swarm et Kubernetes sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur. Création du conteneur J’ai décidé d’utiliser Samba pour l’application de test. Samba est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445. C’est la première fois que je travaille avec Docker, j’ai donc modifié un conteneur Samba standard afin d’inclure le fichier que je voulais servir. Après le tutoriel de Docker, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement: Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec smbclient Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur. Préparer les machines virtuelles J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox. J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler: https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png Ensuite, j’ai ajouté un serveur DHCP pour attribuer des adresses IP à chaque machine virtuelle: Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100. Docker Swarm Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes. Mise en place de Docker Swarm Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le tutoriel officiel, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle: C’est tout: le moteur Docker tourne maintenant en mode Swarm. Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les instructions d’installation: Déploiement de l’application Docker Swarm utilise le format Docker Compose pour spécifier les conteneurs à exécuter et les ports qu’ils exportent. Après le didacticiel Docker Compose, j’ai créé ce manifeste Docker Compose: Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur. Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé: Une fois le conteneur créé, l’application peut être déployée avec la commande docker stack deploy, en spécifiant le nom du service: Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec smbclient: Ajout d’un autre noeud Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm: Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire: Sur le nouveau noeud de travail, le nouveau conteneur est apparu: Test de l’équilibrage de charge (load balancing) Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm. Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec nc: Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe. Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100): Et sur le noeud travailleur (10.133.7.50): Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement. J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère). Kubernetes Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant. Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que minikube, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB. Mise en place de Kubernetes Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel Kubernetes the Hard Way Les développeurs de Kubernetes ont simplifié l’utilisation de kubeadm. Malheureusement, Kubernetes étant si flexible, le tutoriel sur kubeadm ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même. Voici ce que j’ai fini par lancer. J’ai d’abord dû désactiver Swap sur chaque noeud: Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante: L’option --pod-network-cidr attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes. Les options --apiserver-advertise-address et --apiserver-cert-extra-sans ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100. Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions: J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait! Après avoir enfin lu les instructions, je devais faire trois autres choses: Tout d’abord, je devais exécuter les commandes données par kubeadm pour configurer un fichier de configuration. Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le tutoriel m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud: Enfin, je devais choisir un réseau pour mon cluster. Installation du réseau Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge. Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et cet article comparatif suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du didacticiel kubeadm pour appliquer la configuration de WeaveNet: Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune. Pour installer MetalLB, j’ai suivi son didacticiel de mise en route et j’ai tout d’abord déployé MetalLB: Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant ce fichier de configuration: Déploiement de l’application Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité. Après avoir lu le tutoriel de Kubernetes, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement. https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447 Ce service demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge. https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8 Cet objet Deployment indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer. Notez le replicas: 1 - c’est le nombre d’instances du conteneur que je veux exécuter. Je peux déployer ce service sur Kubernetes en utilisant kubectl apply: Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner: Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB: Ajout d’un autre noeud Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par kubeadm sur le nouvel ordinateur: Bizarreries de ma configuration J’ai dû faire deux changements en raison de la configuration de VirtualBox: Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon ce problème, je devais éditer Et changer une ligne en avant de redémarrer Kubernetes: L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de ssh: Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine. En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder. “Scaling” de l’application Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application: Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de CreatingContainer et a commencé à s’exécuter: Test de l’équilibrage de charge J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant. Master: Worker: Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi. Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours. Comparaison et conclusion Fonctionnalités communes aux deux: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale. Les atouts de Docker Swarm: une configuration simple, aucune configuration requise, une intégration étroite avec Docker. Les points forts de Kubernetes: composants souples, nombreuses ressources disponibles et add-ons. Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité. J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais tout désactiver en même temps. Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un tableau de bord sophistiqué. Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser minikube, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise. Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile. En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes. Ce que j’ai appris Comment travailler avec les conteneurs Docker Comment configurer un cluster Docker Swarm à deux noeuds Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP Comment déployer une application sur Docker Swarm et Kubernetes Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98 Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent","timeToRead":16,"frontmatter":{"title":"Docker Swarm vs Kubernetes","subtitle":"J'ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser.","tags":["Kubernetes","Docker","DevOps"],"date":"2018-11-01T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB6OtBGFa2/8QAGhABAAIDAQAAAAAAAAAAAAAAAQACAxESE//aAAgBAQABBQJyW7vkSe1iaNoIBU//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Bia//xAAZEQABBQAAAAAAAAAAAAAAAAAAAQIRElH/2gAIAQIBAT8BlCzcP//EABoQAAMBAAMAAAAAAAAAAAAAAAABESESMUH/2gAIAQEABj8CU6MU30k5FhGRKH//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhQTFRYf/aAAgBAQABPyHjwtkhQI2lwNVg2E1C5WkTplYQ8n//2gAMAwEAAgADAAAAEAjv/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhYf/aAAgBAwEBPxAQO3S//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAgEBPxDsOTD/xAAaEAEBAQADAQAAAAAAAAAAAAABEQAhMUFx/9oACAEBAAE/EEhJlEVOtxTFYSS051ExT5vZ7lQoemZMl9hTc8RsE3//2Q==","aspectRatio":1.7772511848341233,"src":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg","srcSet":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/68709/dockerswarm-vs-kubernetes.jpg 310w,\n/static/6dc1a1b98e66073dbd7e7471a7fff24c/53593/dockerswarm-vs-kubernetes.jpg 620w,\n/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg 750w","sizes":"(max-width: 750px) 100vw, 750px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/dockerSwarm-vs-kubernetes/"}}},{"node":{"excerpt":"Les conteneurs d’applications ont émergé comme un outil puissant dans le développement de logiciels modernes. Plus légers et plus économes en ressources que les machines virtuelles traditionnelles, les conteneurs offrent aux organisations informatiques de nouvelles opportunités dans le contrôle de version, le déploiement, la graduation (scaling) et la sécurité. Ce post traitera de ce que sont exactement les conteneurs, pourquoi ils se révèlent si avantageux, comment les utilisateurs les utilisent et des meilleures pratiques pour conteneuriser vos applications Node.js avec Docker. Qu’est-ce qu’un conteneur ? En termes simples, les conteneurs exécutent des instances d’ images de conteneur. Les images sont des alternatives en couches aux disques de machines virtuelles (VMD) qui permettent aux applications d’être extraites de l’environnement dans lequel elles sont réellement exécutées. Les images conteneur sont exécutables, logiciel isolé avec accès aux ressources de l’hôte, au réseau et au système de fichiers. Ces images sont créées avec leurs propres outils système, bibliothèques, code, environnement d’exécution et dépendances associées codées en dur. Cela permet aux conteneurs d’être filés indépendamment de l’environnement. Cette approche “tout-en-un” aide à résoudre les problèmes d’application indépendante, en améliorant la sécurité des systèmes et en facilitant le débogage. Contrairement aux machines virtuelles traditionnelles, les images de conteneur donnent à chacune de ses instances un accès partagé au système d’exploitation hôte via une exécution de conteneur. Cet accès partagé aux ressources du système hôte permet d’obtenir des performances et des ressources qui ne sont pas trouvées dans d’autres méthodes de virtualisation.  Imaginez une image de conteneur qui nécessite 500 Mo. Dans un environnement conteneurisé, ces 500 Mo peuvent être partagés entre des centaines de conteneurs en supposant qu’ils fonctionnent tous sur la même image de base. D’un autre côté, les machines virtuelles auraient besoin de 500 Mo par machine virtuelle . Cela rend les conteneurs beaucoup plus adaptés aux horizontal scaling et aux environnements restreints aux ressources. Pourquoi les conteneurs d’applications ? La légèreté et la reproductibilité des conteneurs en ont fait une option de plus en plus utilisée par les entreprises souhaitant développer des applications logicielles évolutives, hautement disponibles et contrôlées par les versions. Les conteneurs offrent plusieurs avantages clés aux développeurs: Léger et économe en ressources. Par rapport aux machines virtuelles, qui génèrent des copies de leur système d’exploitation hôte pour chaque application ou processus, les conteneurs ont nettement moins d’impact sur la mémoire, l’utilisation du processeur et l’espace disque. Immuable. Les conteneurs sont générés à partir d’une seule source vérifiée, une image. Si des modifications sont validées sur une image, une nouvelle image est créée . Cela rend les changements d’image de conteneur faciles à suivre et les restaurations de déploiement intuitives. La reproductibilité et la stabilité des conteneurs permettent aux équipes de développement d’éviter la dérive de la configuration, ce qui simplifie considérablement le test de version et la mise en miroir des environnements de développement et de production. Portable. La nature isolée et autonome des conteneurs en fait un excellent choix pour les applications qui doivent fonctionner sur une multitude de services, de plates-formes et d’environnements. Ils peuvent fonctionner sous Linux, Windows et macOS. Fournissez-les depuis le cloud, sur site ou là où votre infrastructure l’exige. Évolutif et hautement disponible. Les conteneurs sont facilement reproductibles et peuvent répondre dynamiquement aux demandes de trafic, grâce aux services d’orchestration tels que les instances de conteneur Azure, Google Cloud Engine et Amazon ECS, qui facilitent et accélèrent la création ou le retrait de conteneurs de votre infrastructure. Cas d’utilisation de conteneurs d’applications Toutes les applications et les organisations n’auront pas les mêmes besoins en infrastructure. Les avantages susmentionnés des conteneurs les rendent particulièrement aptes à répondre aux besoins suivant: DEVOPS ORGANISATIONS Pour les équipes qui travaillent à pratiquer «l’infrastructure comme code» et qui cherchent à adopter le paradigme DevOps, les conteneurs offrent des opportunités inégalées. Leur portabilité, leur résistance à la dérive de configuration et leur temps de démarrage rapide font des conteneurs un excellent outil pour tester rapidement et de manière reproductible différents environnements de code, indépendamment de la machine ou de l’emplacement. MICROSERVICE ET ARCHITECTURES DISTRIBUÉES Une expression courante dans le développement de microservices est «faites une chose et faites-le bien», et cela s’harmonise étroitement avec les conteneurs d’applications. Les conteneurs offrent un excellent moyen d’envelopper les microservices et de les isoler de l’environnement d’application plus large. Ceci est très utile lorsque vous souhaitez mettre à jour des (micro-) services spécifiques d’une suite d’applications sans mettre à jour l’ensemble de l’application. TESTS A / B Les conteneurs facilitent le déploiement de plusieurs versions de la même application. Lorsqu’ils sont associés à des déploiements incrémentaux , les conteneurs peuvent maintenir votre application dans un état dynamique et réactif aux tests. Voulez-vous tester une nouvelle fonctionnalité de performance? Faites défiler un nouveau conteneur, ajoutez des mises à jour, acheminez 1% du trafic vers celui-ci et collectez des commentaires sur les utilisateurs et les performances. À mesure que les changements se stabilisent et que votre équipe décide de l’appliquer à l’application en général, les conteneurs peuvent rendre cette transition fluide et efficace. Conteneurs et Node.js En raison de la pertinence des conteneurs d’applications pour les environnements d’application ciblés, Node.js est sans doute le meilleur moteur d’exécution pour la conteneurisation. Dépendances explicites. Les applications Node.js conteneurisées peuvent verrouiller les arborescences de dépendance et gérer des fichiers stables package.json , package-lock.json ou npm-shrinkwrap.json . Démarrage rapide et redémarrage. Les conteneurs sont légers et démarrent rapidement, ce qui en fait une paire stratégique pour les applications Node.js. L’une des fonctionnalités les plus appréciées de Node.js est son temps de démarrage impressionnant. Cette performance de démarrage robuste obtient des processus terminés redémarrés rapidement et les applications stabilisées; La conteneurisation fournit une solution évolutive pour maintenir cette performance. Scaling au niveau du processus. Semblable à la meilleure pratique de Node.js consistant à faire tourner plus de processus au lieu de plus de threads, un environnement conteneurisé augmentera le nombre de processus en augmentant le nombre de conteneurs. Cet horizontal scaling crée de la redondance et permet de maintenir les applications hautement disponibles, sans le coût significatif des ressources d’une nouvelle machine virtuelle par processus. Dockeriser votre application Node.js Présentation de Docker Docker est un système de fichiers en couches pour l’envoi d’images, et permet aux organisations d’abstraire leurs applications de leur infrastructure. Avec Docker, les images sont générées via un fichier Docker. Ce fichier fournit des configurations et des commandes pour générer des images par programme. Chaque commande Docker dans un Dockerfile ajoute un ‘calque’. Plus il y a de couches, plus le conteneur résultant est grand. Voici un exemple simple de Dockerfile:  La commande FROM désigne l’image de base qui sera utilisée; dans ce cas, il s’agit de l’image de la ligne de lancement Node.js 8 LTS. La commande RUN prend les commandes bash comme arguments. Dans la ligne 2, nous créons un répertoire pour placer l’application Node.js. La ligne 3 indique à Docker que le répertoire de travail pour chaque commande après la ligne 3 sera le répertoire de l’application. La ligne 5 copie tout le répertoire courant dans le répertoire courant de l’image, qui est /home/nodejs/app précédemment défini par la commande WORKDIR de manière similaire. Sur la ligne 6, nous installons l’installation de production. Enfin, sur la ligne 8, nous transmettons à Docker une commande et un argument pour exécuter l’application Node.js à l’intérieur du conteneur. L’exemple ci-dessus fournit un Dockerfile de base, mais finalement problématique. Dans la section suivante, nous examinerons certaines des meilleures pratiques de Dockerfile pour exécuter Node.js en production. Meilleures pratiques Dockerfile NE PAS EXÉCUTER L’APPLICATION EN TANT QUE ROOT Assurez-vous que l’application exécutée dans le conteneur Docker n’est pas exécutée en tant que root.  Dans l’exemple ci-dessus, quelques lignes de code ont été ajoutées à l’exemple Dockerfile d’origine pour extraire l’image de la dernière version LTS de Node.js, ainsi que pour ajouter et définir un nouvel utilisateur, nodejs . De cette façon, dans le cas où une vulnérabilité de l’application est exploitée, et que quelqu’un réussit à entrer dans le conteneur au niveau du système, au mieux, ce sont des utilisateurs nodejs qui n’ont pas d’autorisations root et n’existent pas sur l’hôte. CACHE NODE_MODULES Docker construit chaque ligne d’un Dockerfile individuellement. Cela forme les “couches” de l’image Docker. Au fur et à mesure qu’une image est construite, Docker met en cache chaque couche.  Sur la ligne 9 du Dockerfile ci-dessus, le fichier package.json est copié dans le répertoire de travail établi sur la ligne 7. Après le npm install sur la ligne 10, la ligne 11 copie le répertoire courant dans le répertoire de travail (l’image). Si aucune modification n’est apportée à votre package.json , Docker ne reconstruira pas la couche d’image npm install, ce qui peut considérablement améliorer les temps de construction. CONFIGUREZ VOTRE ENVIRONNEMENT Il est important de définir explicitement les variables d’environnement que votre application Node.js s’attend à rester constante tout au long du cycle de vie du conteneur.  DockerHub fournit une ressource centralisée pour la découverte d’images de conteneur, la distribution et la gestion des modifications, la collaboration entre utilisateurs et équipes et l’automatisation du flux de travail tout au long du pipeline de développement’.\nPour associer la CLI Docker à votre compte DockerHub, utilisez la docker login  docker login [OPTIONS] [SERVER] : docker login [OPTIONS] [SERVER] COMPTES GITHUB PRIVÉS ET MODULES NPM Docker exécute ses builds à l’intérieur d’un sandbox et cet environnement sandbox n’a pas accès à des informations telles que les clés ssh ou les informations d’identification npm. Pour contourner cette contrainte, quelques options recommandées sont disponibles pour les développeurs: Stocker les clés et les informations d’identification sur le système CI / CD. Les problèmes de sécurité d’avoir des informations d’identification sensibles à l’intérieur de la construction Docker peuvent être évités entièrement en ne les mettant jamais là en premier lieu. Au lieu de cela, stockez-les et récupérez-les dans le système CI / CD de votre infrastructure, et copiez manuellement les dépendances privées dans l’image. Utilisez un serveur npm interne. À l’aide d’un outil comme Verdaccio, configurez un proxy npm qui garde le flux des modules internes et des informations d’ identification privées . SOYEZ EXPLICITE AVEC LES TAGS Les balises aident à différencier les différentes versions d’images. Les balises peuvent être utilisées pour identifier les builds, les équipes qui travaillent sur l’image et littéralement toute autre désignation utile à une organisation pour gérer le développement des images. Si aucune balise n’est ajoutée explicitement, Docker assignera une balise par défaut à latest après la docker build. En tant que tag, le développement est le latest en date, mais peut être très problématique dans les environnements de production et de mise en scène. Pour éviter les problèmes latest , soyez explicites avec vos balises de construction. Voici un exemple de script attribuant des balises avec des variables d’environnement pour le git sha, le nom de la branche et le numéro de build de la construction, tous les trois pouvant être très utiles dans la gestion des versions, du débogage et du déploiement:  En savoir plus sur le marquage ici. Conteneurs et gestion des processus Les conteneurs sont conçus pour être légers et bien adaptés au niveau du processus, ce qui contribue à simplifier la gestion des processus: si le processus se termine, le conteneur se ferme. Cependant, cette cartographie 1:1 est une idéalisation qui n’est pas toujours maintenue dans la pratique. Comme les conteneurs Docker ne sont pas fournis avec un gestionnaire de processus, ajoutez un outil pour une gestion simple des processus. dumb-init de Yelp est un superviseur de processus simple et léger et un système d’initialisation conçu pour fonctionner en tant que PID 1 dans des environnements de conteneur. Cette désignation PID 1 pour le processus dumb-init est normalement affectée à un conteneur Linux en cours d’exécution et possède ses propres idiosyncrasies de signalisation du noyau qui compliquent la gestion des processus. Dumb-init fournit un niveau d’abstraction qui lui permet d’agir comme un proxy de signal, assurant le comportement attendu du processus. Ce qu’il faut inclure dans vos conteneurs d’applications Le principal avantage des conteneurs est qu’ils ne fournissent que ce qui est nécessaire. Gardez cela à l’esprit lorsque vous ajoutez des calques à vos images. Voici une liste de contrôle à inclure lors de la création d’images de conteneur: Votre code d’application et ses dépendances. Variables d’environnement nécessaires. Un proxy de signal simple pour la gestion de processus, comme dumb-init.\nC’est tout. Conclusion Les conteneurs sont une solution de virtualisation moderne mieux adaptée aux infrastructures nécessitant un partage efficace des ressources, des délais de démarrage rapides et une mise à l’échelle rapide. Les organisations DevOps utilisent des conteneurs d’applications pour implémenter ‘l’infrastructure en tant que code’, des équipes développant des microservices et s’appuyant sur des architectures distribuées et des groupes d’assurance qualité (QA) tirant parti de stratégies telles que les tests A/B et les déploiements incrémentaux en production. Tout comme l’approche recommandée pour Node.js monothread est 1 processus: 1 application, la meilleure pratique pour les conteneurs d’applications est 1 processus: 1 conteneur. Cette relation en miroir fait sans doute de Node.js l’environnement d’exécution le plus approprié pour le développement de conteneurs. Docker est une plate-forme ouverte pour le développement, l’expédition et l’exécution d’applications conteneurisées. Docker vous permet de séparer vos applications de votre infrastructure afin de pouvoir livrer des logiciels rapidement. Lorsque vous utilisez Docker avec Node.js, gardez à l’esprit: Ne lancez pas l’application en tant que root Cache node_modules Utilisez votre système CI / CD ou un serveur interne pour conserver les informations d’identification sensibles hors de l’image du conteneur Soyez explicite avec les balises de construction Gardez les récipients légers !","timeToRead":14,"frontmatter":{"title":"Conteneuriser des applications Node.js avec Docker","subtitle":"La légèreté et la reproductibilité des conteneurs en ont fait une option de plus en plus utilisée par les entreprises souhaitant développer des applications logicielles évolutives, hautement disponibles et contrôlées par les versions.","tags":["Docker","Node.js","Javascript","DevOps"],"date":"2018-05-11T08:00:00.000Z","image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgX/xAAVAQEBAAAAAAAAAAAAAAAAAAACA//aAAwDAQACEAMQAAAB01MmaaUEf//EABkQAAIDAQAAAAAAAAAAAAAAAAECABETEv/aAAgBAQABBQIuKDct1cwW8ReQn//EABcRAQADAAAAAAAAAAAAAAAAAAACERL/2gAIAQMBAT8BqLL/xAAYEQACAwAAAAAAAAAAAAAAAAAAAQIREv/aAAgBAgEBPwFuVmmf/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAEQMSFBYf/aAAgBAQAGPwLsYNlstn//xAAaEAEAAwADAAAAAAAAAAAAAAABABEhgbHx/9oACAEBAAE/IcACCRgtUqAWHkxa23U9Sf/aAAwDAQACAAMAAAAQtP8A/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARUf/aAAgBAwEBPxAAxkdv/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAERMVH/2gAIAQIBAT8QYkqFgf/EABwQAQADAAIDAAAAAAAAAAAAAAEAESFBcTGRsf/aAAgBAQABPxA9p8EzIu7rChBd+Q3XwtSb7+wiB2HEr0inD1H/2Q==","aspectRatio":1.5337423312883436,"src":"/static/aa737b43d3d846778b615068a6705bf0/8a760/container.jpg","srcSet":"/static/aa737b43d3d846778b615068a6705bf0/68709/container.jpg 310w,\n/static/aa737b43d3d846778b615068a6705bf0/53593/container.jpg 620w,\n/static/aa737b43d3d846778b615068a6705bf0/8a760/container.jpg 1240w,\n/static/aa737b43d3d846778b615068a6705bf0/0e6ff/container.jpg 1860w,\n/static/aa737b43d3d846778b615068a6705bf0/883ab/container.jpg 2000w","sizes":"(max-width: 1240px) 100vw, 1240px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/docker-nodejs/"}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"tag":"DevOps","tagURL":"dev-ops","limit":9,"skip":0,"numPages":1,"currentPage":1}}}
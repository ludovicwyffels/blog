{"componentChunkName":"component---src-templates-author-tsx","path":"/author/ludo/page/4","webpackCompilationHash":"28b74a5dec589e4586df","result":{"data":{"authorYaml":{"id":"ludo","name":"Wyffels Ludovic","website":"https://ludovicwyffels.github.io","twitter":"WYFFELSLudovic","bio":"Développeur senior. Fullstack + DevOps","facebook":"ludovicwyffels","location":"Comines, Belgique","profile_image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAABMUlEQVQ4y2MwsXf5T03MMGogGBvbOYNpUwdXOIbJwdgwNQQNhCk0snX6r2thC8Y6QGxg7QCWA/FBcrgMxepCkAZrV6//EYmpYBydkvHfMzjiv5mj2//whJT/Nm7ecEMJGgjykp6V/f+AqLj/O/Ye+L8diA8dO/m/pbv/v72n3/+tu/f9D45N/K9naYcSFESFoRXQlSCX2rh7g/kgr1u5epIWhiAMCi/3wLD/fVNn/u+fNgtMT5g+6//kmXOB/Jn/PYLCwWrINnDijDlAPBvKJ8NAZC+DIgCkGYRBbJAYSV5GjpTte/b/PwiMkLj0nP8xqVn/Dxw9AY6kwOh40iIFlmzCgUkmKjnjvx0wdu08fMFskJi1mxfxyQY9YetAEzJyQic5YaNnPeTgIDnrjZaHJGMACtTMXoVAJ6sAAAAASUVORK5CYII=","aspectRatio":1,"src":"/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png","srcSet":"/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png 400w","sizes":"(max-width: 400px) 100vw, 400px"}}},"avatar":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAABMUlEQVQ4y2MwsXf5T03MMGogGBvbOYNpUwdXOIbJwdgwNQQNhCk0snX6r2thC8Y6QGxg7QCWA/FBcrgMxepCkAZrV6//EYmpYBydkvHfMzjiv5mj2//whJT/Nm7ecEMJGgjykp6V/f+AqLj/O/Ye+L8diA8dO/m/pbv/v72n3/+tu/f9D45N/K9naYcSFESFoRXQlSCX2rh7g/kgr1u5epIWhiAMCi/3wLD/fVNn/u+fNgtMT5g+6//kmXOB/Jn/PYLCwWrINnDijDlAPBvKJ8NAZC+DIgCkGYRBbJAYSV5GjpTte/b/PwiMkLj0nP8xqVn/Dxw9AY6kwOh40iIFlmzCgUkmKjnjvx0wdu08fMFskJi1mxfxyQY9YetAEzJyQic5YaNnPeTgIDnrjZaHJGMACtTMXoVAJ6sAAAAASUVORK5CYII=","aspectRatio":1,"src":"/static/5f2c129e42248a92c87b13b4293950cf/7c0ed/ghost.png","srcSet":"/static/5f2c129e42248a92c87b13b4293950cf/09f8c/ghost.png 50w,\n/static/5f2c129e42248a92c87b13b4293950cf/bf65b/ghost.png 100w,\n/static/5f2c129e42248a92c87b13b4293950cf/7c0ed/ghost.png 200w,\n/static/5f2c129e42248a92c87b13b4293950cf/fdbb0/ghost.png 300w,\n/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png 400w","sizes":"(max-width: 200px) 100vw, 200px"}}}},"allMarkdownRemark":{"totalCount":38,"edges":[{"node":{"excerpt":"J’ai récemment commencé à utiliser OpenVPN et à profiter des différentes options qu’il propose, notamment le masquage du trafic VPN sous TCP:443 (autrement appelé HTTPS). Personnellement, j’aime utiliser GCE pour héberger mes VPN, car je profite de la puissance et de la personnalisation que cela procure. Cependant, il n’y avait absolument aucun tutoriel sur la configuration d’OpenVPN sur GCE. Il existe 10 étapes dans ce didacticiel, mais chaque étape peut exécuter plusieurs lignes de code. Si vous essayez simplement d’ajouter plus de clients, passer aux étapes 4, 9 et 10. Table des matières [toc] 1. Création de l’instance Nous voulons utiliser une instance n1-standard-1 pour sa durabilité et son coût. Dirigez-vous vers le moteur de calcul et cliquez sur “CREATE INSTANCE”.  Configurez-le avec les options affichées, mais n’appuyez pas encore sur “Créer” Si vous ne voyez pas l’image, nous utilisons une instance n1-standard-1 (1 vCPU, 3,75 Go de mémoire) avec Ubuntu 16.04 LTS installé (vous pouvez choisir un espace de stockage, mais 10 Go suffisent). Puisque notre VPN utilisera TCP: 443, nous devons autoriser le trafic HTTPS maintenant ou plus tard. Vous pouvez donc également le faire immédiatement. Appuyez sur le bouton “Gestion, sécurité, disques, réseau, …” pour ouvrir les options avancées. Cliquez sur l’onglet Réseau et cliquez sur le bouton d’édition en regard de l’interface réseau sélectionnée. Cliquez sur le menu déroulant “Adresse IP externe” et sélectionnez “Créer une adresse IP”. Entrez un nom, puis appuyez sur “Réserver”. Cette opération associera une adresse IP statique à votre instance, ce qui est probablement ce que vous voulez, sauf si vous souhaitez régénérer les fichiers de configuration de quelque temps. En ce qui concerne les frais, Google ne facture pas les adresses IP statiques pendant leur utilisation. La machine n1-standard-1 ne devrait pas vous coûter plus de 30 $ par mois. Certes, cela coûte un peu plus cher que la plupart des services VPN, mais la plupart des services VPN ne vous donnent pas un contrôle total :)  Vos interfaces réseau doivent ressembler à l’image. Veillez à activer le transfert IP, car cela ne peut plus être modifié après la création de l’instance. En règle générale, vous souhaiterez probablement garder votre adresse IP externe secrète. Cela ne me dérange pas de rendre celui-ci public, car cette instance n’existera pas lorsque vous lirez ce tutoriel, et l’adresse IP appartiendra probablement à quelqu’un d’autre. Appuyez sur “Terminé” et “Créer”. Une fois l’instance prête (un bouton “SSH” noir apparaîtra), cliquez ou double-cliquez sur le bonton “SSH” pour ouvrir une fenêtre SSH. Le reste de la configuration se fera sur cette machine, que nous appellerons le “server”. 2. Installation OpenVPN et EasyRSA OpenVPN est (évidemment) le serveur VPN que nous utilisons, et EasyRSA est un paquet qui nous permettra de configurer une autorité de certification interne à utiliser. Bien que ce ne soit généralement pas le cas, pour cette configuration, il est souvent nécessaire de lancer une mise à jour d’apt pour pouvoir installer un ou les deux packages. Les deux d’entre eux sont dans les dépôts par défaut d’Ubuntu. Temps de configurer. 3. Mise en place du CA Comme OpenVPN utilise TLS/SSL, il a besoin de certificats pour chiffrer le trafic. Pour cela, nous devrons émettre nos propres certificats de confiance, ce que nous pouvons faire à l’aide de la CA que nous sommes sur le point de configurer. Commençons par copier le easy-rsa de modèles du paquet easy-rsa. Il y a des valeurs que nous pouvons vouloir éditer, alors allons-y et nano vars. Si vous voulez utiliser vim, continuez, mais puisque j’essaie de garder mes tutoriels aussi simples que possible, j’utilise nano. À partir de ce moment, s’il y a une partie d’une commande qui dépend d’une valeur que vous choisissez, elle sera en gras. Tout le code qui n’est pas généré par un utilisateur sera en italique. Faites défiler vers le bas du fichier (ce n’est pas trop long) et vous devriez trouver ce qui suit: Éditez autant d’entre eux (ou aucun d’eux) que vous le souhaitez. Juste en dessous de ces valeurs devrait être export KEY_NAME=\"EasyRSA\". Pour simplifier notre configuration, nous allons modifier cette ligne comme suit: Enregistrez et fermez le fichier (ctrl + x, y, entrez). Les variables que nous venons de modifier seront utilisées pour le processus de signature de notre autorité de certification. Toujours dans le openvpn-ca, exécutez la commande suivante: Si vous avez bien fait, le re NOTE: If you run ./clean-all, I will be doing a rm -rf on /home/username/openvpn-ca/keys. C’est ce que nous voulons. Pour assurer un environnement de travail propre, nous allons lancer ./clean-all. Maintenant, construisons le CA Toutes les variables que nous avons déjà définies doivent se renseigner elles-mêmes. Il suffit donc d’y accéder en appuyant plusieurs fois sur Entrée. À la fin, vous aurez une autorité de certification prête à commencer à signer. Nous avons également besoin d’un certificat de serveur et d’une clé de cryptage pour garantir la sécurité de notre trafic. Créons notre certificat de serveur et notre clé. Passer à nouveau les invites de commande. Nous n’entrerons pas un mot de passe, cette fois. Les deux dernières invites vous obligent à entrer y pour signer le certificat. Assurez-vous de ne pas les ignorer! Comme indiqué précédemment, nous avons également besoin d’une clé de chiffrement. Pour les besoins de ce didacticiel, nous allons générer une clé Diffie-Hellman, qui a tendance à être plutôt forte. Bien sûr, une grande force entraîne une grande inefficacité. Par conséquent, quelle que soit votre système, la commande suivante prendra probablement quelques minutes. Nous renforcerons cela avec une signature HMAC, afin de nous assurer que la vérification de l’intégrité TLS est plus sûre. 4. Générer un certificat client Naturellement, si vous allez utiliser une autorité de certification, votre client doit également disposer d’un certificat. Bien que vous puissiez le faire sur votre ordinateur client et que le serveur le signe, nous essayons de garder les choses simples et hébergées sur un seul ordinateur. Si vous avez plusieurs clients, vous pouvez suivre cette étape plusieurs fois. Assurez-vous simplement de rendre vos noms de client uniques. Assurez-vous que vous êtes dans le openvpn-ca et que votre fichier vars est synchronisé. Maintenant, nous allons construire une clé client en tant que telle: Une fois de plus, tout doit être pré-rempli. Passez donc tous les éléments, sauf les deux dernières invites, qui vous demanderont de signer en entrant y. 5. Configurer le serveur OpenVPN OpenVPN s’installe sous le répertoire /etc/openvpn. Pour que tout fonctionne, nous devons déplacer certains fichiers dans ce dossier. Par défaut, OpenVPN est fourni avec un exemple de configuration. Par souci de simplicité, nous simplifierons la décompression dans notre dossier de configuration. Ensuite, nous allons éditer le fichier de configuration: La première étape consiste à trouver la directive tls-auth. Il y aura un point-virgule (;) à côté de la directive, que nous supprimerons. En dessous, nous ajouterons une ligne. Nous devons également chiffrer notre serveur, éditons donc les directives de chiffrement juste en dessous de cette section. Plus précisément, nous décommentons la ligne AES-128-CBC et ajouterons une directive auth. Ensuite, les paramètres d’utilisateur et de groupe: Facultativement, nous pourrions opter pour l’envoi de tout le trafic via le VPN. Pour cela, recherchez la directive redirect-gateway et décommentez-la. Juste en dessous, il devrait y avoir quelques lignes d’dhcp-option. Décommentez ceux-là aussi. Optionnellement, nous pouvons vouloir changer le port et le protocole sur lesquels OpenVPN fonctionne. La valeur par défaut est UDP: 1194, mais si votre réseau bloque les connexions VPN, ce sera probablement l’une des victimes. Le déguisement serait d’utiliser TCP: 443, qui est le port HTTPS. Si vous n’avez pas utilisé «serveur» comme nom de serveur, vos fichiers crt ont un nom différent. Mettez-les à jour en conséquence. Enregistrez et fermez le fichier. 6. Préparer Ubuntu Bien que nous ayons déjà configuré le transfert IP, etc., nous devons apporter quelques modifications pour activer ces options. Recherchez la ligne suivante et supprimez le # (caractère de commentaire). Sauver et fermer. Pour mettre à jour les paramètres de session, exécutez: Ensuite, nous devons trouver et mettre à jour nos règles de pare-feu (UFW) pour masquer les clients. La première étape consiste à trouver l’interface sur laquelle nous fonctionnons: L’interface que nous voulons est celle qui contient le mot “dev”. Dans notre cas, cela ressemble à ceci: Donc, notre interface est ens4. Avec cela, nous mettrons à jour nos règles de pare-feu: Au-dessus, là où il est indiqué Don't delete these required lines... ajoutez le code suivant: Sauver et fermer. Ensuite, nous devons transférer les paquets. Recherchez la directive DEFAULT_FORWARD_POLICY et remplacez-la de \"DROP\" par \"ACCEPT\". Sauver et fermer. 7. Lancer OpenVPN Pour démarrer le serveur, exécutez ce qui suit: Pour vérifier qu’il a bien démarré, lancez: Si tout se passe bien, vous devriez voir une sortie incluant Active: active (running). Vous devrez peut-être q sur q pour quitter le panneau d’informations. Si cela vous convient, liez le service à la séquence de démarrage. 8. Configuration d’une structure de configuration client Pour faciliter la configuration des configurations client, nous allons d’abord créer une structure. Pour commencer, créez un dossier de configuration pour stocker les fichiers de configuration du client. Les clés du client seront dans ces configurations, alors verrouillons les permissions sur le répertoire files. Copiez l’exemple de configuration. Éditons le fichier: Trouvez la directive remote. Remplacez my-server-1 par l’adresse IP externe publique attribuée à votre instance GCE. Si vous avez choisi un port autre que 1194, mettez-le à jour en conséquence. Mettez également à jour votre protocole. Décommentez l’utilisateur et le groupe: Trouvez les directives ca, cert et key, commentez-les, car nos configurations les incluront automatiquement. Utilisez les mêmes paramètres de chiffrement et d’authentification que précédemment: Quelque part, nous devrons ajouter la key-direction. Assurez-vous d’utiliser 1, comme c’est le cas pour le client. 0 était pour le serveur. Si votre configuration client actuelle est (ou sera) utilisée sur un périphérique Linux, ajoutez les éléments suivants: Notez que si vous les incluez dans un environnement non-Linux (Android et macOS inclus dans un non-Linux), vos clients peuvent se comporter de manière étrange. Sauver et fermer. Ensuite, nous devons écrire un script pour générer nos configs client rapidement et facilement. A l’intérieur, collez ce code: gen_config.sh Le {1} fait ici référence au premier argument, qui sera notre nom de client. Assurez-vous de mettre à jour tiv.key fonction du nom de votre clé HMAC. Autoriser l’exécution de ce script: 9. Générer des configurations client Le pas que vous attendiez tous est enfin arrivé. Nous allons générer nos configurations client. Vérifiez que cela a fonctionné en exécutant: Si tel est le cas, il devrait maintenant y avoir un fichier client.ovpn dans ce répertoire. Nous devons télécharger ce fichier et le transférer sur nos appareils. Pour ce faire, cliquez sur l’icône représentant une roue en haut à droite de la session SSH, puis sélectionnez «Télécharger le fichier». Le chemin pleinement qualifié devrait ressembler à ceci: 10. Installation sur les clients Pour Windows, téléchargez l’application client OpenVPN. Une fois installé, déplacez votre fichier client.ovpn vers C:\\Program Files\\OpenVPN\\config. Pour ouvrir OpenVPN, vous devez exécuter l’application en tant qu’administrateur. Une fois à l’intérieur, vous devriez voir et pouvoir vous connecter à votre VPN. Pour macOS, la plupart des gens suggèrent d’utiliser Tunnelblick bien que tout autre client OpenVPN (même s’il soit sans danger) soit probablement bon aussi. Une fois installé, vous devriez pouvoir faire glisser n’importe .ovpn fichier .ovpn sur l’icône Tunnelblick de la barre de menus pour installer la configuration. Utilisez la barre de menus pour vous connecter ensuite au VPN. Si vous êtes interrogé sur le plugin down-root.so, prenez une décision! Je l’ai sauté parce que je suis capable de reconnecter ma connexion, mais c’est à vous de décider. Sous Linux, installez le paquet openvpn en utilisant votre gestionnaire de paquets. Les utilisateurs de CentOS devront d’abord installer epel-release utilisant yum. Une fois installé, lancez ls /etc/openvpn. Si la sortie ne présente pas de fichier update-resolve-conf, vous devez modifier votre fichier .ovpn et supprimer les lignes suivantes (si vous les avez ajoutées): Si vous n’avez jamais ajouté ces lignes mais trouvé un fichier update-resolv-conf, ajoutez-les! Les utilisateurs de CentOS devront modifier le group dans le fichier .ovpn de nogroup à nobody pour se conformer aux normes du système d’exploitation. Pour vous connecter, lancez sudo openvpn — config client .ovpn. Pour iOS, installez l’application OpenVPN Connect. Ensuite, ouvrez iTunes sur votre ordinateur et accédez à iPhone> Applications. Faites défiler jusqu’à la section «Partage de fichiers» et sélectionnez “OpenVPN”. Faites glisser votre fichier .ovpn dans le panneau “OpenVPN Documents”. Ouvrir l’application sur votre téléphone devrait maintenant afficher un message indiquant qu’un nouveau profil est prêt à être importé. Une fois importé, vous devriez pouvoir vous connecter au VPN. Pour Android, installez l’application OpenVPN Connect. Ensuite, transférez votre fichier .ovpn n’importe où / de toute façon sur votre appareil (je viens d’utiliser Google Drive, mais cela peut ne pas être recommandé). Dans l’application, ouvrez le menu et cliquez sur “Importer”. Accédez à votre fichier .ovpn , cliquez dessus, puis appuyez sur “IMPORT” en haut à droite. Une fois terminé, vous devriez pouvoir vous connecter au VPN. Test du VPN Le moyen le plus simple de tester votre VPN serait de Google “quelle est mon adresse IP” avant et après la connexion. Votre adresse IP doit passer à l’adresse IP publique externe de votre instance GCE une fois que vous êtes connecté au VPN, puis à l’adresse IP de votre réseau une fois déconnectée du VPN. Si tout s’est bien passé, c’est tout! Votre nouveau VPN est prêt à être utilisé. Crédits Cet article est basé en grande partie sur ce didacticiel de la communauté DigitalOcean. Bien que la configuration elle-même soit assez similaire, j’ai pensé écrire un article séparé, car il existe quelques éléments clés différents dans GCE et il est assez facile de se tromper si vous le faites pour la première fois (je parle par expérience).","timeToRead":14,"frontmatter":{"title":"Configuration d'un serveur OpenVPN sur Google Compute Engine","subtitle":"","tags":["Google Cloud","OpenVPN","VPN"],"date":"2019-04-14T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAHtZYUFyf/EABkQAAIDAQAAAAAAAAAAAAAAAAECABESIP/aAAgBAQABBQLRss023H//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAABBAMAAAAAAAAAAAAAAAAAAQIRICEiMv/aAAgBAQAGPwJdTDJOKf/EABoQAAICAwAAAAAAAAAAAAAAAAABESEgMUH/2gAIAQEAAT8hqJULbAuzrD//2gAMAwEAAgADAAAAEPz/AP/EABcRAQEBAQAAAAAAAAAAAAAAAAEAETH/2gAIAQMBAT8QUTll/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARMf/aAAgBAgEBPxAEe2l//8QAHBABAQACAgMAAAAAAAAAAAAAAREAISAxQVFh/9oACAEBAAE/EBhvcvvVw0sN8wxo2hndH6a4f//Z","aspectRatio":1.7763157894736843,"src":"/static/3bf8405e70096f9e75e8c849f54b0d1f/de998/openvpn.jpg","srcSet":"/static/3bf8405e70096f9e75e8c849f54b0d1f/de998/openvpn.jpg 810w","sizes":"(max-width: 810px) 100vw, 810px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/openvpn-google/"}}},{"node":{"excerpt":"Ces trois fonctions sont utiles pour parcourir une liste (ou un tableau) et effectuer une sorte de transformation ou de calcul. Cela produira alors une nouvelle liste ou le résultat du calcul effectué sur la liste précédente. Les types Avant de plonger dans map, filter et reduce, configurons la liste. types.ts Chaque objet de l’assistant a un nom, une maison et le nombre de points qu’il a gagnés pour sa maison. Déclarons un groupe de magiciens et mettons-les dans une liste. wizards.ts Map Maintenant que nous avons la base, allons-y. La première fonction est la plus simple, map. Map itère (ou boucle) sur une liste, applique une fonction à chaque élément de cette liste, puis renvoie une nouvelle liste d’éléments transformés. Regardons un exemple. wizard-names.ts Cette fonction parcourt la liste des assistants, obtient leur nom et le place dans un nouveau tableau. Le résultat de ceci ressemble à ceci. Dans cet exemple, nous utilisions une fonction lambda (ou fonction anonyme), mais nous pouvons également utiliser une fonction nommée. wizardToString.ts Dans cet exemple, nous avons une fonction appelée wizardToString que nous transmettons directement au map. Il retournera alors une nouvelle liste qui ressemble à ceci. Filter Le filter se comporte comme un map dans la mesure où il itère sur la liste, mais au lieu de transformer chaque élément, il transforme la liste entière. Le filter prend une fonction qui renvoie true ou false ou un prédicat. Il renvoie ensuite une nouvelle liste avec des éléments où le prédicat renvoie true. Regardons un exemple. slytherins.ts Dans cet exemple, nous filtrons par-dessus la liste et n’incluons que les sorciers qui se trouvent dans la maison Serpentard. Le résultat serait ceci. En passant, Taylor et Lin sont deux des Serpentards les plus acclamés de notre époque. Comme avec map, nous n’avons pas besoin d’utiliser un lambda , nous pouvons également utiliser une fonction prédéfinie. winners-losers.ts Dans cet exemple, nous faisons deux listes, la liste des Serpentards ayant gagné des points (gagnants) et la liste des Serpentards ayant perdu des points (perdants). Nous pouvons voir ces résultats ci-dessous. Reduce Nous arrivons maintenant à la fonction la plus intéressante, reduce. Reduce itère sur une liste et produit une valeur unique. Regardons un exemple. Supposons que nous voulions obtenir le nombre total de points pour tous les assistants. Nous pouvons utiliser réduire pour faire cela. Que se passe t-il ici? Bien réduire est une fonction qui prend deux arguments, une fonction et une valeur initiale pour l’accumulateur. L’accumulateur est le nom de la chose réduire les rendements. Dans ce cas, nous commençons le compte de points à 0. Maintenant, la fonction prend l’état actuel de l’accumulateur et de l’élément dans la liste qu’il est supposé traiter. Pour le premier assistant, il passera 0 pour l’accumulateur. Cette fonction retourne ensuite accumulator + points. Cela finira par résumer tous les points. Si vous êtes curieux, le résultat est 5487. Maintenant, l’accumulateur peut être n’importe quoi, on peut même utiliser réduire pour produire un objet. Regardons un exemple où nous additionnons les points pour chaque maison. points-per-house.ts Dans ce cas, nous initialisons notre accumulateur ou acc avec {}. Ensuite, pour chaque assistant, nous appelons une fonction qui ajoute le nombre de points qu’il a gagnés pour sa maison. Si vous êtes curieux, ce résultat ressemble à ceci. Regardons un autre exemple, disons que nous voulons le meilleur assistant pour chaque maison. Nous pouvons modifier notre fonction précédente pour utiliser le meilleur assistant pour chaque maison. Si vous êtes curieux, le résultat est le suivant. Juste un peu plus de plaisir maintenant, nous pouvons utiliser Object.values pour transformer cet map. best-names-per-house.ts Maintenant, nous avons de beaux noms pour la meilleure personne dans chaque maison.","timeToRead":5,"frontmatter":{"title":"Map, filter, reduce","subtitle":"Ces trois fonctions sont utiles pour parcourir une liste (ou un tableau) et effectuer une sorte de transformation ou de calcul.","tags":["Javascript","Node.js"],"date":"2019-03-03T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAtElEQVQoz2P4cNufbMRAV83vb4EQyZo/3/P/dNf/28OAr/cDIPoZiLQNqPPeaZ9nl3zP7fW6fNDr872A90RqfnvD/8/TwHmTnfpb7VMTLPasdfv2AGQ5sZr/vw2eM8lx8XTn6b2OM/ocifUz0AagVzcsds3LsFq3wGViu0N6kuWLK35AQQYigwroz5M7PN/e9Duwwf3WcR+Qn4l0NjBsgJ78/jDg4x3/n48CvpAU2sjRixzPABStw4O6JhVqAAAAAElFTkSuQmCC","aspectRatio":1.7777777777777777,"src":"/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png","srcSet":"/static/f10109a249e0ae09b060bb7269f1f4be/4c9af/js.png 930w,\n/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/js-map-filter-reduce/"}}},{"node":{"excerpt":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent). Des cycles de publication de plusieurs mois ou années sont en train de devenir rares, en particulier parmi ceux qui construisent des produits logiciels purs. Aujourd’hui, en utilisant une approche axée sur les services et sur les microservices, les développeurs peuvent concevoir une base de code modulaire. Cela leur permet d’écrire et de déployer des modifications simultanément sur différentes parties de la base de code. Les avantages commerciaux de cycles de déploiement plus courts sont clairs: Le temps de mise sur le marché est réduit Les clients obtiennent la valeur du produit en moins de temps Les commentaires des clients sont également renvoyés plus rapidement dans l’équipe produit, ce qui permet à l’équipe de parcourir les fonctionnalités et de résoudre les problèmes plus rapidement. Le moral des développeurs augmente Cependant, ce changement crée également de nouveaux défis pour les opérations ou l’équipe de DevOps. Avec des déploiements plus fréquents, il est plus probable que le code déployé puisse affecter négativement la fiabilité du site ou l’expérience client. C’est pourquoi il est important de développer des stratégies de déploiement de code minimisant les risques pour le produit et les clients. Dans cet article, nous allons parler de différentes stratégies de déploiement, de meilleures pratiques et d’outils qui permettront à votre équipe de travailler plus rapidement et de manière plus fiable. Défis des applications modernes Les applications modernes sont souvent distribuées et basées sur le cloud. Ils peuvent évoluer pour répondre à la demande et sont plus résistants aux pannes grâce à des architectures hautement disponibles. Ils peuvent utiliser des services entièrement gérés comme AWS Lambda ou Elastic Container Service (ECS), où la plate-forme assume une partie de la responsabilité opérationnelle. Ces applications ont presque toujours des déploiements fréquents. Par exemple, une application mobile ou une application Web grand public peut subir plusieurs modifications au cours d’un mois. Certains sont même déployés en production plusieurs fois par jour. Ils utilisent souvent des architectures de microservices dans lesquelles plusieurs composants travaillent ensemble pour offrir une fonctionnalité complète. Il peut y avoir différents cycles de publication pour différents composants, mais ils doivent tous fonctionner ensemble de manière transparente. L’augmentation du nombre de pièces mobiles signifie plus de chances que quelque chose se passe mal. Avec plusieurs équipes de développement apportant des modifications dans la base de code, il peut être difficile de déterminer la cause première d’un problème inévitablement. Un autre défi: l’abstraction de la couche infrastructure, qui est maintenant considérée comme du code. Le déploiement d’une nouvelle application peut également nécessiter le déploiement d’un nouveau code d’infrastructure. Stratégies de déploiement populaires Pour relever ces défis, les équipes chargées des applications et de l’infrastructure doivent concevoir et adopter une stratégie de déploiement adaptée à leur cas d’utilisation. Nous en examinerons plusieurs et discuterons des avantages et des inconvénients de différentes stratégies de déploiement afin que vous puissiez choisir celle qui convient à votre organisation. Déploiement “Big Bang” Comme son nom l’indique, les déploiements “big bang” mettent à jour des parties entières ou volumineuses d’une application en un seul coup. Cette stratégie remonte à l’époque où le logiciel était publié sur un support physique et installé par le client. Les déploiements à grande échelle ont obligé l’entreprise à procéder à des développements et à des tests approfondis avant la publication, souvent associés au “modèle en cascade” de grandes versions séquentielles. Les applications modernes présentent l’avantage de se mettre à jour régulièrement et automatiquement, côté client ou côté serveur. Cela rend l’approche big bang plus lent et moins agile pour les équipes modernes. Les caractéristiques du déploiement du Big Bang comprennent: Toutes les pièces majeures conditionnées en un seul déploiement; Remplacer en grande partie ou totalement une version de logiciel existante par une nouvelle; Déploiement entraînant généralement de longs cycles de développement et de test; En supposant un risque minimal d’échec, les retours en arrière peuvent être impossibles ou peu pratiques; Les délais de réalisation sont généralement longs et peuvent nécessiter les efforts de plusieurs équipes; Action requise des clients pour mettre à jour l’installation côté client. Les déploiements à fort rendement ne conviennent pas aux applications modernes, car les risques sont inacceptables pour les applications grand public ou critiques pour les entreprises, où les pannes entraînent des pertes financières énormes. Les reculs sont souvent coûteux, prennent du temps, voire impossibles. L’approche big bang peut convenir à des systèmes hors production (par exemple, recréer un environnement de développement) ou à des solutions prêtes à l’emploi, tel que les applications de bureau. Déploiement progressif (Rolling) Les déploiements progressifs, par paliers ou par étapes sont préférables aux déploiements big bang, car ils minimisent de nombreux risques, y compris les temps d’arrêt pour les utilisateurs, sans restauration simple. Dans un déploiement en continu, la nouvelle version d’une application remplace progressivement l’ancienne. Le déploiement réel se produit sur une période de temps. Pendant ce temps, les nouvelles versions et les anciennes versions coexisteront sans affecter la fonctionnalité ou l’expérience utilisateur. Ce processus facilite la restauration de tout nouveau composant incompatible avec les anciens composants. Le diagramme suivant illustre le modèle de déploiement: l’ancienne version apparaît en bleu et la nouvelle version en vert sur chaque serveur du cluster.  Une mise à niveau d’une suite d’applications est un exemple de déploiement progressif. Si les applications d’origine ont été déployées dans des conteneurs, la mise à niveau peut traiter un conteneur à la fois. Chaque conteneur est modifié pour télécharger la dernière image à partir du site du fournisseur de l’application. S’il existe un problème de compatibilité pour l’une des applications, l’ancienne image peut recréer le conteneur. Dans ce cas, les nouvelles versions et les anciennes versions des applications de la suite coexistent jusqu’à ce que chaque application soit mise à niveau. Déploiement Blue-Green, Red-Black ou A/B C’est un autre processus à sécurité intégrée. Dans cette méthode, deux environnements de production identiques fonctionnent en parallèle. L’un est l’environnement de production en cours d’exécution recevant tout le trafic utilisateur (représenté en bleu). L’autre en est un clone, mais inactif (vert). Les deux utilisent la même base de données et la même configuration d’application:  La nouvelle version de l’application est déployée dans l’environnement vert et testée pour ses fonctionnalités et ses performances. Une fois les résultats des tests réussis, le trafic des applications passent du bleu au vert. Le vert devient alors la nouvelle production.  S’il y a un problème après que le vert devient actif, le trafic peut être redirigé vers le bleu. Dans un déploiement Blue-Green, les deux systèmes utilisent la même couche de persistance ou la même base de données. Il est essentiel de synchroniser les données de l’application, mais une base de données en miroir peut aider à atteindre cet objectif. Vous pouvez utiliser la base de données primaire en bleu pour les opérations d’écriture et la base secondaire en vert pour les opérations de lecture. Lors du passage du bleu au vert, la base de données est basculée du primaire au secondaire. Si le vert a également besoin d’écrire des données pendant le test, les bases de données peuvent être en réplication bidirectionnelle. Une fois que le vert devient actif, vous pouvez fermer ou recycler les anciennes instances bleues. Vous pouvez déployer une version plus récente sur ces instances et en faire le nouveau vert pour la prochaine version. Les déploiements Blue-Green reposent sur le routage du trafic. Cela peut être fait en mettant à jour les DNS (CNAMES) pour les hôtes. Cependant, des valeurs TTL longues peuvent retarder ces modifications. Vous pouvez également modifier les paramètres du load balancer pour que les modifications prennent effet immédiatement.  Déploiement canari Le déploiement canari ressemble à du Blue-Green, à la différence qu’il est moins enclin à prendre des risques. Au lieu de passer du bleu au vert en une seule étape, vous utilisez une approche progressive. Avec le déploiement canari, vous déployez un nouveau code d’application dans une petite partie de l’infrastructure de production. Une fois que l’application est validée pour la publication, seuls quelques utilisateurs y sont routés. Cela minimise tout impact. En l’absence d’erreur signalée, la nouvelle version peut progressivement être déployée dans le reste de l’infrastructure. L’image ci-dessous illustre le déploiement canari:  Le principal défi du déploiement canari est de trouver un moyen d’acheminer certains utilisateurs vers la nouvelle application. En outre, certaines applications peuvent toujours avoir besoin du même groupe d’utilisateurs pour les tests, tandis que d’autres peuvent nécessiter un groupe différent à chaque fois. Envisagez un moyen d’acheminer de nouveaux utilisateurs en explorant plusieurs techniques: Exposer les utilisateurs internes au déploiement canari avant d’autoriser l’accès des utilisateurs externes; Baser le routage sur la plage IP source; Livrer l’application dans des régions géographiques spécifiques; Utilisation d’une logique d’application pour déverrouiller de nouvelles fonctionnalités pour des utilisateurs et des groupes spécifiques. Cette logique est supprimée lorsque l’application est en ligne pour le reste des utilisateurs. Meilleures pratiques de déploiement Les équipes d’applications modernes peuvent suivre un certain nombre de meilleures pratiques pour minimiser les risques de déploiement: Utilisez une liste de déploiement. Par exemple, un élément de la liste peut consister à “sauvegarder toutes les bases de données uniquement après que les services d’application ont été arrêtés” afin d’éviter toute corruption des données. Adopter l’intégration continue (CI). CI s’assure que le code archivé dans la branche de fonctionnalité d’un référentiel de code ne fusionne avec sa branche principale qu’après une série de vérifications de dépendance, de tests d’unité et d’intégration et une génération réussit. S’il y a des erreurs le long du chemin, la construction échoue et l’équipe de l’application en est informée. L’utilisation de CI signifie donc que chaque modification apportée à l’application est testée avant son déploiement. Les exemples d’outils de CI incluent: CircleCI, Jenkins. Adoptez la livraison continue (CD). Avec le CD, l’artefact de code construit par le CI est packagé et toujours prêt à être déployé dans un ou plusieurs environnements. Utilisez des environnements d’exploitation standard pour assurer la cohérence de l’environnement. Vous pouvez utiliser des outils tels que Vagrant et Packer pour les postes de travail et les serveurs de développement. Utilisez les outils d’automatisation de build pour automatiser les générations d’environnement. Avec ces outils, il est souvent simple de cliquer sur un bouton pour détruire une pile d’infrastructure complète et la reconstruire à partir de zéro. CloudFormation est un exemple de tels outils. Utilisez des outils de gestion de configuration tels que Puppet, Chef ou Ansible sur les serveurs cibles pour appliquer automatiquement les paramètres du système d’exploitation, appliquer des correctifs ou installer des logiciels. Utilisez des canaux de communication tels que Slack pour les notifications automatisées des générations infructueuses et des échecs d’application. Créez un processus pour alerter l’équipe responsable des déploiements qui échouent. Dans l’idéal, vous les rencontrez dans l’environnement CI, mais si les modifications sont appliquées, vous aurez besoin d’un moyen d’avertir l’équipe responsable. Activez les restaurations automatisées pour les déploiements qui échouent aux vérifications de l’état, que ce soit en raison de problèmes de disponibilité ou de taux d’erreur. Surveillance post-déploiement Même après que vous ayez adopté toutes ces pratiques, il est possible que des choses se passent mal. Pour cette raison, la surveillance des problèmes survenant immédiatement après un déploiement est aussi importante que la planification et l’exécution d’un déploiement parfait. Un outil de surveillance des performances des applications peut aider votre équipe à surveiller les métriques de performances critiques, y compris les temps de réponse du serveur après les déploiements. Les modifications apportées à l’architecture des applications ou du système peuvent affecter considérablement les performances des applications. Une solution de surveillance des erreurs est également essentielle. Il informera rapidement votre équipe des erreurs nouvelles ou réactivées lors d’un déploiement est susceptibles de révéler des bogues importants nécessitant une intervention immédiate. Sans outil de surveillance des erreurs, les bogues n’auraient peut-être jamais été découverts. Alors que quelques utilisateurs rencontrant les bugs prendront le temps de les signaler, la plupart des autres ne le font pas. L’expérience client négative peut entraîner des problèmes de satisfaction au fil du temps ou, pire, empêcher les transactions commerciales d’avoir lieu. Un outil de surveillance des erreurs crée également une visibilité partagée de tous les problèmes de post-déploiement parmi les équipes Opérateur / DevOps et les développeurs. Cette compréhension partagée permet aux équipes d’être plus collaboratives et réactives.","timeToRead":9,"frontmatter":{"title":"Introduction aux stratégies de déploiement","subtitle":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent).","tags":["CICD","Deploy","DevOps"],"date":"2019-02-02T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFak9W5wHg//8QAGxAAAQQDAAAAAAAAAAAAAAAAAQACAxESISL/2gAIAQEAAQUCKy6E1BuzHEyqC//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAQACAwEAAAAAAAAAAAAAAAABMRESIYH/2gAIAQEABj8C8Qi3WdVP/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAERITGR/9oACAEBAAE/IaNiOjRsui2rXBjEiRSTh//aAAwDAQACAAMAAAAQpN//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPxBX/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQIBAT8Qp//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRMUFhwf/aAAgBAQABPxDieh8j0AS9GPIJjMXQwMpryCyIpy61KNPs2wWhNAn/2Q==","aspectRatio":1.7777777777777777,"src":"/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg","srcSet":"/static/b9929b1a4915b5e4d241ad5af23c0901/f8f18/servers.jpg 930w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/deploy/"}}},{"node":{"excerpt":"Dans cet article, vous apprendrez à créer et à gérer facilement des git hooks pour vos projets Node/NPM à l’aide de Husky. Git Hooks En termes simples, les git hooks sont des scripts personnalisés, qui peuvent être exécutés automatiquement lorsque des événements spécifiques se produisent. Des crochets (hooks) côté client sont déclenchés pour des actions telles que le commit ou le merge. Les crochets côté serveur s’exécutent dans des situations telles que la réception de données (git push) du client. Les hooks peuvent exécuter n’importe quelle logique personnalisée et, plus important encore, rejeter l’action effectuée si quelque chose ne va pas. Par exemple, vous pouvez annuler la validation du commit si son message ne contient pas l’ID de problème du suivi des problèmes. Vous pouvez également le refuser si l’analyse de code statique échoue. Cela peut être très utile si vous voulez vous assurer que votre base de code reste propre ou si vous souhaitez appliquer certaines règles de qualité. Mais comment pouvez-vous réellement installer et gérer ces hooks? Chaque fois que vous clonez un repo git, toutes les données git de votre projet sont stockées dans un répertoire .git dans votre dossier. Il contient plusieurs fichiers et sous-répertoires, l’un d’eux étant appelé \u001dhooks\u001d. À l’intérieur, vous trouverez un tas de fichiers.  Chacun d’entre eux est un script, qui est exécuté lorsqu’un événement spécifique se produit. Le nom de l’événement correspond au nom du fichier. Par exemple, la \u001dpre-commit\u001d est exécutée avant de commiter vos modifications. Comme vous pouvez le constater, tous les fichiers ont une extension \u001d.sample\u001d. Git ignore ces fichiers à moins que vous ne les renommiez. Vous devez supprimer l’extension \u001d.sample\u001d pour activer ces hooks. Dans les exemples de fichiers, vous pouvez trouver une description et un exemple d’implémentation que vous pouvez utiliser comme point de départ pour implémenter vos propres hooks. À quoi ça sert? Maintenant, regardons quelques exemples spécifiques de ce qui peut être réalisé avec les git hooks. Comme ce ne sont que des scripts, vous pouvez faire à peu près n’importe quoi. Habituellement, cela signifie effectuer divers contrôles de qualité. Vous pouvez vous assurer que l’utilisateur a son nom et son email renseignés. Vous pouvez vérifier que le message de validation est correctement formaté. Vous pouvez essayer de créer votre application et rejeter le commit si la construction échoue. Vous pouvez exécuter des tests pour vous assurer qu’ils passent avant le commit. L’utilisation typique est également l’analyse de code statique ou le formatage. Cela signifie qu’il faut vérifier dans votre code les problèmes courants, les mauvaises pratiques, les conventions de dénomination, etc. Il peut également être utile d’exécuter un outil tel que Prettier pour s’assurer que le code est bien formaté avant le commit. Cela évite de nombreux maux de tête lors de la révision du code. Vous pouvez même vérifier les failles de sécurité de votre code avec un outil tel que Snyk. Distribution aux membres de l’équipe Avec les hooks côté serveur, la distribution est facile. Vous n’avez généralement qu’un seul référentiel de serveur principal. Cela signifie que tous les membres de l’équipe mettent généralement leurs modifications au même endroit. Vous y installez vos hooks et vous avez terminé. Avec les hooks côté client, cela devient plus compliqué. Lorsque vous clonez un référentiel, les hooks ne sont pas transférés au client. Cela signifie qu’un référentiel fraîchement cloné n’a aucun hook, peu importe le type de hook que vous avez sur le serveur. Si vous voulez que les membres de votre équipe aient un ensemble unifié de git hooks, vous devez les distribuer d’une manière ou d’une autre et vous assurer qu’ils sont inclus dans leur sous-répertoire git hooks. La solution la plus élémentaire consiste à créer un emplacement partagé, où vous stockez vos hooks, puis demandez à vos développeurs de les télécharger et de les mettre dans leur répertoire de hooks. Bien sûr, vous ne pouvez pas être sûr qu’ils le feront réellement. Le problème est qu’ils ont besoin de savoir qu’ils devraient le faire et comment. Et même s’ils le font, ils peuvent être paresseux ou simplement ignorer votre politique. Vous pouvez améliorer un peu cette solution de base en ayant des hooks dans le référentiel de votre projet et en laissant simplement à vos développeurs exécuter un script personnalisé comme celui-ci, qui les copie ensuite dans leur répertoire de hooks. Sinon, git propose une option pour changer la destination du répertoire des hooks en un emplacement personnalisé: Ces solutions facilitent la distribution, mais ne résolvent pas les problèmes essentiels. Husky Installation Vous pouvez installer Husky simplement en exécutant: Alternative, avec yarn: Ajout de hooks Ajouter des hooks avec Husky est facile. Vous devez juste éditer votre package.json. Vous définissez quels scripts doivent être exécutés sur quel événement git. Pour Husky 1.0.0+, utilisez: Si vous utilisez une version de Husky antérieure à 1.0.0 (disponible dans la version candidate du 06/2018), la syntaxe est légèrement différente. Vous ajoutez vos hooks Husky directement dans la section scripts. L’exemple ci-dessus exécute tous vos tests avant de commiter et avant de pousser et si vos tests échouent, l’action git n’est pas exécutée. Bien sûr, vous pouvez exécuter tout autre script nom plutôt que le nom test. Problème de correction automatique avant validation Il est utile de casser le build si quelque chose ne va pas, mais il est encore plus utile de corriger automatiquement les problèmes avant de s’engager. Par exemple, vous pouvez personnaliser votre code en utilisant Prettier avant de faire un commit ou vous pouvez corriger automatiquement les problèmes de formatage, qui peuvent être résolus automatiquement. C’est beaucoup plus facile de cette façon. Heureusement, il existe un outil pour cela. Cela s’appelle lint-staged. Vous pouvez l’installer par: Maintenant, sur votre action de pre-commit, vous exécutez directement \u001dlint-staged\u001d au lieu de votre linter. Dans lint-staged, vous définissez ce qui doit être exécuté: Lorsque vous essayez de commiter maintenant, \u001dlint-staged\u001d peut modifier vos fichiers avant l’exécution de la validation. Cependant, ce qui est vraiment bien, c’est que vous ne filtrez que les fichiers en attente de validation, et non l’ensemble de votre projet. Cela signifie que l’ensemble du processus est beaucoup plus rapide. Bug JetBrains IDE La bonne nouvelle est que les git hooks configurés de cette manière sont exécutés non seulement lorsque vous utilisez git à partir de votre terminal, mais également à partir d’un IDE. La mauvaise nouvelle est que les IDE JetBrains (IDEA, Webstorm, …) ont actuellement un méchant bogue (voir IDEA-135454) et ne fonctionnent pas bien avec cette configuration. Le problème n’est pas résolu avant plusieurs années, mais heureusement, il existe une solution de contournement. Vous devez juste ajouter ce hook post-commit: Bien entendu, il ne s’agit que d’une solution de contournement jusqu’à ce que le problème soit résolu. Le suivi des problèmes de JetBrains contient une fonctionnalité de vote, alors assurez-vous de voter pour que ce problème soit résolu s’il vous pose problème. Intégration continue Une chose à noter est que Husky installe les hooks uniquement lorsqu’il ne s’exécute pas sur un serveur d’intégration continue. Husky peut détecter qu’il est en cours d’exécution dans le cadre d’un travail CI et n’installe aucun hook. En ignorant Les hooks côté client peuvent être utiles, mais vous ne pouvez pas trop compter sur eux. Ils ne sont que le premier niveau de défense. Vous ne pouvez pas être sûr à 100% qu’ils soient exécutés. Ils peuvent être ignorés à la demande en ajoutant une option de ligne de commande: Pour rendre les choses encore plus faciles, les hooks peuvent être désactivés à l’aide de certaines variables environnementales. Pour cette raison, il est toujours utile d’appliquer la même fonctionnalité sur le serveur. Performance Bien que les hooks cotés client tels que le pre-commit puissent s’avérer très utiles, vous devez garder à l’esprit qu’ils prennent un certain temps à s’exécuter. Les commits, qui sont généralement très rapides, car ils ne se produisent que sur le client, peuvent prendre soudainement très longtemps. Vous serez peut-être tenté d’exécuter tous les tests, l’analyse de code statique, les vérifications préalables, etc., avant chaque validation. Lorsqu’un commit prend des années, vos développeurs ne seront pas heureux et seront tentés d’ignorer les hooks lors de l’exécution de leurs commandes git. Vous devez donc trouver le bon équilibre entre ce qui doit être effectué sur le client et ce qui peut être un point d’accès côté serveur. Conclusion Husky est un outil utile qui permet de créer et de gérer facilement des hooks git sur le client. Vous n’avez plus besoin de distribuer vos hooks manuellement. Comme pour tout, gardez le nombre de hooks côté client avec modération afin d’éviter les temps d’exécution trop longs.","timeToRead":7,"frontmatter":{"title":"Git hooks avec Husky","subtitle":"En termes simples, les git hooks sont des scripts personnalisés, qui peuvent être exécutés automatiquement lorsque des événements spécifiques se produisent.","tags":["Git","Node.js","Javascript"],"date":"2018-11-25T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAeZvaUrjYf/EABkQAQADAQEAAAAAAAAAAAAAAAEAAiEREv/aAAgBAQABBQIrr2z4jDGf/8QAFhEBAQEAAAAAAAAAAAAAAAAAABIB/9oACAEDAQE/AZxL/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8Bqv/EABkQAAIDAQAAAAAAAAAAAAAAAAAyASAxkf/aAAgBAQAGPwJTBZ7T/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAERIUGh/9oACAEBAAE/IWy6ZuV4UVsOR8Kf/9oADAMBAAIAAwAAABDb7//EABcRAAMBAAAAAAAAAAAAAAAAAAABETH/2gAIAQMBAT8QbtIP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/ELTT/8QAGxABAQEBAAMBAAAAAAAAAAAAAREAITFBUXH/2gAIAQEAAT8QEkQjOdxTGyQQ/nnNus/UZr6lPWsqdhrblLv/2Q==","aspectRatio":1.6103059581320451,"src":"/static/ac7733f363ac6c799dc4d94c21beb53b/883ab/husky2.jpg","srcSet":"/static/ac7733f363ac6c799dc4d94c21beb53b/f8f18/husky2.jpg 930w,\n/static/ac7733f363ac6c799dc4d94c21beb53b/0e6ff/husky2.jpg 1860w,\n/static/ac7733f363ac6c799dc4d94c21beb53b/883ab/husky2.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/git-husky/"}}},{"node":{"excerpt":"Si vous avez un Kindle d’Amazon, vous ne le savez peut-être pas, mais vous possédez une adresse mail ….@kindle.com qui vous permet de vous envoyer des livres et documents directement sur votre liseuse. Pour cela, il faut ajouter votre email via cette page, dans la section “Settings” > “Personal Document Settings” > “Approved Personal Document E-mail List”. Et juste au-dessus, vous devriez voir dans la section “Send-to-Kindle E-Mail Settings”, le ou les emails @kindle.com associés avec chacun de vos appareils Kindle. En effet, en envoyant un fichier depuis votre email référencé vers votre email kindle.com, vous pouvez le transférer vers votre liseuse sans avoir besoin de brancher le moindre câble ou d’installer le moindre soft. Seulement, Amazon autorise l’envoi uniquement des fichiers aux formats suivants : Kindle Format (.MOBI, .AZW) Microsoft Word (.DOC, .DOCX) HTML (.HTML, .HTM) RTF (.RTF) Text (.TXT) JPEG (.JPEG, .JPG) GIF (.GIF) PNG (.PNG) BMP (.BMP) PDF (.PDF) Malheureusement, comme vous pouvez le voir, pas de format epub. Obligé de brancher le Kindle à votre ordinateur et d’utiliser Calibre ? Non, non, non. Il suffit de vous rendre sur le site Send epub to Kindle , puis de renseigner les infos demandées, à savoir votre email référencé, l’email Kindle et de glisser-déposer le livre au format epub de votre choix. Cliquez ensuite sur le bouton vert « Upload & Send » et tadaaaa, au bout de quelques secondes, le livre apparaitra sur votre liseuse Kindle. Elle est pas belle la vie ?","timeToRead":1,"frontmatter":{"title":"Comment envoyer un epub vers une Kindle sans utiliser Calibre ni de cable USB ?","subtitle":"","tags":["Kindle","Ebook"],"date":"2018-11-24T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGa0ziTSqWf/8QAGBABAQEBAQAAAAAAAAAAAAAAAQIDACL/2gAIAQEAAQUCy0ItmMp9HVjK1mHMj3//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABsQAAIDAAMAAAAAAAAAAAAAAAABAhExEBIh/9oACAEBAAY/As8xkujuUlRnFos//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARITFB/9oACAEBAAE/ISa6jh2N3hhGocLS2JPOElOjWf/aAAwDAQACAAMAAAAQ2P8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/EIw//8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EK2//8QAGxABAQACAwEAAAAAAAAAAAAAAREAMSFRYUH/2gAIAQEAAT8QFwmLUI775mAebFhqWTRLvCPHnyYECp4Z6B6G/HGBI1nef//Z","aspectRatio":1.5,"src":"/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg","srcSet":"/static/e96dc41e5ffd5680b39408b75434d7e0/f8f18/kindle.jpg 930w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/0e6ff/kindle.jpg 1860w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg 1920w","sizes":"(max-width: 1920px) 100vw, 1920px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/kindle/"}}},{"node":{"excerpt":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser. Introduction Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif. Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. Docker Swarm et Kubernetes sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur. Création du conteneur J’ai décidé d’utiliser Samba pour l’application de test. Samba est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445. C’est la première fois que je travaille avec Docker, j’ai donc modifié un conteneur Samba standard afin d’inclure le fichier que je voulais servir. Après le tutoriel de Docker, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement: Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec smbclient Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur. Préparer les machines virtuelles J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox. J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler: https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png Ensuite, j’ai ajouté un serveur DHCP pour attribuer des adresses IP à chaque machine virtuelle: Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100. Docker Swarm Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes. Mise en place de Docker Swarm Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le tutoriel officiel, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle: C’est tout: le moteur Docker tourne maintenant en mode Swarm. Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les instructions d’installation: Déploiement de l’application Docker Swarm utilise le format Docker Compose pour spécifier les conteneurs à exécuter et les ports qu’ils exportent. Après le didacticiel Docker Compose, j’ai créé ce manifeste Docker Compose: Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur. Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé: Une fois le conteneur créé, l’application peut être déployée avec la commande docker stack deploy, en spécifiant le nom du service: Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec smbclient: Ajout d’un autre noeud Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm: Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire: Sur le nouveau noeud de travail, le nouveau conteneur est apparu: Test de l’équilibrage de charge (load balancing) Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm. Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec nc: Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe. Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100): Et sur le noeud travailleur (10.133.7.50): Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement. J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère). Kubernetes Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant. Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que minikube, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB. Mise en place de Kubernetes Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel Kubernetes the Hard Way Les développeurs de Kubernetes ont simplifié l’utilisation de kubeadm. Malheureusement, Kubernetes étant si flexible, le tutoriel sur kubeadm ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même. Voici ce que j’ai fini par lancer. J’ai d’abord dû désactiver Swap sur chaque noeud: Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante: L’option --pod-network-cidr attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes. Les options --apiserver-advertise-address et --apiserver-cert-extra-sans ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100. Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions: J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait! Après avoir enfin lu les instructions, je devais faire trois autres choses: Tout d’abord, je devais exécuter les commandes données par kubeadm pour configurer un fichier de configuration. Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le tutoriel m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud: Enfin, je devais choisir un réseau pour mon cluster. Installation du réseau Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge. Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et cet article comparatif suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du didacticiel kubeadm pour appliquer la configuration de WeaveNet: Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune. Pour installer MetalLB, j’ai suivi son didacticiel de mise en route et j’ai tout d’abord déployé MetalLB: Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant ce fichier de configuration: Déploiement de l’application Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité. Après avoir lu le tutoriel de Kubernetes, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement. https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447 Ce service demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge. https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8 Cet objet Deployment indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer. Notez le replicas: 1 - c’est le nombre d’instances du conteneur que je veux exécuter. Je peux déployer ce service sur Kubernetes en utilisant kubectl apply: Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner: Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB: Ajout d’un autre noeud Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par kubeadm sur le nouvel ordinateur: Bizarreries de ma configuration J’ai dû faire deux changements en raison de la configuration de VirtualBox: Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon ce problème, je devais éditer Et changer une ligne en avant de redémarrer Kubernetes: L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de ssh: Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine. En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder. “Scaling” de l’application Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application: Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de CreatingContainer et a commencé à s’exécuter: Test de l’équilibrage de charge J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant. Master: Worker: Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi. Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours. Comparaison et conclusion Fonctionnalités communes aux deux: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale. Les atouts de Docker Swarm: une configuration simple, aucune configuration requise, une intégration étroite avec Docker. Les points forts de Kubernetes: composants souples, nombreuses ressources disponibles et add-ons. Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité. J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais tout désactiver en même temps. Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un tableau de bord sophistiqué. Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser minikube, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise. Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile. En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes. Ce que j’ai appris Comment travailler avec les conteneurs Docker Comment configurer un cluster Docker Swarm à deux noeuds Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP Comment déployer une application sur Docker Swarm et Kubernetes Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98 Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent","timeToRead":16,"frontmatter":{"title":"Docker Swarm vs Kubernetes","subtitle":"J'ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser.","tags":["Kubernetes","Docker","DevOps"],"date":"2018-11-01T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB6OtBGFa2/8QAGhABAAIDAQAAAAAAAAAAAAAAAQACAxESE//aAAgBAQABBQJyW7vkSe1iaNoIBU//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Bia//xAAZEQABBQAAAAAAAAAAAAAAAAAAAQIRElH/2gAIAQIBAT8BlCzcP//EABoQAAMBAAMAAAAAAAAAAAAAAAABESESMUH/2gAIAQEABj8CU6MU30k5FhGRKH//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhQTFRYf/aAAgBAQABPyHjwtkhQI2lwNVg2E1C5WkTplYQ8n//2gAMAwEAAgADAAAAEAjv/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhYf/aAAgBAwEBPxAQO3S//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAgEBPxDsOTD/xAAaEAEBAQADAQAAAAAAAAAAAAABEQAhMUFx/9oACAEBAAE/EEhJlEVOtxTFYSS051ExT5vZ7lQoemZMl9hTc8RsE3//2Q==","aspectRatio":1.7772511848341233,"src":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg","srcSet":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg 750w","sizes":"(max-width: 750px) 100vw, 750px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/dockerSwarm-vs-kubernetes/"}}},{"node":{"excerpt":"Souvent, je me trouve aux prises avec Sequelize pour trouver une réponse directe à ma requête. Récemment, je travaillais sur une application full stack dans laquelle il était impératif de paginer les résultats depuis le backend (API REST) vers le client. Je me suis battu pour deux raisons. Tout d’abord, venant du context NoSQL, il est difficile de saisir les bases de données SQL. La deuxième raison étant que la documentation de Sequelize ne fournit pas une solution claire et directe à cette abstraction très basique. Beaucoup de gens supposent des choses dans le monde des bases de données SQL. Ainsi, dans cet article, nous allons parler d’un module de base de pagination utilisant Sequelize, MySQL et Node.js. J’utilise des tables et des enregistrements dans votre base de données MySQL. Pour configurer une nouvelle application et établir une connexion à une base de données, lisez mon post sur Premiers pas avec Sequelize. Définir un modèle Je saute directement sur la définition du modèle utilisateur: J’utilise une table contenant une centaine d’enregistrements d’utilisateur que nous voulons afficher sur une application Web, par exemple dans le panneau d’administration, et nous voulons afficher seulement 50 enregistrements à la fois. Dans le fichier api/user.js, je définis un endpoint /:page qui extraira le nombre de résultats nécessaires de la base de données. findAndCountAll est le modèle de recherche dans plusieurs enregistrements de la base de données. Il retourne à la fois les données requises et le nombre d’éléments de cette table. La requête ci-dessus obtiendra 50 enregistrements d’utilisateur à la fois jusqu’à ce que la page suivante soit appelée pour extraire les 50 prochains enregistrements. limit et offset sont nécessaires dans les requêtes liées à la pagination dans lesquelles limit extrait le nombre de lignes en fonction de la requête, tandis que offset est utilisé pour ignorer le nombre","timeToRead":1,"frontmatter":{"title":"Comment paginer des enregistrements dans MySQL avec Sequelize et Nodejs","subtitle":"","tags":["Sequelize","Node.js","Javascript"],"date":"2018-10-13T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAQX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABc/i1RhGH/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAMBAhEEE//aAAgBAQABBQKvRtmuqontsQqdanTwP//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/Aar/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwGI/8QAHBAAAQMFAAAAAAAAAAAAAAAAAAEiMQIREjLh/9oACAEBAAY/An0mV5g4bqI4k//EABoQAQACAwEAAAAAAAAAAAAAAAEAESFBUfH/2gAIAQEAAT8hKyhSrJs7VFOGpdNmVqHIjzP/2gAMAwEAAgADAAAAEOPv/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QhD//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/EA//xAAfEAEAAgIABwAAAAAAAAAAAAABABEhMUFRYXGhsfD/2gAIAQEAAT8Q65gFNZ8S8YYMuKrcvBhNcPuECl5hEFdEsYiW1/O8/9k=","aspectRatio":1.4992503748125936,"src":"/static/e47b6c69cbe49a04cdbe6e0e621f405f/883ab/open-book.jpg","srcSet":"/static/e47b6c69cbe49a04cdbe6e0e621f405f/f8f18/open-book.jpg 930w,\n/static/e47b6c69cbe49a04cdbe6e0e621f405f/0e6ff/open-book.jpg 1860w,\n/static/e47b6c69cbe49a04cdbe6e0e621f405f/883ab/open-book.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/node-sequelize-pagination/"}}},{"node":{"excerpt":"Introduction à l’ORM ORM ou Object Relation Mapping est un processus de mappage entre des objets et des systèmes de base de données relationnels. Un ORM agit comme une interface entre deux systèmes. Les ORM offrent aux développeurs des avantages de base, tels que la réduction du temps et des efforts et la concentration sur la logique métier. Le code est robuste au lieu de redondant. ORM aide à gérer les requêtes sur plusieurs tables de manière efficace. Enfin, un ORM (comme sequelize) est capable de se connecter à différentes bases de données (ce qui est pratique lors du passage d’une base de données à une autre). Démarrer avec Sequelize Débuter avec Sequelize Sequelize est un ORM basé sur des promesses pour Node.js. Sequelize est facile à apprendre et possède des dizaines de fonctionnalités intéressantes comme la synchronisation, l’association, la validation, etc. Il prend également en charge PostgreSQL, MySQL, MariaDB, SQLite et MSSQL. J’utilise actuellement PostgreSQL. Installation Sequelize est disponible via npm. Établissement d’une connexion Sequelize établit une connexion entre l’API / application restante et votre base de données SQL. Pour configurer la connexion de base entre les deux:","timeToRead":1,"frontmatter":{"title":"Démarrer avec Sequelize","subtitle":"Les ORM offrent aux développeurs des avantages de base, tels que la réduction du temps et des efforts et la concentration sur la logique métier.","tags":["Sequelize","Node.js","Javascript"],"date":"2018-10-12T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABAUlEQVQoz2P4DwP//v//8+//33//iQcM2AT/AQGxmiEK3/3+33H75/Jnf/4QbzPEjucff7ptfym87JHMiofVO67fv/v489fvQPt//Pz19+9fkNHvP3389PXXrz+/fyMMZ/gL1n3p8Se7vhNS9buVchfm180+c+HW1j2n9x+5uGnHicvXHmzedXLdliOnzt3cffD8+q1Hf/78hWLz3VdffSefMq3bZhbdXt264O37z7fuPn3+8t3Zi7fvPXh++97Tx09fv3j1/t6jF/cePv8DdguKn689/1q37MyM+VvevP0ICTTSQvv3H6gWIPnvHzTMYWwUQXTNEGE0aTLi+T89NAMA0+5wRDWkongAAAAASUVORK5CYII=","aspectRatio":1.7777777777777777,"src":"/static/335caa592debcad471a1ec9936833b1b/9ecf6/sequelize.png","srcSet":"/static/335caa592debcad471a1ec9936833b1b/4c9af/sequelize.png 930w,\n/static/335caa592debcad471a1ec9936833b1b/9ecf6/sequelize.png 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/node-sequelize-intro/"}}},{"node":{"excerpt":"La plupart des exemples de code couvrant le kit AWS SDK comme ci-dessous, c’est à dire qu’ils importent l’intégralité du kit AWS même s’ils utilisent seulement quelques services AWS, parfois un seul (AWS DynamoDB).  Cependant, la méthode recommandée pour initialiser divers clients de service AWS consiste à ne les importer que lorsque nécessaire, comme ci-dessous. Économise des temps de chargement et de la mémoire précieux, particulièrement utile dans les environnements à ressources de calcul comme un périphérique IoT ou dans une fonction AWS Lamba.  NB: vous pouvez toujours accéder à l’espace de noms AWS global sans chaque service AWS associé en écrivant sous le code. Cette technique est utile lorsque vous appliquez la même configuration à plusieurs services AWS individuels, par exemple pour fournir les mêmes informations d’identification à tous les services AWS. const aws = require('aws-sdk/global'); Consultez la documentation officielle d’AWS pour plus d’informations ci-dessous. https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/creating-and-calling-service-objects.html Bonne programmation!","timeToRead":2,"frontmatter":{"title":"AWS SDK pour Node.js: Meilleures pratiques","subtitle":"","tags":["AWS","Amazon Web Service","Node.js"],"date":"2018-09-23T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAABm0lEQVQoz6VSS0sCURj1p7QKIghatGrTIpNS0zQXIQnqohlHpmYENy3S1AZntLBk5r7mjk96QVGbIFq5alH7Fm2iqGV/oTsp9oIedOAeLt/l8J3zfdeB/gHHlwp8x38RQ4QBohARw2b8KzFGkCCDIEBR9ZBkLLh1gNcI2AEAQvu8AQDwWQwQ0WGtAvaOzdW79sjD3ujz/kCnJQDSoiYmhJgM1CbLsj6ImckG3uzUImdUvKrPXtfdjVLsaDtcWpdVtayqxXK5XHxFSdMKhYJhGBj3EjlYzxMz/dgevG8NP+0OncOFCWfINR1wTbmDfr/f6wsFgj6Pd34u4JmZcU06lQ2FUsoidG1DA9JTmr5tjl02FirbejLBpeSVlWUxJacSfCLB87IkJQVhWWQVWVM1FqTXGdtia5coN83xixpfqSKeW4pGo4uRiCRJ4XBYEARRFDmOi8fjsVgsl8v1nXdXxUxg1p/tic2TPWcymWw2qygKY01lwYssbT6fZ6zr+lvm/o4xAvj1b9ijpbQ3ZNPsX7r4/och+C1+EP8eL/ttumw4YiedAAAAAElFTkSuQmCC","aspectRatio":1.6,"src":"/static/7df7623d3341dedb9671ec0dd508de83/91f24/aws.png","srcSet":"/static/7df7623d3341dedb9671ec0dd508de83/4c9af/aws.png 930w,\n/static/7df7623d3341dedb9671ec0dd508de83/91f24/aws.png 1280w","sizes":"(max-width: 1280px) 100vw, 1280px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/aws-sdk/"}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"authorId":"ludo","limit":9,"skip":27,"numPages":5,"currentPage":4}}}
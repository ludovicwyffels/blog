{"componentChunkName":"component---src-templates-author-tsx","path":"/author/ludo/page/3","webpackCompilationHash":"ba1a271305926245155b","result":{"data":{"authorYaml":{"id":"ludo","name":"Wyffels Ludovic","website":"https://ludovicwyffels.github.io","twitter":"WYFFELSLudovic","bio":"Développeur senior. Fullstack + DevOps","facebook":"ludovicwyffels","location":"Comines, Belgique","profile_image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAABMUlEQVQ4y2MwsXf5T03MMGogGBvbOYNpUwdXOIbJwdgwNQQNhCk0snX6r2thC8Y6QGxg7QCWA/FBcrgMxepCkAZrV6//EYmpYBydkvHfMzjiv5mj2//whJT/Nm7ecEMJGgjykp6V/f+AqLj/O/Ye+L8diA8dO/m/pbv/v72n3/+tu/f9D45N/K9naYcSFESFoRXQlSCX2rh7g/kgr1u5epIWhiAMCi/3wLD/fVNn/u+fNgtMT5g+6//kmXOB/Jn/PYLCwWrINnDijDlAPBvKJ8NAZC+DIgCkGYRBbJAYSV5GjpTte/b/PwiMkLj0nP8xqVn/Dxw9AY6kwOh40iIFlmzCgUkmKjnjvx0wdu08fMFskJi1mxfxyQY9YetAEzJyQic5YaNnPeTgIDnrjZaHJGMACtTMXoVAJ6sAAAAASUVORK5CYII=","aspectRatio":1,"src":"/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png","srcSet":"/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png 400w","sizes":"(max-width: 400px) 100vw, 400px"}}},"avatar":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAABMUlEQVQ4y2MwsXf5T03MMGogGBvbOYNpUwdXOIbJwdgwNQQNhCk0snX6r2thC8Y6QGxg7QCWA/FBcrgMxepCkAZrV6//EYmpYBydkvHfMzjiv5mj2//whJT/Nm7ecEMJGgjykp6V/f+AqLj/O/Ye+L8diA8dO/m/pbv/v72n3/+tu/f9D45N/K9naYcSFESFoRXQlSCX2rh7g/kgr1u5epIWhiAMCi/3wLD/fVNn/u+fNgtMT5g+6//kmXOB/Jn/PYLCwWrINnDijDlAPBvKJ8NAZC+DIgCkGYRBbJAYSV5GjpTte/b/PwiMkLj0nP8xqVn/Dxw9AY6kwOh40iIFlmzCgUkmKjnjvx0wdu08fMFskJi1mxfxyQY9YetAEzJyQic5YaNnPeTgIDnrjZaHJGMACtTMXoVAJ6sAAAAASUVORK5CYII=","aspectRatio":1,"src":"/static/5f2c129e42248a92c87b13b4293950cf/7c0ed/ghost.png","srcSet":"/static/5f2c129e42248a92c87b13b4293950cf/09f8c/ghost.png 50w,\n/static/5f2c129e42248a92c87b13b4293950cf/bf65b/ghost.png 100w,\n/static/5f2c129e42248a92c87b13b4293950cf/7c0ed/ghost.png 200w,\n/static/5f2c129e42248a92c87b13b4293950cf/fdbb0/ghost.png 300w,\n/static/5f2c129e42248a92c87b13b4293950cf/647de/ghost.png 400w","sizes":"(max-width: 200px) 100vw, 200px"}}}},"allMarkdownRemark":{"totalCount":32,"edges":[{"node":{"excerpt":"Comment utiliser la console JavaScript aller au-delà de console.log() L’un des moyens les plus simples de déboguer quoi que ce soit en JavaScript consiste à console.log des éléments à l’aide de console.log. Mais il y a beaucoup d’autres méthodes fournies par la console qui peuvent vous aider à mieux déboguer. Le cas d’utilisation très simple consiste à enregistrer une chaîne ou un groupe d’objets JavaScript. Tout simplement, Imaginons maintenant un scénario où vous devez vous connecter à la console pour regrouper plusieurs objets. Le moyen le plus intuitif de consigner cela consiste à simplement console.log(variable) une après l’autre. Le problème est plus évident quand on voit comment il apparaît sur la console. Aucun nom de variable visible Comme vous pouvez le constater, aucun nom de variable n’est visible. Cela devient extrêmement ennuyeux lorsque vous en avez beaucoup et que vous devez agrandir la petite flèche à gauche pour voir quel est exactement le nom de la variable. Entrez les noms de propriété calculés. Cela nous permet de regrouper toutes les variables ensemble dans un seul console.log({ foo, bar }); et la sortie est facilement visible. Cela réduit également le nombre de lignes console.log dans notre code. console.table() Nous pouvons aller plus loin en plaçant tout cela dans un tableau pour le rendre plus lisible. Chaque fois que vous avez des objets avec des propriétés communes ou un tableau d’objets, utilisez console.table(). Ici, nous pouvons utiliser console.table({ foo, bar}) et la console affiche: console.table en action console.group() Cela peut être utilisé lorsque vous souhaitez regrouper ou imbriquer des détails pertinents afin de pouvoir facilement lire les journaux. Cela peut également être utilisé lorsque vous avez quelques instructions de journal dans une fonction et que vous voulez pouvoir voir clairement l’étendue correspondant à chaque instruction. Par exemple, si vous enregistrez les détails d’un utilisateur: Logs groupés Vous pouvez également utiliser console.groupCollapsed() au lieu de console.group() si vous souhaitez que les groupes soient réduits par défaut. Vous auriez besoin d’appuyer sur le bouton de descripteur sur la gauche pour développer. console.warn() et console.error() Selon la situation, pour vous assurer que votre console est plus lisible, vous pouvez ajouter des logs à l’aide de console.warn() ou console.error() . Il y a aussi console.info() qui affiche une icône ‘i’ dans certains navigateurs. Log d'avertissement et d'erreur Cela peut être poussé plus loin en ajoutant un style personnalisé. Vous pouvez utiliser une directive %c pour ajouter un style à toute instruction de journal. Cela peut être utilisé pour différencier les appels d’API, les événements utilisateur, etc. en conservant une convention. Voici un exemple: Vous pouvez également modifier la font-size et le font-style ainsi que d’autres éléments CSS. Relevé des instructions console.log console.trace() console.trace() une trace de pile dans la console et indique comment le code s’est terminé à un moment donné. Il existe certaines méthodes que vous ne souhaitez appeler qu’une seule fois, comme la suppression d’une base de données. console.trace() peut être utilisé pour s’assurer que le code se comporte comme nous le voulons. console.time() Une autre chose importante en matière de développement front-end est que le code doit être rapide. `console.time() permet de chronométrer certaines opérations dans le code à des fins de test. Sortie console.time() pour les boucles Espérons que l’article fournisse des informations sur les différentes manières d’utiliser la console.","timeToRead":3,"frontmatter":{"title":"Comment utiliser la console JavaScript aller au-delà de console.log()","subtitle":"L'un des moyens les plus simples de déboguer quoi que ce soit en JavaScript consiste à `console.log` des éléments","tags":["Javascript","Node.js"],"date":"2019-04-18T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAtElEQVQoz2P4cNufbMRAV83vb4EQyZo/3/P/dNf/28OAr/cDIPoZiLQNqPPeaZ9nl3zP7fW6fNDr872A90RqfnvD/8/TwHmTnfpb7VMTLPasdfv2AGQ5sZr/vw2eM8lx8XTn6b2OM/ocifUz0AagVzcsds3LsFq3wGViu0N6kuWLK35AQQYigwroz5M7PN/e9Duwwf3WcR+Qn4l0NjBsgJ78/jDg4x3/n48CvpAU2sjRixzPABStw4O6JhVqAAAAAElFTkSuQmCC","aspectRatio":1.7777777777777777,"src":"/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png","srcSet":"/static/f10109a249e0ae09b060bb7269f1f4be/4c9af/js.png 930w,\n/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/js-console-log/"}}},{"node":{"excerpt":"Gatsby est un générateur de site statique pour React.js qui permet à l’utilisateur de créer des sites Web rapides et dynamiques. Gatsby se concentre sur l’optimisation des appareils mobiles et crée automatiquement une PWA (Progressive Web App). Le site Web est alimenté par diverses sources: Markdown, CMS, API, bases de données et bien d’autres. Pourquoi Gatsby.js? J’ai longtemps repoussé la relance de mon site, également parce que je n’avais pas trouvé de solution adéquate pour un blog. Souvent, la configuration était trop compliquée, les possibilités trop petites; mais je ne voulais pas non plus écrire du code HTML pur pour mes articles de blog. Avec la popularité croissante de React.js, je ne pouvais pas échapper à cet hype médiatique et me lire moi-même. Comme je pensais déjà en termes de composants dans la phase de conception, React.js était la prochaine étape logique. Au début de 2019, j’ai découvert Gatsby.js et j’étais enthousiasmé par la possibilité d’écrire mon site Web avec React ainsi que les messages de mon blog avec Markdown ou un CMS “headless” (par exemple, Contentful ou Prismic). Cela vous demande beaucoup de travail et vous permet de démarrer facilement. Magie (GraphQL) Pour obtenir les données des fichiers Markdown, des CMS, etc. dans React et enfin à l’écran, Gatsby utilise le langage de requête GraphQL. La communauté déjà mentionnée fournit certains plugins (par exemple, Contentful, Drupal, Lever, Medium, Wordpress, MongoDB, Markdown, Prismic) pour obtenir les données. L’utilisation ultérieure de GraphQL est un jeu d’enfant: Avec cette requête, vous obtenez tous les projets (y compris les données de Frontmatter) pour les sous-pages de projet, triés par date (en supposant une configuration adéquate). Afin de générer les images appropriées pour chaque taille d’écran, un autre plug-in, qui utilise la bibliothèque Sharp, prend les images définies dans le fichier Markdown et en crée des images dans plusieurs tailles. Si vous rencontrez des problèmes avec la requête, vous pouvez utiliser le débogueur graphique interactif GraphiQL pour afficher des suggestions et tester la requête vous-même - le débogueur crée automatiquement sa propre documentation. Ces données peuvent ensuite être utilisées dans les composants React comme ceci: Quoi d’autre… ? La création de mon site Web a été extrêmement rapide avec React. J’ai également beaucoup appris sur les PWA et continuerai à essayer d’améliorer le score de Lighthouse Score de Google. Le site Web est open source et peut être consulté sur GitHub à des fins d’apprentissage.","timeToRead":2,"frontmatter":{"title":"Bienvenue sur mon blog","subtitle":"Pourquoi Gatsby.js?","tags":["Gatsby","Node.js","TypeScript","Javascript"],"date":"2019-04-17T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAP/aAAwDAQACEAMQAAABZtCCpGB//8QAGhAAAgIDAAAAAAAAAAAAAAAAAgMAEQEEEP/aAAgBAQABBQLZFsSBAqsy+//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAAICAwAAAAAAAAAAAAAAAAIRAEEBEiD/2gAIAQEABj8CxqSGJXfP/8QAGxAAAgMAAwAAAAAAAAAAAAAAAAERITEQQaH/2gAIAQEAAT8h7iKcaJTSdi0a/OGjpn//2gAMAwEAAgADAAAAEMTf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFhEBAQEAAAAAAAAAAAAAAAAAABFB/9oACAECAQE/EMR//8QAHBABAQADAAMBAAAAAAAAAAAAAREAITFBYZHB/9oACAEBAAE/EO7kievEm3HxoqUY9Z+OJwL6wMdGMrXc5Bfuf//Z","aspectRatio":1.5037593984962405,"src":"/static/001de70656809513310eba4aaa7e901c/883ab/blog.jpg","srcSet":"/static/001de70656809513310eba4aaa7e901c/f8f18/blog.jpg 930w,\n/static/001de70656809513310eba4aaa7e901c/0e6ff/blog.jpg 1860w,\n/static/001de70656809513310eba4aaa7e901c/883ab/blog.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/gatsby-welcome/"}}},{"node":{"excerpt":"J’ai récemment commencé à utiliser OpenVPN et à profiter des différentes options qu’il propose, notamment le masquage du trafic VPN sous TCP:443 (autrement appelé HTTPS). Personnellement, j’aime utiliser GCE pour héberger mes VPN, car je profite de la puissance et de la personnalisation que cela procure. Cependant, il n’y avait absolument aucun tutoriel sur la configuration d’OpenVPN sur GCE. Il existe 10 étapes dans ce didacticiel, mais chaque étape peut exécuter plusieurs lignes de code. Si vous essayez simplement d’ajouter plus de clients, passer aux étapes 4, 9 et 10. Table des matières [toc] 1. Création de l’instance Nous voulons utiliser une instance n1-standard-1 pour sa durabilité et son coût. Dirigez-vous vers le moteur de calcul et cliquez sur “CREATE INSTANCE”.  Configurez-le avec les options affichées, mais n’appuyez pas encore sur “Créer” Si vous ne voyez pas l’image, nous utilisons une instance n1-standard-1 (1 vCPU, 3,75 Go de mémoire) avec Ubuntu 16.04 LTS installé (vous pouvez choisir un espace de stockage, mais 10 Go suffisent). Puisque notre VPN utilisera TCP: 443, nous devons autoriser le trafic HTTPS maintenant ou plus tard. Vous pouvez donc également le faire immédiatement. Appuyez sur le bouton “Gestion, sécurité, disques, réseau, …” pour ouvrir les options avancées. Cliquez sur l’onglet Réseau et cliquez sur le bouton d’édition en regard de l’interface réseau sélectionnée. Cliquez sur le menu déroulant “Adresse IP externe” et sélectionnez “Créer une adresse IP”. Entrez un nom, puis appuyez sur “Réserver”. Cette opération associera une adresse IP statique à votre instance, ce qui est probablement ce que vous voulez, sauf si vous souhaitez régénérer les fichiers de configuration de quelque temps. En ce qui concerne les frais, Google ne facture pas les adresses IP statiques pendant leur utilisation. La machine n1-standard-1 ne devrait pas vous coûter plus de 30 $ par mois. Certes, cela coûte un peu plus cher que la plupart des services VPN, mais la plupart des services VPN ne vous donnent pas un contrôle total :)  Vos interfaces réseau doivent ressembler à l’image. Veillez à activer le transfert IP, car cela ne peut plus être modifié après la création de l’instance. En règle générale, vous souhaiterez probablement garder votre adresse IP externe secrète. Cela ne me dérange pas de rendre celui-ci public, car cette instance n’existera pas lorsque vous lirez ce tutoriel, et l’adresse IP appartiendra probablement à quelqu’un d’autre. Appuyez sur “Terminé” et “Créer”. Une fois l’instance prête (un bouton “SSH” noir apparaîtra), cliquez ou double-cliquez sur le bonton “SSH” pour ouvrir une fenêtre SSH. Le reste de la configuration se fera sur cette machine, que nous appellerons le “server”. 2. Installation OpenVPN et EasyRSA OpenVPN est (évidemment) le serveur VPN que nous utilisons, et EasyRSA est un paquet qui nous permettra de configurer une autorité de certification interne à utiliser. Bien que ce ne soit généralement pas le cas, pour cette configuration, il est souvent nécessaire de lancer une mise à jour d’apt pour pouvoir installer un ou les deux packages. Les deux d’entre eux sont dans les dépôts par défaut d’Ubuntu. Temps de configurer. 3. Mise en place du CA Comme OpenVPN utilise TLS/SSL, il a besoin de certificats pour chiffrer le trafic. Pour cela, nous devrons émettre nos propres certificats de confiance, ce que nous pouvons faire à l’aide de la CA que nous sommes sur le point de configurer. Commençons par copier le easy-rsa de modèles du paquet easy-rsa. Il y a des valeurs que nous pouvons vouloir éditer, alors allons-y et nano vars. Si vous voulez utiliser vim, continuez, mais puisque j’essaie de garder mes tutoriels aussi simples que possible, j’utilise nano. À partir de ce moment, s’il y a une partie d’une commande qui dépend d’une valeur que vous choisissez, elle sera en gras. Tout le code qui n’est pas généré par un utilisateur sera en italique. Faites défiler vers le bas du fichier (ce n’est pas trop long) et vous devriez trouver ce qui suit: Éditez autant d’entre eux (ou aucun d’eux) que vous le souhaitez. Juste en dessous de ces valeurs devrait être export KEY_NAME=\"EasyRSA\". Pour simplifier notre configuration, nous allons modifier cette ligne comme suit: Enregistrez et fermez le fichier (ctrl + x, y, entrez). Les variables que nous venons de modifier seront utilisées pour le processus de signature de notre autorité de certification. Toujours dans le openvpn-ca, exécutez la commande suivante: Si vous avez bien fait, le re NOTE: If you run ./clean-all, I will be doing a rm -rf on /home/username/openvpn-ca/keys. C’est ce que nous voulons. Pour assurer un environnement de travail propre, nous allons lancer ./clean-all. Maintenant, construisons le CA Toutes les variables que nous avons déjà définies doivent se renseigner elles-mêmes. Il suffit donc d’y accéder en appuyant plusieurs fois sur Entrée. À la fin, vous aurez une autorité de certification prête à commencer à signer. Nous avons également besoin d’un certificat de serveur et d’une clé de cryptage pour garantir la sécurité de notre trafic. Créons notre certificat de serveur et notre clé. Passer à nouveau les invites de commande. Nous n’entrerons pas un mot de passe, cette fois. Les deux dernières invites vous obligent à entrer y pour signer le certificat. Assurez-vous de ne pas les ignorer! Comme indiqué précédemment, nous avons également besoin d’une clé de chiffrement. Pour les besoins de ce didacticiel, nous allons générer une clé Diffie-Hellman, qui a tendance à être plutôt forte. Bien sûr, une grande force entraîne une grande inefficacité. Par conséquent, quelle que soit votre système, la commande suivante prendra probablement quelques minutes. Nous renforcerons cela avec une signature HMAC, afin de nous assurer que la vérification de l’intégrité TLS est plus sûre. 4. Générer un certificat client Naturellement, si vous allez utiliser une autorité de certification, votre client doit également disposer d’un certificat. Bien que vous puissiez le faire sur votre ordinateur client et que le serveur le signe, nous essayons de garder les choses simples et hébergées sur un seul ordinateur. Si vous avez plusieurs clients, vous pouvez suivre cette étape plusieurs fois. Assurez-vous simplement de rendre vos noms de client uniques. Assurez-vous que vous êtes dans le openvpn-ca et que votre fichier vars est synchronisé. Maintenant, nous allons construire une clé client en tant que telle: Une fois de plus, tout doit être pré-rempli. Passez donc tous les éléments, sauf les deux dernières invites, qui vous demanderont de signer en entrant y. 5. Configurer le serveur OpenVPN OpenVPN s’installe sous le répertoire /etc/openvpn. Pour que tout fonctionne, nous devons déplacer certains fichiers dans ce dossier. Par défaut, OpenVPN est fourni avec un exemple de configuration. Par souci de simplicité, nous simplifierons la décompression dans notre dossier de configuration. Ensuite, nous allons éditer le fichier de configuration: La première étape consiste à trouver la directive tls-auth. Il y aura un point-virgule (;) à côté de la directive, que nous supprimerons. En dessous, nous ajouterons une ligne. Nous devons également chiffrer notre serveur, éditons donc les directives de chiffrement juste en dessous de cette section. Plus précisément, nous décommentons la ligne AES-128-CBC et ajouterons une directive auth. Ensuite, les paramètres d’utilisateur et de groupe: Facultativement, nous pourrions opter pour l’envoi de tout le trafic via le VPN. Pour cela, recherchez la directive redirect-gateway et décommentez-la. Juste en dessous, il devrait y avoir quelques lignes d’dhcp-option. Décommentez ceux-là aussi. Optionnellement, nous pouvons vouloir changer le port et le protocole sur lesquels OpenVPN fonctionne. La valeur par défaut est UDP: 1194, mais si votre réseau bloque les connexions VPN, ce sera probablement l’une des victimes. Le déguisement serait d’utiliser TCP: 443, qui est le port HTTPS. Si vous n’avez pas utilisé «serveur» comme nom de serveur, vos fichiers crt ont un nom différent. Mettez-les à jour en conséquence. Enregistrez et fermez le fichier. 6. Préparer Ubuntu Bien que nous ayons déjà configuré le transfert IP, etc., nous devons apporter quelques modifications pour activer ces options. Recherchez la ligne suivante et supprimez le # (caractère de commentaire). Sauver et fermer. Pour mettre à jour les paramètres de session, exécutez: Ensuite, nous devons trouver et mettre à jour nos règles de pare-feu (UFW) pour masquer les clients. La première étape consiste à trouver l’interface sur laquelle nous fonctionnons: L’interface que nous voulons est celle qui contient le mot “dev”. Dans notre cas, cela ressemble à ceci: Donc, notre interface est ens4. Avec cela, nous mettrons à jour nos règles de pare-feu: Au-dessus, là où il est indiqué Don't delete these required lines... ajoutez le code suivant: Sauver et fermer. Ensuite, nous devons transférer les paquets. Recherchez la directive DEFAULT_FORWARD_POLICY et remplacez-la de \"DROP\" par \"ACCEPT\". Sauver et fermer. 7. Lancer OpenVPN Pour démarrer le serveur, exécutez ce qui suit: Pour vérifier qu’il a bien démarré, lancez: Si tout se passe bien, vous devriez voir une sortie incluant Active: active (running). Vous devrez peut-être q sur q pour quitter le panneau d’informations. Si cela vous convient, liez le service à la séquence de démarrage. 8. Configuration d’une structure de configuration client Pour faciliter la configuration des configurations client, nous allons d’abord créer une structure. Pour commencer, créez un dossier de configuration pour stocker les fichiers de configuration du client. Les clés du client seront dans ces configurations, alors verrouillons les permissions sur le répertoire files. Copiez l’exemple de configuration. Éditons le fichier: Trouvez la directive remote. Remplacez my-server-1 par l’adresse IP externe publique attribuée à votre instance GCE. Si vous avez choisi un port autre que 1194, mettez-le à jour en conséquence. Mettez également à jour votre protocole. Décommentez l’utilisateur et le groupe: Trouvez les directives ca, cert et key, commentez-les, car nos configurations les incluront automatiquement. Utilisez les mêmes paramètres de chiffrement et d’authentification que précédemment: Quelque part, nous devrons ajouter la key-direction. Assurez-vous d’utiliser 1, comme c’est le cas pour le client. 0 était pour le serveur. Si votre configuration client actuelle est (ou sera) utilisée sur un périphérique Linux, ajoutez les éléments suivants: Notez que si vous les incluez dans un environnement non-Linux (Android et macOS inclus dans un non-Linux), vos clients peuvent se comporter de manière étrange. Sauver et fermer. Ensuite, nous devons écrire un script pour générer nos configs client rapidement et facilement. A l’intérieur, collez ce code: gen_config.sh Le {1} fait ici référence au premier argument, qui sera notre nom de client. Assurez-vous de mettre à jour tiv.key fonction du nom de votre clé HMAC. Autoriser l’exécution de ce script: 9. Générer des configurations client Le pas que vous attendiez tous est enfin arrivé. Nous allons générer nos configurations client. Vérifiez que cela a fonctionné en exécutant: Si tel est le cas, il devrait maintenant y avoir un fichier client.ovpn dans ce répertoire. Nous devons télécharger ce fichier et le transférer sur nos appareils. Pour ce faire, cliquez sur l’icône représentant une roue en haut à droite de la session SSH, puis sélectionnez «Télécharger le fichier». Le chemin pleinement qualifié devrait ressembler à ceci: 10. Installation sur les clients Pour Windows, téléchargez l’application client OpenVPN. Une fois installé, déplacez votre fichier client.ovpn vers C:\\Program Files\\OpenVPN\\config. Pour ouvrir OpenVPN, vous devez exécuter l’application en tant qu’administrateur. Une fois à l’intérieur, vous devriez voir et pouvoir vous connecter à votre VPN. Pour macOS, la plupart des gens suggèrent d’utiliser Tunnelblick bien que tout autre client OpenVPN (même s’il soit sans danger) soit probablement bon aussi. Une fois installé, vous devriez pouvoir faire glisser n’importe .ovpn fichier .ovpn sur l’icône Tunnelblick de la barre de menus pour installer la configuration. Utilisez la barre de menus pour vous connecter ensuite au VPN. Si vous êtes interrogé sur le plugin down-root.so, prenez une décision! Je l’ai sauté parce que je suis capable de reconnecter ma connexion, mais c’est à vous de décider. Sous Linux, installez le paquet openvpn en utilisant votre gestionnaire de paquets. Les utilisateurs de CentOS devront d’abord installer epel-release utilisant yum. Une fois installé, lancez ls /etc/openvpn. Si la sortie ne présente pas de fichier update-resolve-conf, vous devez modifier votre fichier .ovpn et supprimer les lignes suivantes (si vous les avez ajoutées): Si vous n’avez jamais ajouté ces lignes mais trouvé un fichier update-resolv-conf, ajoutez-les! Les utilisateurs de CentOS devront modifier le group dans le fichier .ovpn de nogroup à nobody pour se conformer aux normes du système d’exploitation. Pour vous connecter, lancez sudo openvpn — config client .ovpn. Pour iOS, installez l’application OpenVPN Connect. Ensuite, ouvrez iTunes sur votre ordinateur et accédez à iPhone> Applications. Faites défiler jusqu’à la section «Partage de fichiers» et sélectionnez “OpenVPN”. Faites glisser votre fichier .ovpn dans le panneau “OpenVPN Documents”. Ouvrir l’application sur votre téléphone devrait maintenant afficher un message indiquant qu’un nouveau profil est prêt à être importé. Une fois importé, vous devriez pouvoir vous connecter au VPN. Pour Android, installez l’application OpenVPN Connect. Ensuite, transférez votre fichier .ovpn n’importe où / de toute façon sur votre appareil (je viens d’utiliser Google Drive, mais cela peut ne pas être recommandé). Dans l’application, ouvrez le menu et cliquez sur “Importer”. Accédez à votre fichier .ovpn , cliquez dessus, puis appuyez sur “IMPORT” en haut à droite. Une fois terminé, vous devriez pouvoir vous connecter au VPN. Test du VPN Le moyen le plus simple de tester votre VPN serait de Google “quelle est mon adresse IP” avant et après la connexion. Votre adresse IP doit passer à l’adresse IP publique externe de votre instance GCE une fois que vous êtes connecté au VPN, puis à l’adresse IP de votre réseau une fois déconnectée du VPN. Si tout s’est bien passé, c’est tout! Votre nouveau VPN est prêt à être utilisé. Crédits Cet article est basé en grande partie sur ce didacticiel de la communauté DigitalOcean. Bien que la configuration elle-même soit assez similaire, j’ai pensé écrire un article séparé, car il existe quelques éléments clés différents dans GCE et il est assez facile de se tromper si vous le faites pour la première fois (je parle par expérience).","timeToRead":14,"frontmatter":{"title":"Configuration d'un serveur OpenVPN sur Google Compute Engine","subtitle":"","tags":["Google Cloud","OpenVPN","VPN"],"date":"2019-04-14T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEA/9oADAMBAAIQAxAAAAHtZYUFyf/EABkQAAIDAQAAAAAAAAAAAAAAAAECABESIP/aAAgBAQABBQLRss023H//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAABBAMAAAAAAAAAAAAAAAAAAQIRICEiMv/aAAgBAQAGPwJdTDJOKf/EABoQAAICAwAAAAAAAAAAAAAAAAABESEgMUH/2gAIAQEAAT8hqJULbAuzrD//2gAMAwEAAgADAAAAEPz/AP/EABcRAQEBAQAAAAAAAAAAAAAAAAEAETH/2gAIAQMBAT8QUTll/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARMf/aAAgBAgEBPxAEe2l//8QAHBABAQACAgMAAAAAAAAAAAAAAREAISAxQVFh/9oACAEBAAE/EBhvcvvVw0sN8wxo2hndH6a4f//Z","aspectRatio":1.7763157894736843,"src":"/static/3bf8405e70096f9e75e8c849f54b0d1f/de998/openvpn.jpg","srcSet":"/static/3bf8405e70096f9e75e8c849f54b0d1f/de998/openvpn.jpg 810w","sizes":"(max-width: 810px) 100vw, 810px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/openvpn-google/"}}},{"node":{"excerpt":"Principales commandes SQL/PostgreSQL que vous avez besoin de connaître. Ici, nous allons passer en revue les commandes les plus importantes et les plus élémentaires que vous devez connaître pour créer, gérer et supprimer vos données sur votre base de données. Outre les commandes ici, j’utilise la syntaxe PostgreSQL. La plupart des bases de données relationnelles partagent les mêmes concepts. En apprenant cela, vous pourrez créer et gérer votre base de données. Ce sont les commandes que vous pouvez vous attendre à apprendre ici: [toc] Create table Créons notre premier client: Creusons par étapes: Créez la table avec le nom CREATE TABLE <NAME> (qui pour nous est “customer”). Ajoutez les colonnes et le type de données approprié, comme l’email CHARACTER(255) C’est simple, nous n’entrerons pas dans le type de données que ous pourrions utiliser, car il est long et peut changer de base de données à une autre base de données, l’idée est d’apprendre le principe. Insert Nous passons maintenant à notre premier tableau et vous souhaitez maintenant à notre premier tableau et vous souhaitez maintenant ajouter des informations, sinon quel est le but, pour cela, vous lancerez cette commande: Le code parle tout seul, nous voulons ajouter <INSERT INTO> notre table appelée “customer” pour la colonne portant le nom de clé “name” et “age” les VALUES suivant l’ordre, nommez ”Felipe” et l’âge 31. Alter Table Il est normal que vous créiez votre base de données et que vous commenciez par la base, puis que vous commenciez à vous améliorer, et avec la commande, vous puissiez modifier la structure de la table. Mais bien sûr, ce n’est pas illimité si vous essayez de modifier des données existantes et susceptibles de provoquer un conflit, comme le passage d’un nombre entier à une chaîne . Comme vous pouvez le constater, nous ajoutons une nouvelle COLUMN avec le nom height et le type INTEGER.  Cette commande a beaucoup d’autres propriétés, vous pouvez donc consulter la documentation afin de voir toutes les autres variations. Update  Nous avons ajouté des données à l’intérieur, mais considérons que vous avez commis une erreur ou, en fait, que vous souhaitez simplement mettre à jour, car c’est l’idée, pouvoir conserver des données et les manipuler. Ce sera utile non? Comme nous venons de modifier notre table et que nous ne voulons pas avoir de valeur vide pour une raison quelconque, nous faisons un UPDATE sur notre table client avec cette commande et SET la hauteur pour que tout le monde soit à 0, comme une valeur par défaut. Mais je ne veux pas faire cela à tout le monde, bien sûr, alors ce qui suit est plus spécifique. Nous avons fait la même chose qu’avant, à la différence que nous utilisons une nouvelle clause WHERE préciser ce que nous voulons changer, je parlerai plus en détail de cette question. Pour le moment, il vous suffit de comprendre que, si vous utilisez cette clause, je valide si “name” correspond au nom, et seulement s’il est vrai, je mets à jour les valeurs. Et ce genre de validation est vraiment important, et parce que ceci est une autre façon de faire avec encore plus de “validations”. Nous avons deux différences importantes où, voyons: Notre clause WHERE a maintenant une autre clause appelée AND , et cela nous permet de “concaténer” deux valeurs que nous voulons vérifier, nous disons que les deux doivent correspondre pour que notre validation soit réussie . Il est important de noter qu’ils sont à l’intérieur de ”()”, il s’agit en quelque sorte de “retenir” en tant que valeur ou contrainte , nous vérifions en gros ce qui est à l’intérieur qui donnera le “résultat” qui sera vrai ou faux . Le second est OR et correspond exactement à ce que suggère le “nom”. Si notre clause WHERE ne convient pas, nous vérifions celle-ci. Upsert En plus du nom qui a l’air bizarre, cette commande est très utile, car nous parlons de mise à jour, l’un des scénarios les plus courants consiste à vouloir mettre à jour quelque chose qui existe déjà, mais comment savez-vous qu’il existe déjà? Pour cela, vous pouvez généralement penser, commencez par vérifier s’il existe et si vous utilisez la commande update, non? Mais en réalité vous savez déjà ce que vous voulez, vous voulez que si cette valeur est déjà mise à jour. PostgreSQL implémente cela il n’y a pas si longtemps, comme d’autres bases de données l’avaient déjà assez longtemps, voyons comment implémenter l’utilisation de Postgres. Vous devez d’abord savoir que nous disposons de deux contraintes que nous pouvons utiliser pour “dire” ce que nous voulons faire si cette valeur existe déjà. ON CONFLICT DO NOTHING ON CONFLICT DO UPDATE Par le nom, vous pouvez déjà imaginer ce qui se passe, lors de la mise à jour si vous trouvez un “conflit” avec un autre, ne faites rien. Regardons comment utiliser ON CONFLICT DO NOTHING : En gros, ce qui va se passer n’est rien, si la base de données trouve déjà une valeur, ne fait rien. D’autre part pour ON CONFLICT DO UPDATE : Voici exactement ce que dit le nom, si vous trouvez un conflit, mettez-le à jour. Il est simple d’utiliser ce scénario lorsque vous savez que vous souhaitez mettre à jour lorsque vous avez la même valeur. Rappelez-vous que l’implémentation change légèrement pour chaque base de données, mais qu’il est important de connaître le concept, il suffit de regarder sur Internet le nom de contrainte spécifique de votre base de données. Select Maintenant que nous avons en principe commencé le début en suivant une méthode logique, comme créer votre base de données, mettre à jour, supprimer, la plupart des bases afin de configurer votre base de données, nous devons obtenir les données. Très probablement, ce sera l’une des commandes les plus utilisées par vous, alors voyons-le. La base est très simple, allons-y étape par étape: SELECT * - Ce que vous faites est de dire “je veux” (SELECT) tout (*); FROM - D’où je veux (FROM) c’est à partir du nom de la table (customer). Ici, je reçois tout ce qui se trouve dans ma table client, soyons plus précis. Maintenant que vous avez déjà compris comment vous créez, vous avez probablement compris que nous voulions des données spécifiques, à savoir les données des colonnes “height” et “name” de ma table customer . Cette contrainte offre de nombreuses autres possibilités dont vous avez absolument besoin dans la documentation pour pouvoir visualiser toutes les autres variantes. Un de plus, est pour la situation que vous ne voulez pas réellement sélectionner par le nom de la colonne, mais si elle correspond à la valeur, essayez ceci: Comme je l’ai dit, nous rechercherons des données dans la colonne “hauteur” et toutes les valeurs correspondant à la chaîne “Felipe” en regard du nom de la colonne correspondante. Where Cette contrainte est une autre que vous utiliserez encore plus avec la contrainte SELECT . Voyons comment nous utilisons, vous utiliserez ceci pour filtrer ce que vous voulez lors de la sélection de vos données. Comme vous pouvez le constater, nous recherchons la taille du client et WHERE la “height” est supérieure à 50 . Mais vous pouvez ajouter encore plus de contraintes (filtres). Nous avons ajouté ici une nouvelle contrainte AND qui fait exactement ce que son nom suggère, qui doit être plus haut que 50 ET inférieur à 80. Une autre contrainte que nous pouvons utiliser est OR. Ici nous avons ajouté une autre contrainte OR cela signifie que si notre vérification “précédente” ne correspond pas comme son nom l’indique “OU”, nous retournons sous forme de correspondance. Join C’est certainement un sujet très important, non seulement parce que dans un entretien d’embauche, ils vous le demanderont certainement, mais aussi parce qu’il y a beaucoup de possibilités. Même si la première fois que vous essayez de comprendre, ça a l’air bizarre. Pour cela nous avons 5 types: CROSS JOIN INNER JOIN LEFT JOIN RIGHT JOIN FULL JOIN Je vais essayer d’expliquer de la manière la plus simple, mais je vous recommande d’essayer cela en direct. Lorsque vous le ferez, vous comprendrez mieux et vous verrez comme cela est facile. Pour cela, nous avons besoin de quelques données pour pouvoir visualiser. Pour cela, je vais mettre une donnée à insérer, je reconnais que vous savez déjà comment créer une table. Nous aurons notre client et la table de contact . Nos données clients: Et pour nos coordonnées: Vous devez juste exécuter ceci sur votre table déjà créée et l’insérer. Cross join Je dirais que vous n’utiliserez pas trop celle-ci, car elle peut générer beaucoup de données. En gros, il joint ici les deux tables et génère une sortie avec toutes les données. La façon dont cette contrainte fonctionne consiste à renvoyer des données de produit cartésiennes entre les deux tables. Cette commande multiplie la première table par la quantité de données de la seconde table. Elle renverra toujours la combinaison possible. Je crois que vous pouvez imaginer combien de données cela peut créer, non?! Il est facile de voir la représentation de la façon dont cette requête sera traitée à partir de cette image, mais cela ne correspond pas seulement:  Allons à la commande: Je n’ajouterai pas une impression de la sortie complète ici, car elle est trop grande, mais cette opération générera un total de 36 éléments/lignes, c’est-à-dire la multiplication entre le premier client de la table ayant 9 éléments multiplié par la table de contacts contenant 4 Mais une information précieuse ici est que, avec cela, nous allons générer une sortie qui contient toutes les lignes de la première table et toutes les lignes de la seconde:  Et pourquoi répéter table? Eh bien, nous avons SELECT *, nous avons choisi d’obtenir toutes les données des deux, mais montrons-leur comment ne pas répéter ou, mieux, comment choisir la ligne de sortie que nous voulons. Utilisez cette requête: Une partie de la sortie:  Si vous faites attention, ce que nous avons fait était de dire comment sera généré, en spécifiant la ligne que nous voulons et d’où customer.\"name\" et contact.zip_code afin que la sortie suive cet ordre. Je n’ai pas ajouté la “fin” de cette requête, ni même la précédente, mais parlons du concept de “jointure”, nous cherchons fondamentalement une correspondance, mais pour cette contrainte, nous “obtenons tout” à partir d’une table et “Tout à partir de” deuxième table, mais dans le type CROSS si un élément ne correspond pas retournera quand même, mais s’il n’y a pas de correspondance, la valeur sera vide.  Parce que si vous faites attention à notre table de contact pour l’utilisateur “felipe@gmail.com ”, il n’y a pas de correspondance avec personne à la table des clients , mais sera quand même retournée. Inner join Maintenant que nous savons déjà quelque chose de base concernant ce que nous pouvons faire en matière de jointure , passons à la suivante, qui ressemble beaucoup à la jointure croisée, mais avec une simple différence qu’ici ne renverra que des correspondances. C’est la représentation du type de jointure que nous faisons ici entre les tables, car vous ne voyez que le “centre” qui correspond à ce que nous aurons comme données.  Ici, la syntaxe est un peu différente, nous devons ajouter une contrainte supplémentaire, celle qui recherchera la ligne “validateur “. La principale différence que nous avons ici est que nous allons rechercher une correspondance en utilisant la table client et la table de contact , pour cela nous utiliserons la ligne “zip_code “. C’est ce que vous aurez comme récupération de données.  Il est facile de voir que lorsque nous retournons toutes les cellules, nous avons le code postal de la cellule du client et du contact , toutes les correspond. Cette jointure est très utile pour récupérer des données que vous souhaitez faire correspondre à une valeur spécifique, différente de la relation ay. Left joint Comme vous le savez peut-être, l’objectif principal est de faire correspondre les données et, si les données correspondent, nous renvoyons cette ligne. Vous pouvez voir ici que nous avons au centre la partie jointure , c’est-à-dire le validateur, et la partie gauche qui signifie que si la jointure ne correspond pas, les données seront renvoyées mais avec la ligne pour les données qui ne correspond pas “vide”.  Mais ici maintenant, nous commençons à faire les choses différemment. Voyons en quoi consiste cette requête: La “transcription” de ce que nous demandons ici est la suivante: nous voulons toutes les données de LEFT qui, dans notre scénario, sont des clients , et nous voulons uniquement contacter la table celles qui correspondent à notre validation. En faisant cela, vous devriez avoir ces données comme résultat de cette requête:  Right join Maintenant que nous avons déjà vu comment il est laissé joindre cela n’est pas différent, la seule différence est que nous voulons maintenant que le côté “droit” soit la table principale. Cela signifie que nous renverrons toutes les données de la table droite même si elles ne correspondent pas. Bien sûr ce qui n’a pas de correspondance est vide.  Question: Mais il est important de ne pas se méprendre pour obtenir toutes les données de droite ou de gauche avec uniquement la quantité exacte de données dont nous disposons à cette table spécifique. Ce que je veux dire par là, c’est que pour cette jointure, nous voulons “tout” du côté droit, c’est notre table de contact qui contient 4 éléments que nous avons ajoutés, et voici le résultat:  Nous avons 6 éléments et non 4, mais rappelez-vous, ce n’est pas une requête de “tout renvoyer d’un côté s’il y a correspondance ”, c’est une requête qui renvoie tout d’ un “côté” qui correspond, nous avons 2 correspondances pour le code postal 3423PO , c’est un match alors revenez. Full join Maintenant le dernier, qui ressemble à la jointure croisée, la différence ici est que nous recherchons une correspondance et non un produit cartésien comme la jointure croisée. Nous chercherons la jointure dans toute la zone bleue, en validant fondamentalement les deux côtés en fonction de notre contrainte de “validation”, qui dans notre scénario est zip_code.  La jointure complète combine les deux tables, les données gauche et droite, et renvoie toutes les correspondances et toutes les correspondances , regardons la requête Voici ce que nous aurons comme retour:  Ici, nous suivons le même principe que les “jointures” avant, c’est ce qui n’a pas de correspondance nous avons encore les données, mais “vide” Union Cette commande est très utile, car il est question d’obtenir des données à partir de 2 tables, mais ne confondez bien sûr pas les deux. Ce que fait Union, c’est que nous allons combiner les données des deux côtés, mais que nous ne renverrons que des données “uniques” entre les deux, si “identiques” ne renvoient pas les doublons, par exemple, c’est ce qui se produit avec join. C’est la requête que nous utilisons: C’est ce que nous aurons comme retour:  Comme vous pouvez le constater, nous spécifions les colonnes que nous voulons, ce qui est obligatoire, car nous devons préciser lesquelles sont “égales” afin qu’il puisse comparer et renvoyer des éléments uniques entre les deux. Une chose importante concernant cette commande est qu’il opère de manière très efficace, cela signifie que l’ordre peut changer. Order C’est une contrainte très utile pour organiser les données que vous demandez. Par conséquent, nous ne “filtrons” pas les données. En fait, il organise votre réponse en fonction de certains critères. Simplement, ordonnons notre requête en fonction de l’âge de nos clients: Et voici la réponse à laquelle vous devez vous attendre:  Alias Ceci n’est pas destiné à interroger quelque chose ou quelque chose comme ça, c’est juste un moyen de vous aider à organiser le nom d’une table, c’est simplement un surnom . La syntaxe de base est la suivante: Et vous devriez obtenir quelque chose comme ça:  Séparons pour chaque partie, d’abord ce que nous avons fait, nous sélectionnons ce que nous voulons, mais si vous faites attention, nous ajoutons un caractère ”A” avant le nom de la colonne , dans notre nom de scénario et code postal . Mais nous devons dire à notre code SQL ce que ce personnage ou “pseudo” signifie, n’est-ce pas?! Et nous le faisons après, lorsque nous disons FROM customer A, nous disons ici comment SQL devrait interpréter cette lettre, ce devrait être l’alias du client . Vous pouvez également le faire lorsque vous avez plusieurs tables, par exemple: Vous pouvez voir que les deux sont “identiques”, mais nous renommons pour avoir un autre alias. Une autre chose que vous pouvez faire est de “changer” le nom de la table que vous retournez, par exemple: Si vous voyez ici, j’ai utilisé deux formats d’alias, pour le premier, je n’utilise pas l’alias pour savoir quelle colonne est, mais ce que je veux, c’est que le retour de cette colonne soit appelé “Âge de la personne” et le second est l’alias façon de l’appel que nous avons déjà appris, c’est le résultat.  Delete Comme son nom l’indique, il s’agit d’une suppression, et je n’ai pas besoin de vous avertir qu’il s’agit d’une commande très “dangereuse”, alors faites attention, je vais montrer 2 syntaxe de base: Vous avez peut-être remarqué que nous ne spécifions aucune donnée particulière. Nous allons donc tout supprimer, c’est simple, mais nous souhaitons supprimer une donnée spécifique. Pour cela, procédez comme suit: Ici, je dirais que la façon la plus sûre de faire, utilisez une personne spécifique. Drop table Le sujet est sur la suppression, continuons avec ceci, la commande précédente est de supprimer les données d’une table / colonne, maintenant nous allons continuer, mais avec un autre type de suppression, celle qui supprime la table ou la supprime. C’est simple comme ça, mais vous devez être encore plus prudent que la commande delete . Ici, vous allez laisser tomber la table des trous et il n’y a aucun moyen de récupérer. Une autre différence majeure est que cela ne devrait pas être utilisé dans votre “programme”. Cette commande ne devrait être utilisée que par la commande de terminal lorsque vous savez ce qui doit être fait ou supprimé.","timeToRead":17,"frontmatter":{"title":"Principales commandes SQL/PostgreSQL que vous avez besoin de connaître","subtitle":"","tags":["SQL","PostgreSQL"],"date":"2019-04-14T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAABSUlEQVQoz2P4jwT+/fsHJB8/ftw9Z/WMFdsmL9nYPW9d66zVs1ds/vL5E1wBHDD8Rwd/ZyzbqJXYF9uxyrlktnZSf3THGuf8SYeOngDJ/f2LV/O/39OWbTbOmJTRtyarf5117tTSmdvSOpedPH2WgGaI3K4DRywzJwQ1LAuuX2yfPz2uc3XfvNWfPrwj4GyI3LMnj/xLJsbXzWxfuE03eUJ8+4rJsxddvXqFsLP/AvX/+3fg8LHV6zctWLVJP21iYuPsM2dOv/vwgZgAg7gfpOjS5atGyd1N05f///8bqzJ0zdevX799+/bTp08fPHj45MnTRctXHz194cHLDzcevfqD6mYsmk+fPr1v377169dv27Zt69ZtO7dvPXb28pErD7ccu/bq/ReQy/E7+/fv38CA+f79+7Nnz/6BAdDOn7/+ELYZovo/cQAAExNKPTC47rUAAAAASUVORK5CYII=","aspectRatio":1.7777777777777777,"src":"/static/769fd2188f26939bc5770b83a10e20f3/9ecf6/postgres.png","srcSet":"/static/769fd2188f26939bc5770b83a10e20f3/4c9af/postgres.png 930w,\n/static/769fd2188f26939bc5770b83a10e20f3/9ecf6/postgres.png 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/sql-principes/"}}},{"node":{"excerpt":"Ces trois fonctions sont utiles pour parcourir une liste (ou un tableau) et effectuer une sorte de transformation ou de calcul. Cela produira alors une nouvelle liste ou le résultat du calcul effectué sur la liste précédente. Les types Avant de plonger dans map, filter et reduce, configurons la liste. types.ts Chaque objet de l’assistant a un nom, une maison et le nombre de points qu’il a gagnés pour sa maison. Déclarons un groupe de magiciens et mettons-les dans une liste. wizards.ts Map Maintenant que nous avons la base, allons-y. La première fonction est la plus simple, map. Map itère (ou boucle) sur une liste, applique une fonction à chaque élément de cette liste, puis renvoie une nouvelle liste d’éléments transformés. Regardons un exemple. wizard-names.ts Cette fonction parcourt la liste des assistants, obtient leur nom et le place dans un nouveau tableau. Le résultat de ceci ressemble à ceci. Dans cet exemple, nous utilisions une fonction lambda (ou fonction anonyme), mais nous pouvons également utiliser une fonction nommée. wizardToString.ts Dans cet exemple, nous avons une fonction appelée wizardToString que nous transmettons directement au map. Il retournera alors une nouvelle liste qui ressemble à ceci. Filter Le filter se comporte comme un map dans la mesure où il itère sur la liste, mais au lieu de transformer chaque élément, il transforme la liste entière. Le filter prend une fonction qui renvoie true ou false ou un prédicat. Il renvoie ensuite une nouvelle liste avec des éléments où le prédicat renvoie true. Regardons un exemple. slytherins.ts Dans cet exemple, nous filtrons par-dessus la liste et n’incluons que les sorciers qui se trouvent dans la maison Serpentard. Le résultat serait ceci. En passant, Taylor et Lin sont deux des Serpentards les plus acclamés de notre époque. Comme avec map, nous n’avons pas besoin d’utiliser un lambda , nous pouvons également utiliser une fonction prédéfinie. winners-losers.ts Dans cet exemple, nous faisons deux listes, la liste des Serpentards ayant gagné des points (gagnants) et la liste des Serpentards ayant perdu des points (perdants). Nous pouvons voir ces résultats ci-dessous. Reduce Nous arrivons maintenant à la fonction la plus intéressante, reduce. Reduce itère sur une liste et produit une valeur unique. Regardons un exemple. Supposons que nous voulions obtenir le nombre total de points pour tous les assistants. Nous pouvons utiliser réduire pour faire cela. Que se passe t-il ici? Bien réduire est une fonction qui prend deux arguments, une fonction et une valeur initiale pour l’accumulateur. L’accumulateur est le nom de la chose réduire les rendements. Dans ce cas, nous commençons le compte de points à 0. Maintenant, la fonction prend l’état actuel de l’accumulateur et de l’élément dans la liste qu’il est supposé traiter. Pour le premier assistant, il passera 0 pour l’accumulateur. Cette fonction retourne ensuite accumulator + points. Cela finira par résumer tous les points. Si vous êtes curieux, le résultat est 5487. Maintenant, l’accumulateur peut être n’importe quoi, on peut même utiliser réduire pour produire un objet. Regardons un exemple où nous additionnons les points pour chaque maison. points-per-house.ts Dans ce cas, nous initialisons notre accumulateur ou acc avec {}. Ensuite, pour chaque assistant, nous appelons une fonction qui ajoute le nombre de points qu’il a gagnés pour sa maison. Si vous êtes curieux, ce résultat ressemble à ceci. Regardons un autre exemple, disons que nous voulons le meilleur assistant pour chaque maison. Nous pouvons modifier notre fonction précédente pour utiliser le meilleur assistant pour chaque maison. Si vous êtes curieux, le résultat est le suivant. Juste un peu plus de plaisir maintenant, nous pouvons utiliser Object.values pour transformer cet map. best-names-per-house.ts Maintenant, nous avons de beaux noms pour la meilleure personne dans chaque maison.","timeToRead":5,"frontmatter":{"title":"Map, filter, reduce","subtitle":"Ces trois fonctions sont utiles pour parcourir une liste (ou un tableau) et effectuer une sorte de transformation ou de calcul.","tags":["Javascript","Node.js"],"date":"2019-03-03T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAtElEQVQoz2P4cNufbMRAV83vb4EQyZo/3/P/dNf/28OAr/cDIPoZiLQNqPPeaZ9nl3zP7fW6fNDr872A90RqfnvD/8/TwHmTnfpb7VMTLPasdfv2AGQ5sZr/vw2eM8lx8XTn6b2OM/ocifUz0AagVzcsds3LsFq3wGViu0N6kuWLK35AQQYigwroz5M7PN/e9Duwwf3WcR+Qn4l0NjBsgJ78/jDg4x3/n48CvpAU2sjRixzPABStw4O6JhVqAAAAAElFTkSuQmCC","aspectRatio":1.7777777777777777,"src":"/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png","srcSet":"/static/f10109a249e0ae09b060bb7269f1f4be/4c9af/js.png 930w,\n/static/f10109a249e0ae09b060bb7269f1f4be/9ecf6/js.png 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/js-map-filter-reduce/"}}},{"node":{"excerpt":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent). Des cycles de publication de plusieurs mois ou années sont en train de devenir rares, en particulier parmi ceux qui construisent des produits logiciels purs. Aujourd’hui, en utilisant une approche axée sur les services et sur les microservices, les développeurs peuvent concevoir une base de code modulaire. Cela leur permet d’écrire et de déployer des modifications simultanément sur différentes parties de la base de code. Les avantages commerciaux de cycles de déploiement plus courts sont clairs: Le temps de mise sur le marché est réduit Les clients obtiennent la valeur du produit en moins de temps Les commentaires des clients sont également renvoyés plus rapidement dans l’équipe produit, ce qui permet à l’équipe de parcourir les fonctionnalités et de résoudre les problèmes plus rapidement. Le moral des développeurs augmente Cependant, ce changement crée également de nouveaux défis pour les opérations ou l’équipe de DevOps. Avec des déploiements plus fréquents, il est plus probable que le code déployé puisse affecter négativement la fiabilité du site ou l’expérience client. C’est pourquoi il est important de développer des stratégies de déploiement de code minimisant les risques pour le produit et les clients. Dans cet article, nous allons parler de différentes stratégies de déploiement, de meilleures pratiques et d’outils qui permettront à votre équipe de travailler plus rapidement et de manière plus fiable. Défis des applications modernes Les applications modernes sont souvent distribuées et basées sur le cloud. Ils peuvent évoluer pour répondre à la demande et sont plus résistants aux pannes grâce à des architectures hautement disponibles. Ils peuvent utiliser des services entièrement gérés comme AWS Lambda ou Elastic Container Service (ECS), où la plate-forme assume une partie de la responsabilité opérationnelle. Ces applications ont presque toujours des déploiements fréquents. Par exemple, une application mobile ou une application Web grand public peut subir plusieurs modifications au cours d’un mois. Certains sont même déployés en production plusieurs fois par jour. Ils utilisent souvent des architectures de microservices dans lesquelles plusieurs composants travaillent ensemble pour offrir une fonctionnalité complète. Il peut y avoir différents cycles de publication pour différents composants, mais ils doivent tous fonctionner ensemble de manière transparente. L’augmentation du nombre de pièces mobiles signifie plus de chances que quelque chose se passe mal. Avec plusieurs équipes de développement apportant des modifications dans la base de code, il peut être difficile de déterminer la cause première d’un problème inévitablement. Un autre défi: l’abstraction de la couche infrastructure, qui est maintenant considérée comme du code. Le déploiement d’une nouvelle application peut également nécessiter le déploiement d’un nouveau code d’infrastructure. Stratégies de déploiement populaires Pour relever ces défis, les équipes chargées des applications et de l’infrastructure doivent concevoir et adopter une stratégie de déploiement adaptée à leur cas d’utilisation. Nous en examinerons plusieurs et discuterons des avantages et des inconvénients de différentes stratégies de déploiement afin que vous puissiez choisir celle qui convient à votre organisation. Déploiement “Big Bang” Comme son nom l’indique, les déploiements “big bang” mettent à jour des parties entières ou volumineuses d’une application en un seul coup. Cette stratégie remonte à l’époque où le logiciel était publié sur un support physique et installé par le client. Les déploiements à grande échelle ont obligé l’entreprise à procéder à des développements et à des tests approfondis avant la publication, souvent associés au “modèle en cascade” de grandes versions séquentielles. Les applications modernes présentent l’avantage de se mettre à jour régulièrement et automatiquement, côté client ou côté serveur. Cela rend l’approche big bang plus lent et moins agile pour les équipes modernes. Les caractéristiques du déploiement du Big Bang comprennent: Toutes les pièces majeures conditionnées en un seul déploiement; Remplacer en grande partie ou totalement une version de logiciel existante par une nouvelle; Déploiement entraînant généralement de longs cycles de développement et de test; En supposant un risque minimal d’échec, les retours en arrière peuvent être impossibles ou peu pratiques; Les délais de réalisation sont généralement longs et peuvent nécessiter les efforts de plusieurs équipes; Action requise des clients pour mettre à jour l’installation côté client. Les déploiements à fort rendement ne conviennent pas aux applications modernes, car les risques sont inacceptables pour les applications grand public ou critiques pour les entreprises, où les pannes entraînent des pertes financières énormes. Les reculs sont souvent coûteux, prennent du temps, voire impossibles. L’approche big bang peut convenir à des systèmes hors production (par exemple, recréer un environnement de développement) ou à des solutions prêtes à l’emploi, tel que les applications de bureau. Déploiement progressif (Rolling) Les déploiements progressifs, par paliers ou par étapes sont préférables aux déploiements big bang, car ils minimisent de nombreux risques, y compris les temps d’arrêt pour les utilisateurs, sans restauration simple. Dans un déploiement en continu, la nouvelle version d’une application remplace progressivement l’ancienne. Le déploiement réel se produit sur une période de temps. Pendant ce temps, les nouvelles versions et les anciennes versions coexisteront sans affecter la fonctionnalité ou l’expérience utilisateur. Ce processus facilite la restauration de tout nouveau composant incompatible avec les anciens composants. Le diagramme suivant illustre le modèle de déploiement: l’ancienne version apparaît en bleu et la nouvelle version en vert sur chaque serveur du cluster.  Une mise à niveau d’une suite d’applications est un exemple de déploiement progressif. Si les applications d’origine ont été déployées dans des conteneurs, la mise à niveau peut traiter un conteneur à la fois. Chaque conteneur est modifié pour télécharger la dernière image à partir du site du fournisseur de l’application. S’il existe un problème de compatibilité pour l’une des applications, l’ancienne image peut recréer le conteneur. Dans ce cas, les nouvelles versions et les anciennes versions des applications de la suite coexistent jusqu’à ce que chaque application soit mise à niveau. Déploiement Blue-Green, Red-Black ou A/B C’est un autre processus à sécurité intégrée. Dans cette méthode, deux environnements de production identiques fonctionnent en parallèle. L’un est l’environnement de production en cours d’exécution recevant tout le trafic utilisateur (représenté en bleu). L’autre en est un clone, mais inactif (vert). Les deux utilisent la même base de données et la même configuration d’application:  La nouvelle version de l’application est déployée dans l’environnement vert et testée pour ses fonctionnalités et ses performances. Une fois les résultats des tests réussis, le trafic des applications passent du bleu au vert. Le vert devient alors la nouvelle production.  S’il y a un problème après que le vert devient actif, le trafic peut être redirigé vers le bleu. Dans un déploiement Blue-Green, les deux systèmes utilisent la même couche de persistance ou la même base de données. Il est essentiel de synchroniser les données de l’application, mais une base de données en miroir peut aider à atteindre cet objectif. Vous pouvez utiliser la base de données primaire en bleu pour les opérations d’écriture et la base secondaire en vert pour les opérations de lecture. Lors du passage du bleu au vert, la base de données est basculée du primaire au secondaire. Si le vert a également besoin d’écrire des données pendant le test, les bases de données peuvent être en réplication bidirectionnelle. Une fois que le vert devient actif, vous pouvez fermer ou recycler les anciennes instances bleues. Vous pouvez déployer une version plus récente sur ces instances et en faire le nouveau vert pour la prochaine version. Les déploiements Blue-Green reposent sur le routage du trafic. Cela peut être fait en mettant à jour les DNS (CNAMES) pour les hôtes. Cependant, des valeurs TTL longues peuvent retarder ces modifications. Vous pouvez également modifier les paramètres du load balancer pour que les modifications prennent effet immédiatement.  Déploiement canari Le déploiement canari ressemble à du Blue-Green, à la différence qu’il est moins enclin à prendre des risques. Au lieu de passer du bleu au vert en une seule étape, vous utilisez une approche progressive. Avec le déploiement canari, vous déployez un nouveau code d’application dans une petite partie de l’infrastructure de production. Une fois que l’application est validée pour la publication, seuls quelques utilisateurs y sont routés. Cela minimise tout impact. En l’absence d’erreur signalée, la nouvelle version peut progressivement être déployée dans le reste de l’infrastructure. L’image ci-dessous illustre le déploiement canari:  Le principal défi du déploiement canari est de trouver un moyen d’acheminer certains utilisateurs vers la nouvelle application. En outre, certaines applications peuvent toujours avoir besoin du même groupe d’utilisateurs pour les tests, tandis que d’autres peuvent nécessiter un groupe différent à chaque fois. Envisagez un moyen d’acheminer de nouveaux utilisateurs en explorant plusieurs techniques: Exposer les utilisateurs internes au déploiement canari avant d’autoriser l’accès des utilisateurs externes; Baser le routage sur la plage IP source; Livrer l’application dans des régions géographiques spécifiques; Utilisation d’une logique d’application pour déverrouiller de nouvelles fonctionnalités pour des utilisateurs et des groupes spécifiques. Cette logique est supprimée lorsque l’application est en ligne pour le reste des utilisateurs. Meilleures pratiques de déploiement Les équipes d’applications modernes peuvent suivre un certain nombre de meilleures pratiques pour minimiser les risques de déploiement: Utilisez une liste de déploiement. Par exemple, un élément de la liste peut consister à “sauvegarder toutes les bases de données uniquement après que les services d’application ont été arrêtés” afin d’éviter toute corruption des données. Adopter l’intégration continue (CI). CI s’assure que le code archivé dans la branche de fonctionnalité d’un référentiel de code ne fusionne avec sa branche principale qu’après une série de vérifications de dépendance, de tests d’unité et d’intégration et une génération réussit. S’il y a des erreurs le long du chemin, la construction échoue et l’équipe de l’application en est informée. L’utilisation de CI signifie donc que chaque modification apportée à l’application est testée avant son déploiement. Les exemples d’outils de CI incluent: CircleCI, Jenkins. Adoptez la livraison continue (CD). Avec le CD, l’artefact de code construit par le CI est packagé et toujours prêt à être déployé dans un ou plusieurs environnements. Utilisez des environnements d’exploitation standard pour assurer la cohérence de l’environnement. Vous pouvez utiliser des outils tels que Vagrant et Packer pour les postes de travail et les serveurs de développement. Utilisez les outils d’automatisation de build pour automatiser les générations d’environnement. Avec ces outils, il est souvent simple de cliquer sur un bouton pour détruire une pile d’infrastructure complète et la reconstruire à partir de zéro. CloudFormation est un exemple de tels outils. Utilisez des outils de gestion de configuration tels que Puppet, Chef ou Ansible sur les serveurs cibles pour appliquer automatiquement les paramètres du système d’exploitation, appliquer des correctifs ou installer des logiciels. Utilisez des canaux de communication tels que Slack pour les notifications automatisées des générations infructueuses et des échecs d’application. Créez un processus pour alerter l’équipe responsable des déploiements qui échouent. Dans l’idéal, vous les rencontrez dans l’environnement CI, mais si les modifications sont appliquées, vous aurez besoin d’un moyen d’avertir l’équipe responsable. Activez les restaurations automatisées pour les déploiements qui échouent aux vérifications de l’état, que ce soit en raison de problèmes de disponibilité ou de taux d’erreur. Surveillance post-déploiement Même après que vous ayez adopté toutes ces pratiques, il est possible que des choses se passent mal. Pour cette raison, la surveillance des problèmes survenant immédiatement après un déploiement est aussi importante que la planification et l’exécution d’un déploiement parfait. Un outil de surveillance des performances des applications peut aider votre équipe à surveiller les métriques de performances critiques, y compris les temps de réponse du serveur après les déploiements. Les modifications apportées à l’architecture des applications ou du système peuvent affecter considérablement les performances des applications. Une solution de surveillance des erreurs est également essentielle. Il informera rapidement votre équipe des erreurs nouvelles ou réactivées lors d’un déploiement est susceptibles de révéler des bogues importants nécessitant une intervention immédiate. Sans outil de surveillance des erreurs, les bogues n’auraient peut-être jamais été découverts. Alors que quelques utilisateurs rencontrant les bugs prendront le temps de les signaler, la plupart des autres ne le font pas. L’expérience client négative peut entraîner des problèmes de satisfaction au fil du temps ou, pire, empêcher les transactions commerciales d’avoir lieu. Un outil de surveillance des erreurs crée également une visibilité partagée de tous les problèmes de post-déploiement parmi les équipes Opérateur / DevOps et les développeurs. Cette compréhension partagée permet aux équipes d’être plus collaboratives et réactives.","timeToRead":9,"frontmatter":{"title":"Introduction aux stratégies de déploiement","subtitle":"De nos jours, le changement le plus important dans le développement logiciel est la fréquence des déploiements. Les équipes de produits déploient les versions en production plus tôt (et plus souvent).","tags":["CICD","Deploy","DevOps"],"date":"2019-02-02T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFak9W5wHg//8QAGxAAAQQDAAAAAAAAAAAAAAAAAQACAxESISL/2gAIAQEAAQUCKy6E1BuzHEyqC//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAQACAwEAAAAAAAAAAAAAAAABMRESIYH/2gAIAQEABj8C8Qi3WdVP/8QAGRAAAgMBAAAAAAAAAAAAAAAAAAERITGR/9oACAEBAAE/IaNiOjRsui2rXBjEiRSTh//aAAwDAQACAAMAAAAQpN//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPxBX/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQIBAT8Qp//EABwQAQACAgMBAAAAAAAAAAAAAAEAESFRMUFhwf/aAAgBAQABPxDieh8j0AS9GPIJjMXQwMpryCyIpy61KNPs2wWhNAn/2Q==","aspectRatio":1.7777777777777777,"src":"/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg","srcSet":"/static/b9929b1a4915b5e4d241ad5af23c0901/f8f18/servers.jpg 930w,\n/static/b9929b1a4915b5e4d241ad5af23c0901/989b1/servers.jpg 1600w","sizes":"(max-width: 1600px) 100vw, 1600px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/deploy/"}}},{"node":{"excerpt":"Dans cet article, vous apprendrez à créer et à gérer facilement des git hooks pour vos projets Node/NPM à l’aide de Husky. Git Hooks En termes simples, les git hooks sont des scripts personnalisés, qui peuvent être exécutés automatiquement lorsque des événements spécifiques se produisent. Des crochets (hooks) côté client sont déclenchés pour des actions telles que le commit ou le merge. Les crochets côté serveur s’exécutent dans des situations telles que la réception de données (git push) du client. Les hooks peuvent exécuter n’importe quelle logique personnalisée et, plus important encore, rejeter l’action effectuée si quelque chose ne va pas. Par exemple, vous pouvez annuler la validation du commit si son message ne contient pas l’ID de problème du suivi des problèmes. Vous pouvez également le refuser si l’analyse de code statique échoue. Cela peut être très utile si vous voulez vous assurer que votre base de code reste propre ou si vous souhaitez appliquer certaines règles de qualité. Mais comment pouvez-vous réellement installer et gérer ces hooks? Chaque fois que vous clonez un repo git, toutes les données git de votre projet sont stockées dans un répertoire .git dans votre dossier. Il contient plusieurs fichiers et sous-répertoires, l’un d’eux étant appelé \u001dhooks\u001d. À l’intérieur, vous trouverez un tas de fichiers.  Chacun d’entre eux est un script, qui est exécuté lorsqu’un événement spécifique se produit. Le nom de l’événement correspond au nom du fichier. Par exemple, la \u001dpre-commit\u001d est exécutée avant de commiter vos modifications. Comme vous pouvez le constater, tous les fichiers ont une extension \u001d.sample\u001d. Git ignore ces fichiers à moins que vous ne les renommiez. Vous devez supprimer l’extension \u001d.sample\u001d pour activer ces hooks. Dans les exemples de fichiers, vous pouvez trouver une description et un exemple d’implémentation que vous pouvez utiliser comme point de départ pour implémenter vos propres hooks. À quoi ça sert? Maintenant, regardons quelques exemples spécifiques de ce qui peut être réalisé avec les git hooks. Comme ce ne sont que des scripts, vous pouvez faire à peu près n’importe quoi. Habituellement, cela signifie effectuer divers contrôles de qualité. Vous pouvez vous assurer que l’utilisateur a son nom et son email renseignés. Vous pouvez vérifier que le message de validation est correctement formaté. Vous pouvez essayer de créer votre application et rejeter le commit si la construction échoue. Vous pouvez exécuter des tests pour vous assurer qu’ils passent avant le commit. L’utilisation typique est également l’analyse de code statique ou le formatage. Cela signifie qu’il faut vérifier dans votre code les problèmes courants, les mauvaises pratiques, les conventions de dénomination, etc. Il peut également être utile d’exécuter un outil tel que Prettier pour s’assurer que le code est bien formaté avant le commit. Cela évite de nombreux maux de tête lors de la révision du code. Vous pouvez même vérifier les failles de sécurité de votre code avec un outil tel que Snyk. Distribution aux membres de l’équipe Avec les hooks côté serveur, la distribution est facile. Vous n’avez généralement qu’un seul référentiel de serveur principal. Cela signifie que tous les membres de l’équipe mettent généralement leurs modifications au même endroit. Vous y installez vos hooks et vous avez terminé. Avec les hooks côté client, cela devient plus compliqué. Lorsque vous clonez un référentiel, les hooks ne sont pas transférés au client. Cela signifie qu’un référentiel fraîchement cloné n’a aucun hook, peu importe le type de hook que vous avez sur le serveur. Si vous voulez que les membres de votre équipe aient un ensemble unifié de git hooks, vous devez les distribuer d’une manière ou d’une autre et vous assurer qu’ils sont inclus dans leur sous-répertoire git hooks. La solution la plus élémentaire consiste à créer un emplacement partagé, où vous stockez vos hooks, puis demandez à vos développeurs de les télécharger et de les mettre dans leur répertoire de hooks. Bien sûr, vous ne pouvez pas être sûr qu’ils le feront réellement. Le problème est qu’ils ont besoin de savoir qu’ils devraient le faire et comment. Et même s’ils le font, ils peuvent être paresseux ou simplement ignorer votre politique. Vous pouvez améliorer un peu cette solution de base en ayant des hooks dans le référentiel de votre projet et en laissant simplement à vos développeurs exécuter un script personnalisé comme celui-ci, qui les copie ensuite dans leur répertoire de hooks. Sinon, git propose une option pour changer la destination du répertoire des hooks en un emplacement personnalisé: Ces solutions facilitent la distribution, mais ne résolvent pas les problèmes essentiels. Husky Installation Vous pouvez installer Husky simplement en exécutant: Alternative, avec yarn: Ajout de hooks Ajouter des hooks avec Husky est facile. Vous devez juste éditer votre package.json. Vous définissez quels scripts doivent être exécutés sur quel événement git. Pour Husky 1.0.0+, utilisez: Si vous utilisez une version de Husky antérieure à 1.0.0 (disponible dans la version candidate du 06/2018), la syntaxe est légèrement différente. Vous ajoutez vos hooks Husky directement dans la section scripts. L’exemple ci-dessus exécute tous vos tests avant de commiter et avant de pousser et si vos tests échouent, l’action git n’est pas exécutée. Bien sûr, vous pouvez exécuter tout autre script nom plutôt que le nom test. Problème de correction automatique avant validation Il est utile de casser le build si quelque chose ne va pas, mais il est encore plus utile de corriger automatiquement les problèmes avant de s’engager. Par exemple, vous pouvez personnaliser votre code en utilisant Prettier avant de faire un commit ou vous pouvez corriger automatiquement les problèmes de formatage, qui peuvent être résolus automatiquement. C’est beaucoup plus facile de cette façon. Heureusement, il existe un outil pour cela. Cela s’appelle lint-staged. Vous pouvez l’installer par: Maintenant, sur votre action de pre-commit, vous exécutez directement \u001dlint-staged\u001d au lieu de votre linter. Dans lint-staged, vous définissez ce qui doit être exécuté: Lorsque vous essayez de commiter maintenant, \u001dlint-staged\u001d peut modifier vos fichiers avant l’exécution de la validation. Cependant, ce qui est vraiment bien, c’est que vous ne filtrez que les fichiers en attente de validation, et non l’ensemble de votre projet. Cela signifie que l’ensemble du processus est beaucoup plus rapide. Bug JetBrains IDE La bonne nouvelle est que les git hooks configurés de cette manière sont exécutés non seulement lorsque vous utilisez git à partir de votre terminal, mais également à partir d’un IDE. La mauvaise nouvelle est que les IDE JetBrains (IDEA, Webstorm, …) ont actuellement un méchant bogue (voir IDEA-135454) et ne fonctionnent pas bien avec cette configuration. Le problème n’est pas résolu avant plusieurs années, mais heureusement, il existe une solution de contournement. Vous devez juste ajouter ce hook post-commit: Bien entendu, il ne s’agit que d’une solution de contournement jusqu’à ce que le problème soit résolu. Le suivi des problèmes de JetBrains contient une fonctionnalité de vote, alors assurez-vous de voter pour que ce problème soit résolu s’il vous pose problème. Intégration continue Une chose à noter est que Husky installe les hooks uniquement lorsqu’il ne s’exécute pas sur un serveur d’intégration continue. Husky peut détecter qu’il est en cours d’exécution dans le cadre d’un travail CI et n’installe aucun hook. En ignorant Les hooks côté client peuvent être utiles, mais vous ne pouvez pas trop compter sur eux. Ils ne sont que le premier niveau de défense. Vous ne pouvez pas être sûr à 100% qu’ils soient exécutés. Ils peuvent être ignorés à la demande en ajoutant une option de ligne de commande: Pour rendre les choses encore plus faciles, les hooks peuvent être désactivés à l’aide de certaines variables environnementales. Pour cette raison, il est toujours utile d’appliquer la même fonctionnalité sur le serveur. Performance Bien que les hooks cotés client tels que le pre-commit puissent s’avérer très utiles, vous devez garder à l’esprit qu’ils prennent un certain temps à s’exécuter. Les commits, qui sont généralement très rapides, car ils ne se produisent que sur le client, peuvent prendre soudainement très longtemps. Vous serez peut-être tenté d’exécuter tous les tests, l’analyse de code statique, les vérifications préalables, etc., avant chaque validation. Lorsqu’un commit prend des années, vos développeurs ne seront pas heureux et seront tentés d’ignorer les hooks lors de l’exécution de leurs commandes git. Vous devez donc trouver le bon équilibre entre ce qui doit être effectué sur le client et ce qui peut être un point d’accès côté serveur. Conclusion Husky est un outil utile qui permet de créer et de gérer facilement des hooks git sur le client. Vous n’avez plus besoin de distribuer vos hooks manuellement. Comme pour tout, gardez le nombre de hooks côté client avec modération afin d’éviter les temps d’exécution trop longs.","timeToRead":7,"frontmatter":{"title":"Git hooks avec Husky","subtitle":"En termes simples, les git hooks sont des scripts personnalisés, qui peuvent être exécutés automatiquement lorsque des événements spécifiques se produisent.","tags":["Git","Node.js","Javascript"],"date":"2018-11-25T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAQBAgMF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAeZvaUrjYf/EABkQAQADAQEAAAAAAAAAAAAAAAEAAiEREv/aAAgBAQABBQIrr2z4jDGf/8QAFhEBAQEAAAAAAAAAAAAAAAAAABIB/9oACAEDAQE/AZxL/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8Bqv/EABkQAAIDAQAAAAAAAAAAAAAAAAAyASAxkf/aAAgBAQAGPwJTBZ7T/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAERIUGh/9oACAEBAAE/IWy6ZuV4UVsOR8Kf/9oADAMBAAIAAwAAABDb7//EABcRAAMBAAAAAAAAAAAAAAAAAAABETH/2gAIAQMBAT8QbtIP/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/ELTT/8QAGxABAQEBAAMBAAAAAAAAAAAAAREAITFBUXH/2gAIAQEAAT8QEkQjOdxTGyQQ/nnNus/UZr6lPWsqdhrblLv/2Q==","aspectRatio":1.6103059581320451,"src":"/static/ac7733f363ac6c799dc4d94c21beb53b/883ab/husky2.jpg","srcSet":"/static/ac7733f363ac6c799dc4d94c21beb53b/f8f18/husky2.jpg 930w,\n/static/ac7733f363ac6c799dc4d94c21beb53b/0e6ff/husky2.jpg 1860w,\n/static/ac7733f363ac6c799dc4d94c21beb53b/883ab/husky2.jpg 2000w","sizes":"(max-width: 2000px) 100vw, 2000px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/git-husky/"}}},{"node":{"excerpt":"Si vous avez un Kindle d’Amazon, vous ne le savez peut-être pas, mais vous possédez une adresse mail ….@kindle.com qui vous permet de vous envoyer des livres et documents directement sur votre liseuse. Pour cela, il faut ajouter votre email via cette page, dans la section “Settings” > “Personal Document Settings” > “Approved Personal Document E-mail List”. Et juste au-dessus, vous devriez voir dans la section “Send-to-Kindle E-Mail Settings”, le ou les emails @kindle.com associés avec chacun de vos appareils Kindle. En effet, en envoyant un fichier depuis votre email référencé vers votre email kindle.com, vous pouvez le transférer vers votre liseuse sans avoir besoin de brancher le moindre câble ou d’installer le moindre soft. Seulement, Amazon autorise l’envoi uniquement des fichiers aux formats suivants : Kindle Format (.MOBI, .AZW) Microsoft Word (.DOC, .DOCX) HTML (.HTML, .HTM) RTF (.RTF) Text (.TXT) JPEG (.JPEG, .JPG) GIF (.GIF) PNG (.PNG) BMP (.BMP) PDF (.PDF) Malheureusement, comme vous pouvez le voir, pas de format epub. Obligé de brancher le Kindle à votre ordinateur et d’utiliser Calibre ? Non, non, non. Il suffit de vous rendre sur le site Send epub to Kindle , puis de renseigner les infos demandées, à savoir votre email référencé, l’email Kindle et de glisser-déposer le livre au format epub de votre choix. Cliquez ensuite sur le bouton vert « Upload & Send » et tadaaaa, au bout de quelques secondes, le livre apparaitra sur votre liseuse Kindle. Elle est pas belle la vie ?","timeToRead":1,"frontmatter":{"title":"Comment envoyer un epub vers une Kindle sans utiliser Calibre ni de cable USB ?","subtitle":"","tags":["Kindle","Ebook"],"date":"2018-11-24T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGa0ziTSqWf/8QAGBABAQEBAQAAAAAAAAAAAAAAAQIDACL/2gAIAQEAAQUCy0ItmMp9HVjK1mHMj3//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABsQAAIDAAMAAAAAAAAAAAAAAAABAhExEBIh/9oACAEBAAY/As8xkujuUlRnFos//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARITFB/9oACAEBAAE/ISa6jh2N3hhGocLS2JPOElOjWf/aAAwDAQACAAMAAAAQ2P8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/EIw//8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EK2//8QAGxABAQACAwEAAAAAAAAAAAAAAREAMSFRYUH/2gAIAQEAAT8QFwmLUI775mAebFhqWTRLvCPHnyYECp4Z6B6G/HGBI1nef//Z","aspectRatio":1.5,"src":"/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg","srcSet":"/static/e96dc41e5ffd5680b39408b75434d7e0/f8f18/kindle.jpg 930w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/0e6ff/kindle.jpg 1860w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg 1920w","sizes":"(max-width: 1920px) 100vw, 1920px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/kindle/"}}},{"node":{"excerpt":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser. Introduction Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif. Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. Docker Swarm et Kubernetes sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur. Création du conteneur J’ai décidé d’utiliser Samba pour l’application de test. Samba est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445. C’est la première fois que je travaille avec Docker, j’ai donc modifié un conteneur Samba standard afin d’inclure le fichier que je voulais servir. Après le tutoriel de Docker, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement: Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec smbclient Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur. Préparer les machines virtuelles J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox. J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler: https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png Ensuite, j’ai ajouté un serveur DHCP pour attribuer des adresses IP à chaque machine virtuelle: Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100. Docker Swarm Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes. Mise en place de Docker Swarm Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le tutoriel officiel, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle: C’est tout: le moteur Docker tourne maintenant en mode Swarm. Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les instructions d’installation: Déploiement de l’application Docker Swarm utilise le format Docker Compose pour spécifier les conteneurs à exécuter et les ports qu’ils exportent. Après le didacticiel Docker Compose, j’ai créé ce manifeste Docker Compose: Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur. Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé: Une fois le conteneur créé, l’application peut être déployée avec la commande docker stack deploy, en spécifiant le nom du service: Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec smbclient: Ajout d’un autre noeud Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm: Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire: Sur le nouveau noeud de travail, le nouveau conteneur est apparu: Test de l’équilibrage de charge (load balancing) Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm. Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec nc: Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe. Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100): Et sur le noeud travailleur (10.133.7.50): Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement. J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère). Kubernetes Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant. Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que minikube, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB. Mise en place de Kubernetes Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel Kubernetes the Hard Way Les développeurs de Kubernetes ont simplifié l’utilisation de kubeadm. Malheureusement, Kubernetes étant si flexible, le tutoriel sur kubeadm ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même. Voici ce que j’ai fini par lancer. J’ai d’abord dû désactiver Swap sur chaque noeud: Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante: L’option --pod-network-cidr attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes. Les options --apiserver-advertise-address et --apiserver-cert-extra-sans ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100. Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions: J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait! Après avoir enfin lu les instructions, je devais faire trois autres choses: Tout d’abord, je devais exécuter les commandes données par kubeadm pour configurer un fichier de configuration. Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le tutoriel m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud: Enfin, je devais choisir un réseau pour mon cluster. Installation du réseau Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge. Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et cet article comparatif suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du didacticiel kubeadm pour appliquer la configuration de WeaveNet: Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune. Pour installer MetalLB, j’ai suivi son didacticiel de mise en route et j’ai tout d’abord déployé MetalLB: Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant ce fichier de configuration: Déploiement de l’application Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité. Après avoir lu le tutoriel de Kubernetes, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement. https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447 Ce service demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge. https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8 Cet objet Deployment indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer. Notez le replicas: 1 - c’est le nombre d’instances du conteneur que je veux exécuter. Je peux déployer ce service sur Kubernetes en utilisant kubectl apply: Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner: Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB: Ajout d’un autre noeud Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par kubeadm sur le nouvel ordinateur: Bizarreries de ma configuration J’ai dû faire deux changements en raison de la configuration de VirtualBox: Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon ce problème, je devais éditer Et changer une ligne en avant de redémarrer Kubernetes: L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de ssh: Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine. En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder. “Scaling” de l’application Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application: Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de CreatingContainer et a commencé à s’exécuter: Test de l’équilibrage de charge J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant. Master: Worker: Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi. Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours. Comparaison et conclusion Fonctionnalités communes aux deux: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale. Les atouts de Docker Swarm: une configuration simple, aucune configuration requise, une intégration étroite avec Docker. Les points forts de Kubernetes: composants souples, nombreuses ressources disponibles et add-ons. Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité. J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais tout désactiver en même temps. Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un tableau de bord sophistiqué. Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser minikube, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise. Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile. En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes. Ce que j’ai appris Comment travailler avec les conteneurs Docker Comment configurer un cluster Docker Swarm à deux noeuds Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP Comment déployer une application sur Docker Swarm et Kubernetes Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98 Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent","timeToRead":16,"frontmatter":{"title":"Docker Swarm vs Kubernetes","subtitle":"J'ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser.","tags":["Kubernetes","Docker","DevOps"],"date":"2018-11-01T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB6OtBGFa2/8QAGhABAAIDAQAAAAAAAAAAAAAAAQACAxESE//aAAgBAQABBQJyW7vkSe1iaNoIBU//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Bia//xAAZEQABBQAAAAAAAAAAAAAAAAAAAQIRElH/2gAIAQIBAT8BlCzcP//EABoQAAMBAAMAAAAAAAAAAAAAAAABESESMUH/2gAIAQEABj8CU6MU30k5FhGRKH//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhQTFRYf/aAAgBAQABPyHjwtkhQI2lwNVg2E1C5WkTplYQ8n//2gAMAwEAAgADAAAAEAjv/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhYf/aAAgBAwEBPxAQO3S//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAgEBPxDsOTD/xAAaEAEBAQADAQAAAAAAAAAAAAABEQAhMUFx/9oACAEBAAE/EEhJlEVOtxTFYSS051ExT5vZ7lQoemZMl9hTc8RsE3//2Q==","aspectRatio":1.7772511848341233,"src":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg","srcSet":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg 750w","sizes":"(max-width: 750px) 100vw, 750px"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/dockerSwarm-vs-kubernetes/"}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"authorId":"ludo","limit":9,"skip":18,"numPages":4,"currentPage":3}}}
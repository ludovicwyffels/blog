{"componentChunkName":"component---src-templates-post-tsx","path":"/dockerSwarm-vs-kubernetes/","webpackCompilationHash":"28b74a5dec589e4586df","result":{"data":{"logo":{"childImageSharp":{"fixed":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsSAAALEgHS3X78AAABFUlEQVQY023PvUrDUBTA8ZuCtoMoOLQ4dWhN14x2kEKXFkGoohQ/QBexOHRycAidKkgHHRq69APRoTjFJYODIA6+gSBYfYMmfYLE/5VT6NALP845NyeHc9UoCHZRQvonCLKKE4ZhAXYURTsYoE9dIy5iCzfUJnEdV+RF4iu6ikFH2EPxazzOcJmg4Rnn5C8ysIEOHqTe5/sT3qXOwYOtRr5/wLAqzpCTDXu4p2ETtyjgGJ/cX+se8jeU9fa4RBt5vWENpjzdkoGPcGg4RQsbOMEFmnBRRwVDHEK/yFXfvr/CoPjvZLKcdByDyzWGeajigzorGy0oOeRJiUtITe/pX1XzDk3buIMltSFR/zTtic3060X+8z8PsEUt8G3LVQAAAABJRU5ErkJggg==","width":400,"height":128,"src":"/static/2b5eaa0de166a8b5faebad4955c2200c/647de/ghost-logo.png","srcSet":"/static/2b5eaa0de166a8b5faebad4955c2200c/647de/ghost-logo.png 1x"}}},"markdownRemark":{"html":"<p>J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser.</p>\n<h2>Introduction</h2>\n<p>Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif.</p>\n<p>Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. <a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Docker Swarm</a> et <a href=\"https://kubernetes.io/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kubernetes</a> sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur.</p>\n<h2>Création du conteneur</h2>\n<p>J’ai décidé d’utiliser Samba pour l’application de test. <a href=\"https://fr.wikipedia.org/wiki/Samba_(informatique)\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Samba</a> est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445.</p>\n<p>C’est la première fois que je travaille avec Docker, j’ai donc modifié un <a href=\"https://github.com/crops/samba\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">conteneur Samba standard</a> afin d’inclure le fichier que je voulais servir.</p>\n<p>Après le <a href=\"https://docs.docker.com/get-started/part2/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tutoriel de Docker</a>, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">docker build -t sambaonly-v1 .\ndocker run --init -p 445:445 -i sambaonly-v1</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec <a href=\"https://www.samba.org/samba/docs/current/man-html/smbclient.1.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">smbclient</a></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">~$ smbclient \\\\\\\\localhost\\\\workdir -U %\nWARNING: The &quot;syslog&quot; option is deprecated\nTry &quot;help&quot; to get a list of possible commands.\nsmb: \\&gt; ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\&gt;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur.</p>\n<h2>Préparer les machines virtuelles</h2>\n<p>J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox.</p>\n<p>J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler:</p>\n<p><a href=\"https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png</a></p>\n<p>Ensuite, j’ai <a href=\"https://www.virtualbox.org/manual/ch08.html#vboxmanage-dhcpserver\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ajouté un serveur DHCP</a> pour attribuer des adresses IP à chaque machine virtuelle:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">VBoxManage dhcpserver add --netname intnet --ip 10.133.7.99 --netmask 255.255.255.0 --lowerip 10.133.7.100 --upperip 10.133.7.200 --enable</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100.</p>\n<h2>Docker Swarm</h2>\n<p>Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes.</p>\n<h3>Mise en place de Docker Swarm</h3>\n<p>Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tutoriel officiel</a>, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">~$ docker swarm init --advertise-addr 10.133.7.100 \nSwarm initialized: current node (abcdefghijklmnopqrstuvwxy) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\ndocker swarm join --token SWMTKN-1-abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwx-abcdefghijklmnopqrstuvwxy 10.133.7.100:2377\n\nTo add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>C’est tout: le moteur Docker tourne maintenant en mode Swarm.</p>\n<p>Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les <a href=\"https://docs.docker.com/engine/swarm/stack-deploy/#set-up-a-docker-registry\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">instructions d’installation</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">docker service create --name registry --publish published=5000,target=5000 registry:2</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<h2>Déploiement de l’application</h2>\n<p>Docker Swarm utilise le format <a href=\"https://docs.docker.com/compose/overview/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Docker Compose</a> pour spécifier les conteneurs à exécuter et les ports qu’ils exportent.</p>\n<p>Après le <a href=\"https://docs.docker.com/compose/gettingstarted/#step-3-define-services-in-a-compose-file\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">didacticiel Docker Compose</a>, j’ai créé ce manifeste Docker Compose:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">version: &#39;3.7&#39;\nservices:\n  samba:\n    image: 127.0.0.1:5000/samba\n    build: sambaonly\n    init: true\n    stdin_open: true\n    ports:\n      - &quot;445:445&quot;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur.</p>\n<p>Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">docker-compose build\ndocker-compose push</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Une fois le conteneur créé, l’application peut être déployée avec la commande <code>docker stack deploy</code>, en spécifiant le nom du service:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">$ docker stack deploy --compose-file docker-compose.yml samba-swarm\nIgnoring unsupported options: build\nCreating network samba-swarm_default\nCreating service samba-swarm_samba\n~/Documents/docker$ docker stack services samba-swarm\nID           NAME                  MODE       REPLICAS IMAGE PORTS\nyg8x8yfytq5d samba-swarm_samba     replicated 1/1</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec <code>smbclient</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">~$ smbclient \\\\\\\\localhost\\\\workdir -U %\nWARNING: The &quot;syslog&quot; option is deprecated\nTry &quot;help&quot; to get a list of possible commands.\nsmb: \\&gt; ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\&gt;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Ajout d’un autre noeud</h2>\n<p>Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ralph:~# docker swarm join --token SWMTKN-1-abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwx-abcdefghijklmnopqrstuvwxy 10.133.7.100:2377\n\nThis node joined a swarm as a worker.</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n<p>Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">~/Documents/docker$ docker service scale samba-swarm_samba=2\nsamba-swarm_samba scaled to 2 overall progress: 2 out of 2 tasks\n1/2: running [==================================================&gt;]\n2/2: running [==================================================&gt;] verify: Service converged</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Sur le nouveau noeud de travail, le nouveau conteneur est apparu:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ralph:~# docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n7539549283bd 127.0.0.1:5000/samba:latest &quot;/usr/sbin/smbd -FS …&quot; 20 seconds ago Up 18 seconds 445/tcp samba-swarm_samba.1.abcdefghijklmnopqrstuvwxy</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n<h2>Test de l’équilibrage de charge (load balancing)</h2>\n<p>Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm.</p>\n<p>Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec <code>nc</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">print(&quot;#!/bin/bash&quot;)\nfor i in range(1000):\n    print(&quot;nc -v 10.133.7.100 445 &amp;&quot;)\nprint(&quot;wait&quot;)</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe.</p>\n<p>Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100):</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">~$ ps -ef|grep smbd|wc\n506 5567 42504</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Et sur le noeud travailleur (10.133.7.50):</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ralph:~# ps -ef|grep smbd|wc\n506 3545 28862</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement.</p>\n<p>J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère).</p>\n<h1>Kubernetes</h1>\n<p>Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant.</p>\n<p>Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que <a href=\"https://kubernetes.io/docs/setup/minikube/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><code>minikube</code></a>, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB.</p>\n<h2>Mise en place de Kubernetes</h2>\n<p>Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel <a href=\"https://github.com/kelseyhightower/kubernetes-the-hard-way\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kubernetes the Hard Way</a></p>\n<p>Les développeurs de Kubernetes ont simplifié l’utilisation de <code>kubeadm</code>.</p>\n<p>Malheureusement, Kubernetes étant si flexible, le <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tutoriel sur <code>kubeadm</code></a> ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même.</p>\n<p>Voici ce que j’ai fini par lancer.</p>\n<p>J’ai d’abord dû désactiver Swap sur chaque noeud:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">root@dora:~# swapoff -a\nroot@dora:~# systemctl restart kubelet.service</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">sudo kubeadm init --pod-network-cidr=10.134.0.0/16 --apiserver-advertise-address=10.133.7.100 --apiserver-cert-extra-sans=10.0.2.15</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>L’option <code>--pod-network-cidr</code> attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes.</p>\n<p>Les options <code>--apiserver-advertise-address</code> et <code>--apiserver-cert-extra-sans</code> ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100.</p>\n<p>Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Your Kubernetes master has initialized successfully!\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\nYou can now join any number of machines by running the following on each node as root:\n\nkubeadm join 10.133.7.100:6443 --token abcdefghijklmnopqrstuvw --discovery-token-ca-cert-hash sha256:abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkl</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait!</p>\n<p>Après avoir enfin lu les instructions, je devais faire trois autres choses:</p>\n<ul>\n<li>Tout d’abord, je devais exécuter les commandes données par <code>kubeadm</code> pour configurer un fichier de configuration.</li>\n<li>Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#master-isolation\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tutoriel</a> m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">kubectl taint node --all node-role.kubernetes.io/master-</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<ul>\n<li>Enfin, je devais choisir un réseau pour mon cluster.</li>\n</ul>\n<h2>Installation du réseau</h2>\n<p>Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge.</p>\n<p>Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et <a href=\"https://www.objectif-libre.com/en/blog/2018/07/05/k8s-network-solutions-comparison/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">cet article comparatif</a> suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du <a href=\"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">didacticiel kubeadm</a> pour appliquer la configuration de WeaveNet:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#39;\\n&#39;)&quot;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune.</p>\n<p>Pour installer MetalLB, j’ai suivi son <a href=\"https://metallb.universe.tf/tutorial/layer2/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">didacticiel de mise en route</a> et j’ai tout d’abord déployé MetalLB:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant <a href=\"https://gist.github.com/ludovicwyffels/917a399b423868f9415e6c384138e550\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ce fichier de configuration</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">kubectl apply -f metallb-config.yaml</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<h2>Déploiement de l’application</h2>\n<p>Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité.</p>\n<p>Après avoir lu <a href=\"https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">le tutoriel de Kubernetes</a>, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement.</p>\n<p><a href=\"https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447</a></p>\n<p>Ce <a href=\"https://kubernetes.io/docs/concepts/services-networking/#defining-a-service\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">service</a> demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge.</p>\n<p><a href=\"https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8</a></p>\n<p>Cet objet <code>Deployment</code> indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer.</p>\n<p>Notez le <code>replicas: 1</code> - c’est le nombre d’instances du conteneur que je veux exécuter.</p>\n<p>Je peux déployer ce service sur Kubernetes en utilisant <code>kubectl apply</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@dora:~/Documents/docker$ kubectl apply -f kubernetes-samba.yaml\nservice/samba configured\ndeployment.apps/samba configured</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span></span></pre></div>\n<p>Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@dora:~/Documents/docker$ kubectl get pods\nNAME                   READY STATUS  RESTARTS AGE\nsamba-57945b8895-dfzgl 1/1   Running 0        52m\nludo@dora:~/Documents/docker$ kubectl get service samba\nNAME  TYPE         CLUSTER-IP     EXTERNAL-IP  PORT(S)       AGE\nsamba LoadBalancer 10.108.157.165 10.133.7.200 445:30246/TCP 91m</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<p>Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@dora:~$ smbclient \\\\\\\\10.133.7.200\\\\workdir -U %\nWARNING: The &quot;syslog&quot; option is deprecated\nTry &quot;help&quot; to get a list of possible commands.\nsmb: \\&gt; ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\&gt;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Ajout d’un autre noeud</h2>\n<p>Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par <code>kubeadm</code> sur le nouvel ordinateur:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@davy:~$ sudo kubeadm join 10.133.7.100:6443 --token abcdefghijklmnopqrstuvw --discovery-token-ca-cert-hash sha256:abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkl\n\n(snip...)\n\nThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun &#39;kubectl get nodes&#39; on the master to see this node join the cluster.</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Bizarreries de ma configuration</h2>\n<p>J’ai dû faire deux changements en raison de la configuration de VirtualBox:</p>\n<p>Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon <a href=\"https://github.com/kubernetes/kubeadm/issues/203\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ce problème</a>, je devais éditer</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Et changer une ligne en</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --node-ip=10.133.7.101&quot;</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>avant de redémarrer Kubernetes:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">root@davy:~# systemctl daemon-reload\nroot@davy:~# systemctl restart kubelet.service</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de <code>ssh</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@davy:~$ ssh dora.local -L 5000:localhost:5000</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine.</p>\n<p>En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder.</p>\n<h2>“Scaling” de l’application</h2>\n<p>Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">replicas: 2</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de <code>CreatingContainer</code> et a commencé à s’exécuter:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@dora:~/Documents/docker$ kubectl get pods\nNAME                   READY STATUS  RESTARTS AGE\nsamba-57945b8895-dfzgl 1/1   Running 0        62m\nsamba-57945b8895-qhrtl 1/1   Running 0        12m</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Test de l’équilibrage de charge</h2>\n<p>J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant.</p>\n<p>Master:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@dora$ ps -ef|grep smbd|wc\n492 5411 41315</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Worker:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">ludo@davy:~$ ps -ef|grep smbd|wc\n518 5697 43499</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<p>Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi.</p>\n<p>Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours.</p>\n<h1>Comparaison et conclusion</h1>\n<p><strong>Fonctionnalités communes aux deux</strong>: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale.</p>\n<p><strong>Les atouts de Docker Swarm</strong>: une configuration simple, aucune configuration requise, une intégration étroite avec Docker.</p>\n<p><strong>Les points forts de Kubernetes</strong>: composants souples, nombreuses ressources disponibles et add-ons.</p>\n<p>Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité.</p>\n<p>J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais <a href=\"https://docs.docker.com/engine/swarm/ingress/#using-the-routing-mesh\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tout désactiver en même temps</a>.</p>\n<p>Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un <a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">tableau de bord sophistiqué</a>.</p>\n<p>Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser <a href=\"https://kubernetes.io/docs/setup/minikube/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><code>minikube</code></a>, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise.</p>\n<p>Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile.</p>\n<p>En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes.</p>\n<h1>Ce que j’ai appris</h1>\n<ul>\n<li>Comment travailler avec les conteneurs Docker</li>\n<li>Comment configurer un cluster Docker Swarm à deux noeuds</li>\n<li>Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP</li>\n<li>Comment déployer une application sur Docker Swarm et Kubernetes</li>\n<li>Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98</li>\n<li>Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/engine/swarm/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"Docker Swarm"}]},{"type":"text","value":" et "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"Kubernetes"}]},{"type":"text","value":" sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Création du conteneur"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai décidé d’utiliser Samba pour l’application de test. "},{"type":"element","tagName":"a","properties":{"href":"https://fr.wikipedia.org/wiki/Samba_(informatique)","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"Samba"}]},{"type":"text","value":" est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"C’est la première fois que je travaille avec Docker, j’ai donc modifié un "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/crops/samba","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"conteneur Samba standard"}]},{"type":"text","value":" afin d’inclure le fichier que je voulais servir."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après le "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/get-started/part2/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tutoriel de Docker"}]},{"type":"text","value":", j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"docker build -t sambaonly-v1 .\ndocker run --init -p 445:445 -i sambaonly-v1"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec "},{"type":"element","tagName":"a","properties":{"href":"https://www.samba.org/samba/docs/current/man-html/smbclient.1.html","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"smbclient"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"~$ smbclient \\\\\\\\localhost\\\\workdir -U %\nWARNING: The \"syslog\" option is deprecated\nTry \"help\" to get a list of possible commands.\nsmb: \\> ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\>"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Préparer les machines virtuelles"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ensuite, j’ai "},{"type":"element","tagName":"a","properties":{"href":"https://www.virtualbox.org/manual/ch08.html#vboxmanage-dhcpserver","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"ajouté un serveur DHCP"}]},{"type":"text","value":" pour attribuer des adresses IP à chaque machine virtuelle:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"VBoxManage dhcpserver add --netname intnet --ip 10.133.7.99 --netmask 255.255.255.0 --lowerip 10.133.7.100 --upperip 10.133.7.200 --enable"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Docker Swarm"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Mise en place de Docker Swarm"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tutoriel officiel"}]},{"type":"text","value":", j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"~$ docker swarm init --advertise-addr 10.133.7.100 \nSwarm initialized: current node (abcdefghijklmnopqrstuvwxy) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\ndocker swarm join --token SWMTKN-1-abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwx-abcdefghijklmnopqrstuvwxy 10.133.7.100:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions."}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"C’est tout: le moteur Docker tourne maintenant en mode Swarm."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/engine/swarm/stack-deploy/#set-up-a-docker-registry","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"instructions d’installation"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"docker service create --name registry --publish published=5000,target=5000 registry:2"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Déploiement de l’application"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Docker Swarm utilise le format "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/compose/overview/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"Docker Compose"}]},{"type":"text","value":" pour spécifier les conteneurs à exécuter et les ports qu’ils exportent."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après le "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/compose/gettingstarted/#step-3-define-services-in-a-compose-file","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"didacticiel Docker Compose"}]},{"type":"text","value":", j’ai créé ce manifeste Docker Compose:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"version: '3.7'\nservices:\n  samba:\n    image: 127.0.0.1:5000/samba\n    build: sambaonly\n    init: true\n    stdin_open: true\n    ports:\n      - \"445:445\""}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"docker-compose build\ndocker-compose push"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Une fois le conteneur créé, l’application peut être déployée avec la commande "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"docker stack deploy"}]},{"type":"text","value":", en spécifiant le nom du service:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"$ docker stack deploy --compose-file docker-compose.yml samba-swarm\nIgnoring unsupported options: build\nCreating network samba-swarm_default\nCreating service samba-swarm_samba\n~/Documents/docker$ docker stack services samba-swarm\nID           NAME                  MODE       REPLICAS IMAGE PORTS\nyg8x8yfytq5d samba-swarm_samba     replicated 1/1"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"smbclient"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"~$ smbclient \\\\\\\\localhost\\\\workdir -U %\nWARNING: The \"syslog\" option is deprecated\nTry \"help\" to get a list of possible commands.\nsmb: \\> ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\>"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Ajout d’un autre noeud"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ralph:~# docker swarm join --token SWMTKN-1-abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwx-abcdefghijklmnopqrstuvwxy 10.133.7.100:2377\n\nThis node joined a swarm as a worker."}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"~/Documents/docker$ docker service scale samba-swarm_samba=2\nsamba-swarm_samba scaled to 2 overall progress: 2 out of 2 tasks\n1/2: running [==================================================>]\n2/2: running [==================================================>] verify: Service converged"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sur le nouveau noeud de travail, le nouveau conteneur est apparu:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ralph:~# docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n7539549283bd 127.0.0.1:5000/samba:latest \"/usr/sbin/smbd -FS …\" 20 seconds ago Up 18 seconds 445/tcp samba-swarm_samba.1.abcdefghijklmnopqrstuvwxy"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Test de l’équilibrage de charge (load balancing)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"nc"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"print(\"#!/bin/bash\")\nfor i in range(1000):\n    print(\"nc -v 10.133.7.100 445 &\")\nprint(\"wait\")"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100):"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"~$ ps -ef|grep smbd|wc\n506 5567 42504"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Et sur le noeud travailleur (10.133.7.50):"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ralph:~# ps -ef|grep smbd|wc\n506 3545 28862"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère)."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Kubernetes"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/setup/minikube/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"minikube"}]}]},{"type":"text","value":", j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Mise en place de Kubernetes"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/kelseyhightower/kubernetes-the-hard-way","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"Kubernetes the Hard Way"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Les développeurs de Kubernetes ont simplifié l’utilisation de "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"kubeadm"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Malheureusement, Kubernetes étant si flexible, le "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tutoriel sur "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"kubeadm"}]}]},{"type":"text","value":" ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Voici ce que j’ai fini par lancer."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai d’abord dû désactiver Swap sur chaque noeud:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"root@dora:~# swapoff -a\nroot@dora:~# systemctl restart kubelet.service"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"sudo kubeadm init --pod-network-cidr=10.134.0.0/16 --apiserver-advertise-address=10.133.7.100 --apiserver-cert-extra-sans=10.0.2.15"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"L’option "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"--pod-network-cidr"}]},{"type":"text","value":" attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Les options "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"--apiserver-advertise-address"}]},{"type":"text","value":" et "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"--apiserver-cert-extra-sans"}]},{"type":"text","value":" ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Your Kubernetes master has initialized successfully!\nTo start using your cluster, you need to run the following as a regular user:\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\nYou can now join any number of machines by running the following on each node as root:\n\nkubeadm join 10.133.7.100:6443 --token abcdefghijklmnopqrstuvw --discovery-token-ca-cert-hash sha256:abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkl"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait!"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après avoir enfin lu les instructions, je devais faire trois autres choses:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Tout d’abord, je devais exécuter les commandes données par "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"kubeadm"}]},{"type":"text","value":" pour configurer un fichier de configuration."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#master-isolation","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tutoriel"}]},{"type":"text","value":" m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud:"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"kubectl taint node --all node-role.kubernetes.io/master-"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Enfin, je devais choisir un réseau pour mon cluster."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Installation du réseau"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et "},{"type":"element","tagName":"a","properties":{"href":"https://www.objectif-libre.com/en/blog/2018/07/05/k8s-network-solutions-comparison/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"cet article comparatif"}]},{"type":"text","value":" suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"didacticiel kubeadm"}]},{"type":"text","value":" pour appliquer la configuration de WeaveNet:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\""}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pour installer MetalLB, j’ai suivi son "},{"type":"element","tagName":"a","properties":{"href":"https://metallb.universe.tf/tutorial/layer2/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"didacticiel de mise en route"}]},{"type":"text","value":" et j’ai tout d’abord déployé MetalLB:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.7.3/manifests/metallb.yaml"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant "},{"type":"element","tagName":"a","properties":{"href":"https://gist.github.com/ludovicwyffels/917a399b423868f9415e6c384138e550","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"ce fichier de configuration"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"kubectl apply -f metallb-config.yaml"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Déploiement de l’application"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après avoir lu "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"le tutoriel de Kubernetes"}]},{"type":"text","value":", j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ce "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/concepts/services-networking/#defining-a-service","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"service"}]},{"type":"text","value":" demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Cet objet "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"Deployment"}]},{"type":"text","value":" indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Notez le "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"replicas: 1"}]},{"type":"text","value":" - c’est le nombre d’instances du conteneur que je veux exécuter."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Je peux déployer ce service sur Kubernetes en utilisant "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"kubectl apply"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@dora:~/Documents/docker$ kubectl apply -f kubernetes-samba.yaml\nservice/samba configured\ndeployment.apps/samba configured"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@dora:~/Documents/docker$ kubectl get pods\nNAME                   READY STATUS  RESTARTS AGE\nsamba-57945b8895-dfzgl 1/1   Running 0        52m\nludo@dora:~/Documents/docker$ kubectl get service samba\nNAME  TYPE         CLUSTER-IP     EXTERNAL-IP  PORT(S)       AGE\nsamba LoadBalancer 10.108.157.165 10.133.7.200 445:30246/TCP 91m"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@dora:~$ smbclient \\\\\\\\10.133.7.200\\\\workdir -U %\nWARNING: The \"syslog\" option is deprecated\nTry \"help\" to get a list of possible commands.\nsmb: \\> ls\n.                          D        0  Fri Oct  5 12:14:43 2018\n..                         D        0  Sun Oct  7 22:09:49 2018\nhello.txt                  N       13  Fri Oct  5 11:17:34 2018\n\n102685624 blocks of size 1024. 72252576 blocks available\nsmb: \\>"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Ajout d’un autre noeud"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"kubeadm"}]},{"type":"text","value":" sur le nouvel ordinateur:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@davy:~$ sudo kubeadm join 10.133.7.100:6443 --token abcdefghijklmnopqrstuvw --discovery-token-ca-cert-hash sha256:abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijkl\n\n(snip...)\n\nThis node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the master to see this node join the cluster."}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Bizarreries de ma configuration"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai dû faire deux changements en raison de la configuration de VirtualBox:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/kubernetes/kubeadm/issues/203","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"ce problème"}]},{"type":"text","value":", je devais éditer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"/etc/systemd/system/kubelet.service.d/10-kubeadm.conf"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Et changer une ligne en"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --node-ip=10.133.7.101\""}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"avant de redémarrer Kubernetes:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"root@davy:~# systemctl daemon-reload\nroot@davy:~# systemctl restart kubelet.service"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"ssh"}]},{"type":"text","value":":"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@davy:~$ ssh dora.local -L 5000:localhost:5000"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"“Scaling” de l’application"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"replicas: 2"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"CreatingContainer"}]},{"type":"text","value":" et a commencé à s’exécuter:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@dora:~/Documents/docker$ kubectl get pods\nNAME                   READY STATUS  RESTARTS AGE\nsamba-57945b8895-dfzgl 1/1   Running 0        62m\nsamba-57945b8895-qhrtl 1/1   Running 0        12m"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Test de l’équilibrage de charge"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Master:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@dora$ ps -ef|grep smbd|wc\n492 5411 41315"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Worker:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"style":"counter-reset: linenumber NaN","className":["language-text","line-numbers"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"ludo@davy:~$ ps -ef|grep smbd|wc\n518 5697 43499"}]},{"type":"element","tagName":"span","properties":{"ariaHidden":"true","className":["line-numbers-rows"],"style":"white-space: normal; width: auto; left: 0;"},"children":[{"type":"element","tagName":"span","properties":{},"children":[]},{"type":"element","tagName":"span","properties":{},"children":[]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Comparaison et conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Fonctionnalités communes aux deux"}]},{"type":"text","value":": les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Les atouts de Docker Swarm"}]},{"type":"text","value":": une configuration simple, aucune configuration requise, une intégration étroite avec Docker."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Les points forts de Kubernetes"}]},{"type":"text","value":": composants souples, nombreuses ressources disponibles et add-ons."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais "},{"type":"element","tagName":"a","properties":{"href":"https://docs.docker.com/engine/swarm/ingress/#using-the-routing-mesh","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tout désactiver en même temps"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"text","value":"tableau de bord sophistiqué"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser "},{"type":"element","tagName":"a","properties":{"href":"https://kubernetes.io/docs/setup/minikube/","target":"_blank","rel":["nofollow","noopener","noreferrer"]},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"minikube"}]}]},{"type":"text","value":", qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Ce que j’ai appris"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Comment travailler avec les conteneurs Docker"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Comment configurer un cluster Docker Swarm à deux noeuds"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Comment déployer une application sur Docker Swarm et Kubernetes"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser. Introduction Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif. Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. Docker Swarm et Kubernetes sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur. Création du conteneur J’ai décidé d’utiliser Samba pour l’application de test. Samba est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445. C’est la première fois que je travaille avec Docker, j’ai donc modifié un conteneur Samba standard afin d’inclure le fichier que je voulais servir. Après le tutoriel de Docker, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement: Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec smbclient Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur. Préparer les machines virtuelles J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox. J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler: https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png Ensuite, j’ai ajouté un serveur DHCP pour attribuer des adresses IP à chaque machine virtuelle: Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100. Docker Swarm Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes. Mise en place de Docker Swarm Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le tutoriel officiel, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle: C’est tout: le moteur Docker tourne maintenant en mode Swarm. Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les instructions d’installation: Déploiement de l’application Docker Swarm utilise le format Docker Compose pour spécifier les conteneurs à exécuter et les ports qu’ils exportent. Après le didacticiel Docker Compose, j’ai créé ce manifeste Docker Compose: Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur. Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé: Une fois le conteneur créé, l’application peut être déployée avec la commande docker stack deploy, en spécifiant le nom du service: Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec smbclient: Ajout d’un autre noeud Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm: Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire: Sur le nouveau noeud de travail, le nouveau conteneur est apparu: Test de l’équilibrage de charge (load balancing) Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm. Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec nc: Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe. Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100): Et sur le noeud travailleur (10.133.7.50): Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement. J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère). Kubernetes Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant. Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que minikube, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB. Mise en place de Kubernetes Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel Kubernetes the Hard Way Les développeurs de Kubernetes ont simplifié l’utilisation de kubeadm. Malheureusement, Kubernetes étant si flexible, le tutoriel sur kubeadm ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même. Voici ce que j’ai fini par lancer. J’ai d’abord dû désactiver Swap sur chaque noeud: Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante: L’option --pod-network-cidr attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes. Les options --apiserver-advertise-address et --apiserver-cert-extra-sans ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100. Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions: J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait! Après avoir enfin lu les instructions, je devais faire trois autres choses: Tout d’abord, je devais exécuter les commandes données par kubeadm pour configurer un fichier de configuration. Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le tutoriel m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud: Enfin, je devais choisir un réseau pour mon cluster. Installation du réseau Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge. Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et cet article comparatif suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du didacticiel kubeadm pour appliquer la configuration de WeaveNet: Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune. Pour installer MetalLB, j’ai suivi son didacticiel de mise en route et j’ai tout d’abord déployé MetalLB: Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant ce fichier de configuration: Déploiement de l’application Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité. Après avoir lu le tutoriel de Kubernetes, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement. https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447 Ce service demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge. https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8 Cet objet Deployment indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer. Notez le replicas: 1 - c’est le nombre d’instances du conteneur que je veux exécuter. Je peux déployer ce service sur Kubernetes en utilisant kubectl apply: Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner: Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB: Ajout d’un autre noeud Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par kubeadm sur le nouvel ordinateur: Bizarreries de ma configuration J’ai dû faire deux changements en raison de la configuration de VirtualBox: Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon ce problème, je devais éditer Et changer une ligne en avant de redémarrer Kubernetes: L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de ssh: Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine. En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder. “Scaling” de l’application Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application: Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de CreatingContainer et a commencé à s’exécuter: Test de l’équilibrage de charge J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant. Master: Worker: Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi. Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours. Comparaison et conclusion Fonctionnalités communes aux deux: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale. Les atouts de Docker Swarm: une configuration simple, aucune configuration requise, une intégration étroite avec Docker. Les points forts de Kubernetes: composants souples, nombreuses ressources disponibles et add-ons. Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité. J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais tout désactiver en même temps. Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un tableau de bord sophistiqué. Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser minikube, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise. Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile. En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes. Ce que j’ai appris Comment travailler avec les conteneurs Docker Comment configurer un cluster Docker Swarm à deux noeuds Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP Comment déployer une application sur Docker Swarm et Kubernetes Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98 Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent","timeToRead":16,"frontmatter":{"title":"Docker Swarm vs Kubernetes","subtitle":"J'ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser.","userDate":"1 November 2018","date":"2018-11-01T08:00:00.000Z","tags":["Kubernetes","Docker","DevOps"],"image":{"childImageSharp":{"fluid":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAAB6OtBGFa2/8QAGhABAAIDAQAAAAAAAAAAAAAAAQACAxESE//aAAgBAQABBQJyW7vkSe1iaNoIBU//xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAIAQMBAT8Bia//xAAZEQABBQAAAAAAAAAAAAAAAAAAAQIRElH/2gAIAQIBAT8BlCzcP//EABoQAAMBAAMAAAAAAAAAAAAAAAABESESMUH/2gAIAQEABj8CU6MU30k5FhGRKH//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhQTFRYf/aAAgBAQABPyHjwtkhQI2lwNVg2E1C5WkTplYQ8n//2gAMAwEAAgADAAAAEAjv/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAhYf/aAAgBAwEBPxAQO3S//8QAFxEBAQEBAAAAAAAAAAAAAAAAAQARIf/aAAgBAgEBPxDsOTD/xAAaEAEBAQADAQAAAAAAAAAAAAABEQAhMUFx/9oACAEBAAE/EEhJlEVOtxTFYSS051ExT5vZ7lQoemZMl9hTc8RsE3//2Q==","aspectRatio":1.7772511848341233,"src":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg","srcSet":"/static/6dc1a1b98e66073dbd7e7471a7fff24c/9f583/dockerswarm-vs-kubernetes.jpg 750w","sizes":"(max-width: 750px) 100vw, 750px"}}},"author":{"id":"ludo","name":"Wyffels Ludovic","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"__typename":"ImageSharp","fixed":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsSAAALEgHS3X78AAABMUlEQVQ4y2MwsXf5T03MMGogGBvbOYNpUwdXOIbJwdgwNQQNhCk0snX6r2thC8Y6QGxg7QCWA/FBcrgMxepCkAZrV6//EYmpYBydkvHfMzjiv5mj2//whJT/Nm7ecEMJGgjykp6V/f+AqLj/O/Ye+L8diA8dO/m/pbv/v72n3/+tu/f9D45N/K9naYcSFESFoRXQlSCX2rh7g/kgr1u5epIWhiAMCi/3wLD/fVNn/u+fNgtMT5g+6//kmXOB/Jn/PYLCwWrINnDijDlAPBvKJ8NAZC+DIgCkGYRBbJAYSV5GjpTte/b/PwiMkLj0nP8xqVn/Dxw9AY6kwOh40iIFlmzCgUkmKjnjvx0wdu08fMFskJi1mxfxyQY9YetAEzJyQic5YaNnPeTgIDnrjZaHJGMACtTMXoVAJ6sAAAAASUVORK5CYII=","width":400,"height":400,"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png","srcSet":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png 1x"}}]}}}},"relatedPosts":{"totalCount":3,"edges":[{"node":{"id":"b7646ada-6f35-5115-83e5-3a326c8cd1ea","timeToRead":16,"excerpt":"J’ai installé Docker Swarm et Kubernetes sur deux machines virtuelles. J’ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser. Introduction Cela fait des années que je veux essayer des conteneurs: la configuration manuelle de serveurs prend du temps, n’est pas reproductible et risque d’introduire des différences entre mon environnement de test local et la production. Les containers offrent une solution à tous ces problèmes et facilite beaucoup l’exécution d’instances supplémentaires d’une application. Cela peut rendre un service plus évolutif. Pour exécuter un service évolutif, vous avez besoin d’un moteur Container Orchestration qui répartit la charge en exécutant des conteneurs sur plusieurs ordinateurs et en envoyant des demandes à chaque instance de l’application. Docker Swarm et Kubernetes sont deux moteurs d’orchestration populaires. J’ai décidé d’essayer les deux en déployant la même application avec chaque moteur. Création du conteneur J’ai décidé d’utiliser Samba pour l’application de test. Samba est un serveur de fichiers populaire permettant aux ordinateurs Linux de partager des fichiers avec des ordinateurs Windows. Il communique via TCP sur le port 445. C’est la première fois que je travaille avec Docker, j’ai donc modifié un conteneur Samba standard afin d’inclure le fichier que je voulais servir. Après le tutoriel de Docker, j’ai lancé manuellement le conteneur à partir de la ligne de commande pour vérifier son fonctionnement: Et en effet, j’ai pu me connecter au serveur Samba dans le conteneur avec smbclient Maintenant que je sais que le conteneur fonctionne, je peux l’utiliser dans un moteur d’orchestration de conteneur. Préparer les machines virtuelles J’ai créé deux machines virtuelles exécutant Ubuntu 18.04 dans VirtualBox. J’ai ajouté une carte réseau supplémentaire à chaque machine virtuelle, configurée pour le réseau interne afin qu’ils puissent se parler: https://cdn-images-1.medium.com/max/1600/1*chCjRdcU_mV9ioAyQ7oB5A.png Ensuite, j’ai ajouté un serveur DHCP pour attribuer des adresses IP à chaque machine virtuelle: Les machines virtuelles peuvent désormais communiquer entre elles. Cela donne à ma machine virtuelle principale l’adresse IP 10.133.7.100. Docker Swarm Docker Swarm est un moteur d’orchestration de conteneur intégré à Docker lui-même. Quand je l’ai trouvé, j’étais sceptique: pourquoi l’utiliser à la place des Kubernetes, beaucoup plus célèbres? La réponse: Docker Swarm est axé sur la simplicité par rapport à la configuration. Cela ressemblait à l’iOS des moteurs d’orchestration de conteneurs par rapport à l’Android de Kubernetes. Mise en place de Docker Swarm Docker Swarm est facile à installer: il suffit d’installer Docker et docker-compose. Ensuite, après le tutoriel officiel, j’ai exécuté la seule commande nécessaire pour démarrer le noeud du gestionnaire, en transmettant l’adresse IP de la machine virtuelle actuelle: C’est tout: le moteur Docker tourne maintenant en mode Swarm. Ensuite, j’ai déployé un registre privé Docker afin que les autres noeuds puissent extraire des images, en suivant à nouveau les instructions d’installation: Déploiement de l’application Docker Swarm utilise le format Docker Compose pour spécifier les conteneurs à exécuter et les ports qu’ils exportent. Après le didacticiel Docker Compose, j’ai créé ce manifeste Docker Compose: Cela indique à Docker Compose de créer le fichier Docker à partir du répertoire «sambaonly», d’upload/pull les conteneurs construits vers mon registre privé nouvellement configuré et d’exporter le port 445 à partir du conteneur. Pour déployer ce manifeste, j’ai suivi le tutoriel de Docker Swarm. J’ai d’abord utilisé Docker Compose pour créer et télécharger le conteneur dans le registre privé: Une fois le conteneur créé, l’application peut être déployée avec la commande docker stack deploy, en spécifiant le nom du service: Et maintenant, l’application fonctionne sous Samba Swarm. J’ai testé qu’il fonctionne toujours avec smbclient: Ajout d’un autre noeud Ici encore, la simplicité de Docker Swarm transparaît. Pour installer un deuxième noeud, j’ai d’abord installé Docker, puis exécuté la commande que Docker m’avait donnée lors de l’installation de swarm: Pour exécuter mon application sur les deux nœuds, j’ai exécuté la commande scale de Docker Swarm sur le nœud du gestionnaire: Sur le nouveau noeud de travail, le nouveau conteneur est apparu: Test de l’équilibrage de charge (load balancing) Docker Swarm comprend un load balancing intégré appelé routeur Mesh: les demandes adressées à l’adresse IP de tout noeud sont automatiquement réparties sur l’ensemble de Swarm. Pour tester cela, j’ai établi 1000 connexions à l’adresse IP du noeud du gestionnaire avec nc: Samba génère un nouveau processus pour chaque connexion. Par conséquent, si l’équilibrage de la charge fonctionne, je m’attendrais à environ 500 processus Samba sur chaque noeud de Swarm. C’est bien ce qui se passe. Après avoir exécuté le script pour établir 1000 connexions, j’ai vérifié le nombre de processus Samba sur le gestionnaire (10.133.7.100): Et sur le noeud travailleur (10.133.7.50): Ainsi, exactement la moitié des demandes adressées au noeud de gestion ont été redirigées de manière magique vers le premier noeud de travail, ce qui montre que le cluster Swarm fonctionne correctement. J’ai trouvé que Docker Swarm était très facile à installer et il fonctionnait bien sous une charge (légère). Kubernetes Kubernetes est en train de devenir l’industrie standard de l’orchestration de conteneurs. C’est beaucoup plus flexible que Docker Swarm, mais cela rend plus difficile la configuration. Je l’ai trouvé pas si difficile, cependant. Pour cette expérience, au lieu d’utiliser un environnement de développement Kubernetes pré-construit tel que minikube, j’ai décidé de configurer mon propre cluster, à l’aide de Kubadm, WeaveNet et MetalLB. Mise en place de Kubernetes Kubernetes à la réputation d’être difficile à configurer: vous avez entendu le processus complexe en plusieurs étapes du didacticiel Kubernetes the Hard Way Les développeurs de Kubernetes ont simplifié l’utilisation de kubeadm. Malheureusement, Kubernetes étant si flexible, le tutoriel sur kubeadm ne couvre pas encore quelques étapes. J’ai donc dû déterminer le réseau et l’équilibreur de charge à utiliser moi-même. Voici ce que j’ai fini par lancer. J’ai d’abord dû désactiver Swap sur chaque noeud: Ensuite, j’ai configuré le noeud maître (10.133.7.100) avec la commande suivante: L’option --pod-network-cidr attribue une adresse réseau interne à tous les noeuds du réseau, utilisée pour les communications internes dans Kubernetes. Les options --apiserver-advertise-address et --apiserver-cert-extra-sans ont été ajoutées à cause d’un problème particulier dans l’installation de VirtualBox: la carte virtuelle principale des machines virtuelles (IP 10.0.2.15) ne peut accéder qu’à l’Internet. J’ai dû préciser que d’autres noeuds doivent accéder au maître à l’aide de l’adresse IP 10.133.7.100. Après avoir exécuté cette commande, Kubeadm a affiché quelques instructions: J’ai raté ces instructions la première fois et je n’ai donc pas terminé la configuration. J’ai ensuite passé une semaine entière à me demander pourquoi aucun de mes conteneurs ne fonctionnait! Après avoir enfin lu les instructions, je devais faire trois autres choses: Tout d’abord, je devais exécuter les commandes données par kubeadm pour configurer un fichier de configuration. Par défaut, Kubernetes ne planifie pas les conteneurs sur le nœud maître, mais uniquement sur les noeuds de travail. Comme je n’ai qu’un seul noeud pour le moment, le tutoriel m’a montré cette commande pour autoriser l’exécution de conteneurs sur le seul noeud: Enfin, je devais choisir un réseau pour mon cluster. Installation du réseau Contrairement à Docker Swarm, qui doit utiliser sa propre couche de routage maillé pour la mise en réseau et l’équilibrage de la charge, Kubernetes offre de multiples choix pour la mise en réseau et l’équilibrage de la charge. Le composant de mise en réseau permet aux conteneurs de communiquer en interne. J’ai fait des recherches et cet article comparatif suggérait Flannel ou WeaveNet, car ils sont faciles à configurer. Ainsi, j’ai décidé d’essayer WeaveNet. J’ai suivi les instructions du didacticiel kubeadm pour appliquer la configuration de WeaveNet: Ensuite, pour permettre aux conteneurs de communiquer avec le monde extérieur, j’ai besoin d’un équilibreur de charge. D’après mes recherches, j’ai eu l’impression que la plupart des implémentations de l’équilibreur de charge Kubernetes se concentrent uniquement sur les services HTTP, et non sur le TCP brut. Heureusement, j’ai trouvé MetalLB, un projet récent (vieux d’un an) qui comble cette lacune. Pour installer MetalLB, j’ai suivi son didacticiel de mise en route et j’ai tout d’abord déployé MetalLB: Ensuite, j’ai attribué la plage d’adresses IP 10.133.7.200 à 10.133.7.230 à MetalLB, en créant et en appliquant ce fichier de configuration: Déploiement de l’application Les fichiers de configuration du service de Kubernetes sont plus détaillés que ceux de Docker Swarm, en raison de la flexibilité de Kubernetes. En plus de spécifier le conteneur à exécuter, comme Docker Swarm, je dois spécifier comment chaque port doit être traité. Après avoir lu le tutoriel de Kubernetes, j’ai proposé cette configuration de Kubernetes, composée d’un service et d’un déploiement. https://gist.github.com/ludovicwyffels/911bb25b611f3519745aeee0d53c6447 Ce service demande à Kubernetes d’exporter le port TCP 445 de nos conteneurs Samba vers l’équilibreur de charge. https://gist.github.com/ludovicwyffels/41022da159c539e45027c68776f459d8 Cet objet Deployment indique à Kubernetes d’exécuter mon conteneur et d’exporter un port que le service doit gérer. Notez le replicas: 1 - c’est le nombre d’instances du conteneur que je veux exécuter. Je peux déployer ce service sur Kubernetes en utilisant kubectl apply: Et, après avoir redémarré ma machine virtuelle à quelque reprises, le déploiement a finalement commencé à fonctionner: Mon service est maintenant disponible sur l’adresse IP externe attribuée par MetalLB: Ajout d’un autre noeud Ajouter un autre noeud dans un cluster Kubernetes est beaucoup plus simple: il me suffisait d’exécuter la commande donnée par kubeadm sur le nouvel ordinateur: Bizarreries de ma configuration J’ai dû faire deux changements en raison de la configuration de VirtualBox: Premièrement, comme ma machine virtuelle dispose de deux cartes réseau, je dois indiquer manuellement l’adresse IP de ma machine à Kubernetes. Selon ce problème, je devais éditer Et changer une ligne en avant de redémarrer Kubernetes: L’autre solution concerne le registre Docker: comme le nouveau noeud ne peut accéder à mon registre privé sur le noeud maître, j’ai décidé de procéder à un terrible hack et de partager le registre de mon noeud maître vers la nouvelle machine à l’aide de ssh: Cela transmet le port 5000 du noeud principal, dora (qui exécute le registre Docker) à localhost, où Kubernetes peut le trouver sur cette machine. En production réelle, il est probable que le registre Docker sera hébergé sur une machine distincte, afin que tous les noeuds puissent y accéder. “Scaling” de l’application Lors de la deuxième installation de l’ordinateur, j’ai modifié mon déploiement d’origine pour ajouter une autre instance de l’application: Après avoir redémarré le maître et le worker à quelques reprises, la nouvelle instance de mon application a finalement quitté le statut de CreatingContainer et a commencé à s’exécuter: Test de l’équilibrage de charge J’ai utilisé la même procédure pour ouvrir 1000 connexions à Samba s’exécutant sur Kubernetes. Le résultat est intéressant. Master: Worker: Kubernetes / MetalLB a également équilibré la charge sur les deux machines, mais la machine principale a eu un peu moins de connexions que le worker. Je me demande pourquoi. Quoi qu’il en soit, cela montre que j’ai finalement réussi à installer Kubernetes après plusieurs détours. Comparaison et conclusion Fonctionnalités communes aux deux: les deux peuvent gérer des conteneurs et gérer intelligemment les demandes d’équilibrage de charge sur la même application TCP sur deux machines virtuelles différentes. Les deux ont une bonne documentation pour la configuration initiale. Les atouts de Docker Swarm: une configuration simple, aucune configuration requise, une intégration étroite avec Docker. Les points forts de Kubernetes: composants souples, nombreuses ressources disponibles et add-ons. Kubernetes vs Docker Swarm est un compromis entre simplicité et flexibilité. J’ai trouvé plus facile d’installer Docker Swarm, mais je ne peux pas, par exemple, échanger l’équilibreur de charge contre un autre composant. Il n’ya aucun moyen de le configurer: je devrais tout désactiver en même temps. Sur Kubernetes, il m’a fallu un certain temps pour trouver la bonne configuration, mais en échange, je pouvais changer certaines parties de mon cluster selon les besoins et installer facilement des add-ons, tels qu’un tableau de bord sophistiqué. Si vous voulez juste essayer Kubernetes sans toute cette configuration, je vous suggère d’utiliser minikube, qui offre une machine virtuelle de cluster Kubernetes prédéfinie, aucune installation requise. Enfin, je suis impressionné par le fait que les deux moteurs ont pris en charge les services TCP bruts: d’autres fournisseurs de services d’infrastructure en tant que services, tels que Heroku ou Glitch, ne prennent en charge que l’hébergement de sites Web HTTP. La disponibilité des services TCP signifie que l’on peut déployer ses propres serveurs de base de données, ses serveurs de cache et même ses serveurs Minecraft en utilisant les mêmes outils pour déployer des applications Web, faisant de la gestion de l’orchestration de conteneurs une compétence très utile. En conclusion, si je construisais un cluster, j’utiliserais Docker Swarm. Si je payais quelqu’un d’autre pour construire un cluster pour moi, je demanderais Kubernetes. Ce que j’ai appris Comment travailler avec les conteneurs Docker Comment configurer un cluster Docker Swarm à deux noeuds Comment configurer un cluster Kubernetes à deux noeuds et quels choix fonctionneraient pour une application basée sur TCP Comment déployer une application sur Docker Swarm et Kubernetes Comment réparer quoi que ce soit en redémarrant un ordinateur assez souvent, comme si je utilisais encore Windows 98 Kubernetes et Docker Swarm ne sont pas aussi intimidants qu’ils semblent","frontmatter":{"title":"Docker Swarm vs Kubernetes","subtitle":"J'ai trouvé que Docker Swarm est très facile à installer et à configurer, alors que Kubernetes est un peu plus difficile à installer mais reste simple à utiliser."},"fields":{"slug":"/dockerSwarm-vs-kubernetes/"}}},{"node":{"id":"42e1acee-95a7-5a43-8321-5c9d4e0d19ec","timeToRead":8,"excerpt":"Dans Kubernetes, il existe plusieurs façons de publier une application. Il est donc nécessaire de choisir la bonne stratégie pour rendre votre infrastructure fiable lors de la mise à jour d’une application. Le choix de la procédure de déploiement appropriée dépend des besoins. Nous avons énuméré ci-dessous certaines des stratégies possibles à adopter: Recreate RollingUpdate Blue/Green Canary A/B testing Vous pouvez expérimenter chacune de ces stratégies avec Minikube, les manifestes et les étapes à suivre sont expliqués dans ce github Examinons chaque stratégie et voyons quel type d’application vous conviendrez le mieux.  Recreate - idéal pour l’environnement de développement Un déploiement défini avec une stratégie de type recreate mettra fin à toutes les instances en cours d’exécution, puis les recréera avec la version la plus récente.  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/recreate Avantage État d’application entièrement renouvelé Inconvénients Temps d’arrêt qui dépend à la fois de la durée d’arrêt et du démarrage de l’application  RollingUpdate - déploiement lent Ce déploiement met à jour les pods de façon progressive. Un ReplicaSet secondaire est créé avec la nouvelle version de l’application, puis le nombre de répliques de l’ancienne version est réduit et la nouvelle version est augmentée jusqu’à ce que le nombre correct de répliques soit atteint.  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/ramped Lors de la configuration avec la mise à l’ échelle automatique du pod horizontal, il peut être pratique d’utiliser une valeur en pourcentage au lieu d’un nombre pour maxSurge et maxUnavailable . maxSurge permet d’indiquez combien de Pod il peut créer en plus du nombre de répliqua actuellement configurer maxUnavailable permet d’indiquer combien de Pod peuvent être “non disponible” pendant la mise à jour, toujours en fonction du nombre de répliqua configuré Si vous déclenchez un déploiement alors qu’un déploiement existant est en cours, le déploiement mettra le déploiement en pause et passera à une nouvelle version en remplaçant le déploiement. Avantages la version est lentement publiée sur toutes les instances pratique pour les applications avec état pouvant gérer le rééquilibrage des données Inconvénients le déploiement/la restauration peut prendre du temps la prise en charge de plusieurs API est difficile aucun contrôle sur le trafic  Blue/Green - mieux éviter les problèmes de versioning de l’API Un déploiement blue/green diffère d’un déploiement parce que la version “green” de l’application est déployée en parallèle de la version “blue”. Après avoir vérifié que la nouvelle version réponde aux exigences, nous mettons à jour l’objet Kubernetes Service qui joue le rôle d’équilibreur de charge pour envoyer du trafic vers la nouvelle version en remplaçant l’étiquette de version dans le champ selector  Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/blue-green Avantages déploiement instantané éviter le problème de versioning, changer l’état du cluster en une fois Inconvénients nécessite le double des ressources le bon test de toute la plate-forme doit être effectué avant la mise en production la gestion des applications avec état peut être difficile  Canary - laissez le consommateur faire le test Le déploiement Canary consiste à router un sous-ensemble d’utilisateurs vers une nouvelle fonctionnalité. Dans Kubernetes, un déploiement canary peut être effectué en utilisant deux déploiements avec des étiquettes de pods communes. Une réplique de la nouvelle version est publiée à côté de l’ancienne version. Ensuite, après un certain temps et si aucune erreur n’est détectée, augmentez le nombre de répliques de la nouvelle version et supprimez l’ancien déploiement. L’utilisation de cette technique ReplicaSet nécessite de faire tourner autant de pod que nécessaire pour obtenir le bon pourcentage de trafic. Cela dit, si vous voulez envoyer 1% du trafic vers la version B, vous devez avoir un pod fonctionnant avec la version B et 99 pods fonctionnant avec la version A. Cela peut être assez peu pratique à gérer donc si vous recherchez une meilleure répartition du trafic, regarder les équilibreurs de charge tels que HAProxy ou les mailles de service comme Linkerd, qui offrent un meilleur contrôle du trafic.  Dans l’exemple suivant, nous utilisons deux ReplicaSets côte à côte, la version A avec trois répliques (75% du trafic), la version B avec un répliqua (25% du trafic). Manifeste de déploiement tronqué version A: Manifeste de déploiement tronqué version B, notez que ous ne démarrons qu’un seul répliqua de l’application: Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/canary Avantages version publiée pour un sous-ensemble d’utilisateurs pratique pour la surveillance du taux d’erreur et des performances rollback rapide Inconvénients déploiement lent la répartition du trafic ajustée peut être coûteuse (99% A / 1% B = 99 pod A, 1 pod B) La procédure utilisée ci-dessus est native de Kubernetes, nous ajustons le nombre de répliques gérées par un ReplicaSet pour distribuer le trafic entre les versions. Si vous n’êtes pas sûr de l’impact que la sortie d’une nouvelle fonctionnalité pourrait avoir sur la stabilité de la plate-forme, une stratégie de sortie canari est suggérée.  A/B testing - idéal pour testes les fonctionnalités sur un sous-ensemble d’utilisateurs Le test A/B est en fait une technique permettant de prendre des décisions d’affaires basées sur des statistiques, plutôt qu’une stratégie de déploiement. Cependant, il est apparenté et peut être mis en oeuvre à l’aide d’un déploiement canari, c’est pourquoi nous en parlerons brièvement ici. En plus de répartir le trafic entre les versions en fonction du poids, vous pouvez cibler précisément un groupe donné d’utilisateurs en fonction de quelques paramètres (cookie, user agent, etc.). Cette technique est largement utilisée pour tester la conversion d’une fonctionnalité donnée et ne déployer que la version qui convient le plus. Istio, comme les autres maillages de service, fournis un moyen plus fin de subdiviser les instances de service avec un routage dynamique des requêtes basé sur des poids et/ou des en-têtes HTTP.  Vous trouverez ci-dessous un exemple d’installation de règles à l’aide d’Istio. Vous trouverez un exemple complet et les étapes de déploiement à l’adresse https://github.com/ludovicwyffels/k8s-deployment-demo/tree/master/ab-testing D’autres outils comme Linkerd, Traefik, NGINX, HAProxy, vous permettent également de le faire. Avantages nécessite un équilibreur de charge intelligent plusieurs versions en parallèle un contrôle total sur la répartition du trafic Inconvénients les erreurs difficiles à dépanner pour une session donnée, le traçage distribué deviennent obligatoires pas simple, vous devez configurer des outils supplémentaires  En résumé Il y a différentes façons de déployer une application, lors de la mise en production dans un environnement de développement/staging, un déploiement big bang ou progressif est généralement un bon choix. Lorsqu’il s’agit de production, un déploiement progressif ou en blue/green est généralement un bon choix, mais un test approprié de la nouvelle plate-forme est nécessaire. Si vous n’êtes pas sûr de la stabilité de la plate-forme et de l’impact que pourrait avoir la sortie d’une nouvelle version de logiciel, alors une version canari devrait être la bonne solution. Ce faisant, vous laissez le consommateur tester l’application et son intégration à la plate-forme. Enfin, si votre entreprise a besoin de tester une nouvelle fonctionnalité parmi un groupe spécifique d’utilisateurs, par exemple, tous les utilisateurs accédant à l’application à l’aide d’un téléphone mobile sont envoyés à la version A, tous les utilisateurs accédant via un ordinateur passent à la version B. Vous pouvez alors utiliser la technique de test A/B qui en utilisant un réseau de services Kubernetes ou une configuration serveur personnalisée vous permet de déterminer où un utilisateur doit être dirigé en fonction de certains paramètres.","frontmatter":{"title":"Stratégie de déploiement de Kubernetes","subtitle":"Dans Kubernetes, il y a plusieurs façons de publier une application, il est nécessaire de choisir la bonne stratégie pour rendre votre infrastructure fiable pendant la mise à jour d'une application."},"fields":{"slug":"/k8s-deployement-strategies/"}}},{"node":{"id":"c6aa09b0-1bea-52f2-9d19-126b12c5cc10","timeToRead":4,"excerpt":"Minikube est un outil idéal pour configuer localement Kubernetes afin de tester et d’expérimenter vos déploiements. Dans ce guide, je vais essayer de vous aider à le mettre en marche sur votre machine locale, à donner quelques conseils sur où et comment effectuer certaines tâches et à le rendre aussi capable (je suppose quand vous utilisez k8s que vous veulent apprendre et utiliser Helm, etcd, istio, etc.). Installation de minikube Minikube fonctionne avec une machine virtuelle. Pour cela, on peut utiliser diverses options en fonction de vos préférences et de votre système d’exploitation. Ma préférence dans ce cas est Orable VirtualBox. Vous pouvez utiliser brew pour tout installer: Dans ce cas, vous pourriez obtenir une erreur d’installation peu concluante liée à l’installation de virtualbox, en particulier sur Mojave et problablement par la suite. Quoi qu’il en soit, il s’agit problablement d’une nouvelle fonctionnalité de sécurité dans MacOs X qui vous gêne. Allez dans Préférences Système> Sécurité et confidentialité et sur l’écran Général, vous verrez un (ou quelques) messages concernant certains logiciels nécessitant une approbation pour être installés. Vous devez examiner attentivement la liste s’il en existe plusieurs et autoriser l’installation du logiciel dont vous avez besoin - dans ce cas, le logiciel Oracle. Cela fait, vous pourrez relancer la commande ci-dessus et vous devrez alors être prêt pour les étapes suivantes. Exécuter et accéder au cluster Afin d’utiliser de manière optimale les ressources de votre ordinateur local, je suggérerais de l’arrêter quand vous n’en avez plus besoin… Avec VirtualBox au centre, il utilisera la batterie de votre ordinateur portable assez rapidement. Recommencer plus tard vous ramènera là où vous l’avez laissé: Le tableau de bord Kubernetes est également disponible (lorsque minikube est en cours d’exécution): Je vais supposer que vous avez kubectl installé localement et que vous l’utilisez déjà pour certains clusters distants, vous disposez donc de plusieurs contextes. Dans ce cas, vous devez répertorier les contextes et passer à minikube. Vous vous trouvez maintenant dans le contexte de votre cluster k8 local qui s’exécute sur minikube et vous pouvez effectuer toutes les opérations k8 qu’il contient. Ingress Pour exécuter vos déploiements comportant un ingress, vous aurez besoin d’un add-on d’entrée: Assurez-vous que vous configurez l’ingress en fonction de vos hôtes locaux. Cela signifie fondamentalement que tout ce que vous définissez comme hôte dans vos règles d’ingress doit être configuré dans votre fichier /etc/hosts Où [minikube ip] devrait être remplacé par l’ip actuel de minikube. Il fonctionne également avec plusieurs hôtes locaux séparés par des espaces après l’adresse ip de minikube. Voici un raccourci pour le faire en bash Docker registry La réalité de l’utilisation réelle du registre de conteneurs dans l’environnement local est rude. Je vais donc vous fournir une option simple, rapide et simpliste qui facilite le déploiement de votre travail local sur vos k8s locaux, mais vous prive de l’expérience très importante du registre de conteneurs. Registre de conteneurs local Obtenez le contexte de votre docker local pour pointer vers le context minikube:  Dans le contexte minikube, pour démarrer le registre docker local Donc, vous avez maintenant un registre local dans lequel pousser des choses (tant que votre docker est dans le contexte minikube) Vous pouvez maintenant faire: A ce stade, vous pouvez utiliser localhost:5000/<your_tag>: comme image dans votre déploiement et c’est tout. Utilisation du référentiel de conteneur distant Pour utiliser localement le référentiel de conteneur distant, vous devez fournir un moyen d’authentification, qui se base sur les secrets de Kubernetes. Pour la gestion des secrets locaux pour ECR, GCR and Docker registry, je recommande d’utiliser l’addon minikube appelé registry-creds. Je ne le considère pas suffisamment sûr pour être utilisé ailleurs que dans l’environnement local. Helm Helm est un gestionnaire de paquets pour k8s et est souvent utilisé pour la gestion de la configuration d’un déploiement à l’autre. Compte tenu de la grande popularité de l’outil et de son adoption croissante, je voudrais terminer ce guide par une note sur l’ajout de helm à votre environnement Kubernetes local. C’est assez facile à ce stade, il suffit de mettre en place minikube et: Helm utilise un backend appelé Tiller. C’est ce qui est installé/déployé lors de l’exécution de helm init. Une lecture précieuse: https://helm.sh/docs/using_helm/ Maintenant, vous disposez d’un environnement Kubernetes local complet capable d’accepter tous vos déploiements de test avant de décider de les placer dans le cloud.","frontmatter":{"title":"Configuration locale de Kubernetes avec minikube sur MacOS X","subtitle":""},"fields":{"slug":"/k8s-minikube/"}}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/dockerSwarm-vs-kubernetes/","prev":{"excerpt":"Souvent, je me trouve aux prises avec Sequelize pour trouver une réponse directe à ma requête. Récemment, je travaillais sur une application full stack dans laquelle il était impératif de paginer les résultats depuis le backend (API REST) vers le client. Je me suis battu pour deux raisons. Tout d’abord, venant du context NoSQL, il est difficile de saisir les bases de données SQL. La deuxième raison étant que la documentation de Sequelize ne fournit pas une solution claire et directe à cette abstraction très basique. Beaucoup de gens supposent des choses dans le monde des bases de données SQL. Ainsi, dans cet article, nous allons parler d’un module de base de pagination utilisant Sequelize, MySQL et Node.js. J’utilise des tables et des enregistrements dans votre base de données MySQL. Pour configurer une nouvelle application et établir une connexion à une base de données, lisez mon post sur Premiers pas avec Sequelize. Définir un modèle Je saute directement sur la définition du modèle utilisateur: J’utilise une table contenant une centaine d’enregistrements d’utilisateur que nous voulons afficher sur une application Web, par exemple dans le panneau d’administration, et nous voulons afficher seulement 50 enregistrements à la fois. Dans le fichier api/user.js, je définis un endpoint /:page qui extraira le nombre de résultats nécessaires de la base de données. findAndCountAll est le modèle de recherche dans plusieurs enregistrements de la base de données. Il retourne à la fois les données requises et le nombre d’éléments de cette table. La requête ci-dessus obtiendra 50 enregistrements d’utilisateur à la fois jusqu’à ce que la page suivante soit appelée pour extraire les 50 prochains enregistrements. limit et offset sont nécessaires dans les requêtes liées à la pagination dans lesquelles limit extrait le nombre de lignes en fonction de la requête, tandis que offset est utilisé pour ignorer le nombre","timeToRead":1,"frontmatter":{"title":"Comment paginer des enregistrements dans MySQL avec Sequelize et Nodejs","subtitle":"","tags":["Sequelize","Node.js","Javascript"],"category":["Node.js"],"date":"2018-10-13T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"aspectRatio":1.4992503748125936,"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMEAQX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABc/i1RhGH/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAMBAhEEE//aAAgBAQABBQKvRtmuqontsQqdanTwP//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/Aar/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAgEBPwGI/8QAHBAAAQMFAAAAAAAAAAAAAAAAAAEiMQIREjLh/9oACAEBAAY/An0mV5g4bqI4k//EABoQAQACAwEAAAAAAAAAAAAAAAEAESFBUfH/2gAIAQEAAT8hKyhSrJs7VFOGpdNmVqHIjzP/2gAMAwEAAgADAAAAEOPv/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QhD//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/EA//xAAfEAEAAgIABwAAAAAAAAAAAAABABEhMUFRYXGhsfD/2gAIAQEAAT8Q65gFNZ8S8YYMuKrcvBhNcPuECl5hEFdEsYiW1/O8/9k=","sizes":"(max-width: 2000px) 100vw, 2000px","src":"/static/e47b6c69cbe49a04cdbe6e0e621f405f/883ab/open-book.jpg","srcSet":"/static/e47b6c69cbe49a04cdbe6e0e621f405f/f8f18/open-book.jpg 930w,\n/static/e47b6c69cbe49a04cdbe6e0e621f405f/0e6ff/open-book.jpg 1860w,\n/static/e47b6c69cbe49a04cdbe6e0e621f405f/883ab/open-book.jpg 2000w"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/node-sequelize-pagination/"}},"next":{"excerpt":"Si vous avez un Kindle d’Amazon, vous ne le savez peut-être pas, mais vous possédez une adresse mail ….@kindle.com qui vous permet de vous envoyer des livres et documents directement sur votre liseuse. Pour cela, il faut ajouter votre email via cette page, dans la section “Settings” > “Personal Document Settings” > “Approved Personal Document E-mail List”. Et juste au-dessus, vous devriez voir dans la section “Send-to-Kindle E-Mail Settings”, le ou les emails @kindle.com associés avec chacun de vos appareils Kindle. En effet, en envoyant un fichier depuis votre email référencé vers votre email kindle.com, vous pouvez le transférer vers votre liseuse sans avoir besoin de brancher le moindre câble ou d’installer le moindre soft. Seulement, Amazon autorise l’envoi uniquement des fichiers aux formats suivants : Kindle Format (.MOBI, .AZW) Microsoft Word (.DOC, .DOCX) HTML (.HTML, .HTM) RTF (.RTF) Text (.TXT) JPEG (.JPEG, .JPG) GIF (.GIF) PNG (.PNG) BMP (.BMP) PDF (.PDF) Malheureusement, comme vous pouvez le voir, pas de format epub. Obligé de brancher le Kindle à votre ordinateur et d’utiliser Calibre ? Non, non, non. Il suffit de vous rendre sur le site Send epub to Kindle , puis de renseigner les infos demandées, à savoir votre email référencé, l’email Kindle et de glisser-déposer le livre au format epub de votre choix. Cliquez ensuite sur le bouton vert « Upload & Send » et tadaaaa, au bout de quelques secondes, le livre apparaitra sur votre liseuse Kindle. Elle est pas belle la vie ?","timeToRead":1,"frontmatter":{"title":"Comment envoyer un epub vers une Kindle sans utiliser Calibre ni de cable USB ?","subtitle":"","tags":["Kindle","Ebook"],"category":["Kindle","Ebook"],"date":"2018-11-24T08:00:00.000Z","draft":false,"image":{"childImageSharp":{"fluid":{"aspectRatio":1.5,"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGa0ziTSqWf/8QAGBABAQEBAQAAAAAAAAAAAAAAAQIDACL/2gAIAQEAAQUCy0ItmMp9HVjK1mHMj3//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQIBAT8BV//EABsQAAIDAAMAAAAAAAAAAAAAAAABAhExEBIh/9oACAEBAAY/As8xkujuUlRnFos//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARITFB/9oACAEBAAE/ISa6jh2N3hhGocLS2JPOElOjWf/aAAwDAQACAAMAAAAQ2P8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAEDAQE/EIw//8QAFhEBAQEAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EK2//8QAGxABAQACAwEAAAAAAAAAAAAAAREAMSFRYUH/2gAIAQEAAT8QFwmLUI775mAebFhqWTRLvCPHnyYECp4Z6B6G/HGBI1nef//Z","sizes":"(max-width: 1920px) 100vw, 1920px","src":"/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg","srcSet":"/static/e96dc41e5ffd5680b39408b75434d7e0/f8f18/kindle.jpg 930w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/0e6ff/kindle.jpg 1860w,\n/static/e96dc41e5ffd5680b39408b75434d7e0/14dee/kindle.jpg 1920w"}}},"author":{"id":"ludo","bio":"Développeur senior. Fullstack + DevOps","avatar":{"children":[{"fixed":{"src":"/static/5f2c129e42248a92c87b13b4293950cf/4e842/ghost.png"}}]}}},"fields":{"layout":"post","slug":"/kindle/"}},"primaryTag":"Kubernetes","primaryCategory":"DevOps"}}}